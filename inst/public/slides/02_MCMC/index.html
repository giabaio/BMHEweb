<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>‚Å£2. Learning from data using MCMC and BUGS</title>
    <meta charset="utf-8" />
    <meta name="author" content="Nathan Green" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <script src="libs/freezeframe/freezeframe.min.js"></script>
    <script src="libs/xaringanExtra-freezeframe/freezeframe-init.js"></script>
    <script id="xaringanExtra-freezeframe-options" type="application/json">{"selector":"img[src$=\"gif\"]","trigger":"click","overlay":false,"responsive":true,"warnings":true}</script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <!-- (Re)Defines a bunch of LaTeX commands that can then be used directly in the .Rmd file as '\command{...}' -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        /* This enables color macros */
        extensions: ["color.js"],
        Macros: {
          /* Probability & mathematical symbols */
          Pr: "{\\style{font-family:inherit; font-size: 110%;}{\\text{Pr}}}",
          exp: "{\\style{font-family:inherit; font-size: 105%;}{\\text{exp}}}",
          log: "{\\style{font-family:inherit; font-size: 105%;}{\\text{log}}}",
          ln: "{\\style{font-family:inherit; font-size: 105%;}{\\text{ln}}}",
          logit: "{\\style{font-family:inherit; font-size: 100%;}{\\text{logit}}}",
          HR: "{\\style{font-family:inherit; font-size: 105%;}{\\text{HR}}}",
          OR: "{\\style{font-family:inherit; font-size: 105%;}{\\text{OR}}}",
          E: "{\\style{font-family:inherit; font-size: 105%;}{\\text{E}}}",
          Var: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Var}}}",
          Cov: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Cov}}}",
          Corr: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Corr}}}",
          DIC: "{\\style{font-family:inherit; font-size: 105%;}{\\text{DIC}}}",
          se: "{\\style{font-family:inherit; font-size: 100%;}{\\text{se}}}",
          sd: "{\\style{font-family:inherit; font-size: 100%;}{\\text{sd}}}",
          kld: "{\\style{font-family:inherit; font-size: 100%;}{\\text{KLD}}}",
          /* Distributions */
          dnorm: "{\\style{font-family:inherit;}{\\text{Normal}}}",
          dt: "{\\style{font-family:inherit;}{\\text{t}}}",
          ddirch: "{\\style{font-family:inherit;}{\\text{Dirichlet}}}",
          dmulti: "{\\style{font-family:inherit;}{\\text{Multinomial}}}",
          dbeta: "{\\style{font-family:inherit;}{\\text{Beta}}}",
          dgamma: "{\\style{font-family:inherit;}{\\text{Gamma}}}",
          dbern: "{\\style{font-family:inherit;}{\\text{Bernoulli}}}",
          dbin: "{\\style{font-family:inherit;}{\\text{Binomial}}}",
          dpois: "{\\style{font-family:inherit;}{\\text{Poisson}}}",
          dweib: "{\\style{font-family:inherit;}{\\text{Weibull}}}",
          dexp: "{\\style{font-family:inherit;}{\\text{Exponential}}}",
          dlnorm: "{\\style{font-family:inherit;}{\\text{logNormal}}}",
          dunif: "{\\style{font-family:inherit;}{\\text{Uniform}}}",
          /* LaTeX formatting */
          bm: ["{\\boldsymbol #1}",1],
          /* These create macros to typeset numbers in maths with the basic font */
          0: "{\\style{font-family:inherit; font-size: 105%;}{\\text{0}}}",
          1: "{\\style{font-family:inherit; font-size: 105%;}{\\text{1}}}",
          2: "{\\style{font-family:inherit; font-size: 105%;}{\\text{2}}}",
          3: "{\\style{font-family:inherit; font-size: 105%;}{\\text{3}}}",
          4: "{\\style{font-family:inherit; font-size: 105%;}{\\text{4}}}",
          5: "{\\style{font-family:inherit; font-size: 105%;}{\\text{5}}}",
          6: "{\\style{font-family:inherit; font-size: 105%;}{\\text{6}}}",
          7: "{\\style{font-family:inherit; font-size: 105%;}{\\text{7}}}",
          8: "{\\style{font-family:inherit; font-size: 105%;}{\\text{8}}}",
          9: "{\\style{font-family:inherit; font-size: 105%;}{\\text{9}}}",
          /* Health economics quantities */
          icer: "{\\style{font-family:inherit; font-size: 100%;}{\\text{ICER}}}",
          nb: "{\\style{font-family:inherit; font-size: 100%;}{\\text{NB}}}",
          ib: "{\\style{font-family:inherit; font-size: 100%;}{\\text{IB}}}",
          eib: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EIB}}}",
          ol: "{\\style{font-family:inherit; font-size: 100%;}{\\text{OL}}}",
          ceac: "{\\style{font-family:inherit; font-size: 100%;}{\\text{CEAC}}}",
          evpi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVPI}}}",
          evppi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVPPI}}}",
          evsi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVSI}}}"
        }
      }
    });
    </script>
    <link rel="stylesheet" href="../assets/beamer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: title-slide

# &amp;#8291;2. Learning from data using MCMC and `BUGS`

## Nathan Green

### [Department of Statistical Science](https://www.ucl.ac.uk/statistics/) | University College London    

.title-small[
&lt;svg viewBox="0 0 512 512" style="position:relative;display:inline-block;top:.1em;fill:#00acee;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"&gt;&lt;/path&gt;&lt;/svg&gt;  [n.green@ucl.ac.uk](mailto:n.green@ucl.ac.uk)
&lt;svg viewBox="0 0 512 512" style="position:relative;display:inline-block;top:.1em;fill:#EA7600;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M503.52,241.48c-.12-1.56-.24-3.12-.24-4.68v-.12l-.36-4.68v-.12a245.86,245.86,0,0,0-7.32-41.15c0-.12,0-.12-.12-.24l-1.08-4c-.12-.24-.12-.48-.24-.6-.36-1.2-.72-2.52-1.08-3.72-.12-.24-.12-.6-.24-.84-.36-1.2-.72-2.4-1.08-3.48-.12-.36-.24-.6-.36-1-.36-1.2-.72-2.28-1.2-3.48l-.36-1.08c-.36-1.08-.84-2.28-1.2-3.36a8.27,8.27,0,0,0-.36-1c-.48-1.08-.84-2.28-1.32-3.36-.12-.24-.24-.6-.36-.84-.48-1.2-1-2.28-1.44-3.48,0-.12-.12-.24-.12-.36-1.56-3.84-3.24-7.68-5-11.4l-.36-.72c-.48-1-.84-1.8-1.32-2.64-.24-.48-.48-1.08-.72-1.56-.36-.84-.84-1.56-1.2-2.4-.36-.6-.6-1.2-1-1.8s-.84-1.44-1.2-2.28c-.36-.6-.72-1.32-1.08-1.92s-.84-1.44-1.2-2.16a18.07,18.07,0,0,0-1.2-2c-.36-.72-.84-1.32-1.2-2s-.84-1.32-1.2-2-.84-1.32-1.2-1.92-.84-1.44-1.32-2.16a15.63,15.63,0,0,0-1.2-1.8L463.2,119a15.63,15.63,0,0,0-1.2-1.8c-.48-.72-1.08-1.56-1.56-2.28-.36-.48-.72-1.08-1.08-1.56l-1.8-2.52c-.36-.48-.6-.84-1-1.32-1-1.32-1.8-2.52-2.76-3.72a248.76,248.76,0,0,0-23.51-26.64A186.82,186.82,0,0,0,412,62.46c-4-3.48-8.16-6.72-12.48-9.84a162.49,162.49,0,0,0-24.6-15.12c-2.4-1.32-4.8-2.52-7.2-3.72a254,254,0,0,0-55.43-19.56c-1.92-.36-3.84-.84-5.64-1.2h-.12c-1-.12-1.8-.36-2.76-.48a236.35,236.35,0,0,0-38-4H255.14a234.62,234.62,0,0,0-45.48,5c-33.59,7.08-63.23,21.24-82.91,39-1.08,1-1.92,1.68-2.4,2.16l-.48.48H124l-.12.12.12-.12a.12.12,0,0,0,.12-.12l-.12.12a.42.42,0,0,1,.24-.12c14.64-8.76,34.92-16,49.44-19.56l5.88-1.44c.36-.12.84-.12,1.2-.24,1.68-.36,3.36-.72,5.16-1.08.24,0,.6-.12.84-.12C250.94,20.94,319.34,40.14,367,85.61a171.49,171.49,0,0,1,26.88,32.76c30.36,49.2,27.48,111.11,3.84,147.59-34.44,53-111.35,71.27-159,24.84a84.19,84.19,0,0,1-25.56-59,74.05,74.05,0,0,1,6.24-31c1.68-3.84,13.08-25.67,18.24-24.59-13.08-2.76-37.55,2.64-54.71,28.19-15.36,22.92-14.52,58.2-5,83.28a132.85,132.85,0,0,1-12.12-39.24c-12.24-82.55,43.31-153,94.31-170.51-27.48-24-96.47-22.31-147.71,15.36-29.88,22-51.23,53.16-62.51,90.36,1.68-20.88,9.6-52.08,25.8-83.88-17.16,8.88-39,37-49.8,62.88-15.6,37.43-21,82.19-16.08,124.79.36,3.24.72,6.36,1.08,9.6,19.92,117.11,122,206.38,244.78,206.38C392.77,503.42,504,392.19,504,255,503.88,250.48,503.76,245.92,503.52,241.48Z"&gt;&lt;/path&gt;&lt;/svg&gt;  [https://n8thangreen.github.io/](https://n8thangreen.github.io/)
&lt;svg viewBox="0 0 512 512" style="position:relative;display:inline-block;top:.1em;fill:#EA7600;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M503.52,241.48c-.12-1.56-.24-3.12-.24-4.68v-.12l-.36-4.68v-.12a245.86,245.86,0,0,0-7.32-41.15c0-.12,0-.12-.12-.24l-1.08-4c-.12-.24-.12-.48-.24-.6-.36-1.2-.72-2.52-1.08-3.72-.12-.24-.12-.6-.24-.84-.36-1.2-.72-2.4-1.08-3.48-.12-.36-.24-.6-.36-1-.36-1.2-.72-2.28-1.2-3.48l-.36-1.08c-.36-1.08-.84-2.28-1.2-3.36a8.27,8.27,0,0,0-.36-1c-.48-1.08-.84-2.28-1.32-3.36-.12-.24-.24-.6-.36-.84-.48-1.2-1-2.28-1.44-3.48,0-.12-.12-.24-.12-.36-1.56-3.84-3.24-7.68-5-11.4l-.36-.72c-.48-1-.84-1.8-1.32-2.64-.24-.48-.48-1.08-.72-1.56-.36-.84-.84-1.56-1.2-2.4-.36-.6-.6-1.2-1-1.8s-.84-1.44-1.2-2.28c-.36-.6-.72-1.32-1.08-1.92s-.84-1.44-1.2-2.16a18.07,18.07,0,0,0-1.2-2c-.36-.72-.84-1.32-1.2-2s-.84-1.32-1.2-2-.84-1.32-1.2-1.92-.84-1.44-1.32-2.16a15.63,15.63,0,0,0-1.2-1.8L463.2,119a15.63,15.63,0,0,0-1.2-1.8c-.48-.72-1.08-1.56-1.56-2.28-.36-.48-.72-1.08-1.08-1.56l-1.8-2.52c-.36-.48-.6-.84-1-1.32-1-1.32-1.8-2.52-2.76-3.72a248.76,248.76,0,0,0-23.51-26.64A186.82,186.82,0,0,0,412,62.46c-4-3.48-8.16-6.72-12.48-9.84a162.49,162.49,0,0,0-24.6-15.12c-2.4-1.32-4.8-2.52-7.2-3.72a254,254,0,0,0-55.43-19.56c-1.92-.36-3.84-.84-5.64-1.2h-.12c-1-.12-1.8-.36-2.76-.48a236.35,236.35,0,0,0-38-4H255.14a234.62,234.62,0,0,0-45.48,5c-33.59,7.08-63.23,21.24-82.91,39-1.08,1-1.92,1.68-2.4,2.16l-.48.48H124l-.12.12.12-.12a.12.12,0,0,0,.12-.12l-.12.12a.42.42,0,0,1,.24-.12c14.64-8.76,34.92-16,49.44-19.56l5.88-1.44c.36-.12.84-.12,1.2-.24,1.68-.36,3.36-.72,5.16-1.08.24,0,.6-.12.84-.12C250.94,20.94,319.34,40.14,367,85.61a171.49,171.49,0,0,1,26.88,32.76c30.36,49.2,27.48,111.11,3.84,147.59-34.44,53-111.35,71.27-159,24.84a84.19,84.19,0,0,1-25.56-59,74.05,74.05,0,0,1,6.24-31c1.68-3.84,13.08-25.67,18.24-24.59-13.08-2.76-37.55,2.64-54.71,28.19-15.36,22.92-14.52,58.2-5,83.28a132.85,132.85,0,0,1-12.12-39.24c-12.24-82.55,43.31-153,94.31-170.51-27.48-24-96.47-22.31-147.71,15.36-29.88,22-51.23,53.16-62.51,90.36,1.68-20.88,9.6-52.08,25.8-83.88-17.16,8.88-39,37-49.8,62.88-15.6,37.43-21,82.19-16.08,124.79.36,3.24.72,6.36,1.08,9.6,19.92,117.11,122,206.38,244.78,206.38C392.77,503.42,504,392.19,504,255,503.88,250.48,503.76,245.92,503.52,241.48Z"&gt;&lt;/path&gt;&lt;/svg&gt;  [https://egon.stats.ucl.ac.uk/research/statistics-health-economics/](https://egon.stats.ucl.ac.uk/research/statistics-health-economics/)
&lt;svg viewBox="0 0 496 512" style="position:relative;display:inline-block;top:.1em;fill:black;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"&gt;&lt;/path&gt;&lt;/svg&gt;  [https://github.com/n8thangreen](https://github.com/n8thangreen/)
&lt;svg viewBox="0 0 496 512" style="position:relative;display:inline-block;top:.1em;fill:black;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"&gt;&lt;/path&gt;&lt;/svg&gt;  [https://github.com/StatisticsHealthEconomics](https://github.com/StatisticsHealthEconomics)
]

### Bayesian Methods in Health Economics, Lausanne

---

layout: true

.my-footer[ 
.alignleft[
&amp;nbsp; &amp;copy; Nathan Green (UCL) &amp;nbsp;&lt;a href="https://github.com/n8thangreen/"; title="Check out my repos"&gt;&lt;svg viewBox="0 0 496 512" style="position:relative;display:inline-block;top:.1em;fill:#bcc0c4;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;&amp;nbsp; &amp;nbsp;&lt;a href="mailto:n.green@ucl.ac.uk"; title="Email me"&gt;&lt;svg viewBox="0 0 512 512" style="position:relative;display:inline-block;top:.1em;fill:#bcc0c4;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;&amp;nbsp; &amp;nbsp;&lt;a href="https://n8thangreen.github.io/"; title="Visit my website"&gt;&lt;svg viewBox="0 0 512 512" style="position:relative;display:inline-block;top:.1em;fill:#bcc0c4;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;path d="M503.52,241.48c-.12-1.56-.24-3.12-.24-4.68v-.12l-.36-4.68v-.12a245.86,245.86,0,0,0-7.32-41.15c0-.12,0-.12-.12-.24l-1.08-4c-.12-.24-.12-.48-.24-.6-.36-1.2-.72-2.52-1.08-3.72-.12-.24-.12-.6-.24-.84-.36-1.2-.72-2.4-1.08-3.48-.12-.36-.24-.6-.36-1-.36-1.2-.72-2.28-1.2-3.48l-.36-1.08c-.36-1.08-.84-2.28-1.2-3.36a8.27,8.27,0,0,0-.36-1c-.48-1.08-.84-2.28-1.32-3.36-.12-.24-.24-.6-.36-.84-.48-1.2-1-2.28-1.44-3.48,0-.12-.12-.24-.12-.36-1.56-3.84-3.24-7.68-5-11.4l-.36-.72c-.48-1-.84-1.8-1.32-2.64-.24-.48-.48-1.08-.72-1.56-.36-.84-.84-1.56-1.2-2.4-.36-.6-.6-1.2-1-1.8s-.84-1.44-1.2-2.28c-.36-.6-.72-1.32-1.08-1.92s-.84-1.44-1.2-2.16a18.07,18.07,0,0,0-1.2-2c-.36-.72-.84-1.32-1.2-2s-.84-1.32-1.2-2-.84-1.32-1.2-1.92-.84-1.44-1.32-2.16a15.63,15.63,0,0,0-1.2-1.8L463.2,119a15.63,15.63,0,0,0-1.2-1.8c-.48-.72-1.08-1.56-1.56-2.28-.36-.48-.72-1.08-1.08-1.56l-1.8-2.52c-.36-.48-.6-.84-1-1.32-1-1.32-1.8-2.52-2.76-3.72a248.76,248.76,0,0,0-23.51-26.64A186.82,186.82,0,0,0,412,62.46c-4-3.48-8.16-6.72-12.48-9.84a162.49,162.49,0,0,0-24.6-15.12c-2.4-1.32-4.8-2.52-7.2-3.72a254,254,0,0,0-55.43-19.56c-1.92-.36-3.84-.84-5.64-1.2h-.12c-1-.12-1.8-.36-2.76-.48a236.35,236.35,0,0,0-38-4H255.14a234.62,234.62,0,0,0-45.48,5c-33.59,7.08-63.23,21.24-82.91,39-1.08,1-1.92,1.68-2.4,2.16l-.48.48H124l-.12.12.12-.12a.12.12,0,0,0,.12-.12l-.12.12a.42.42,0,0,1,.24-.12c14.64-8.76,34.92-16,49.44-19.56l5.88-1.44c.36-.12.84-.12,1.2-.24,1.68-.36,3.36-.72,5.16-1.08.24,0,.6-.12.84-.12C250.94,20.94,319.34,40.14,367,85.61a171.49,171.49,0,0,1,26.88,32.76c30.36,49.2,27.48,111.11,3.84,147.59-34.44,53-111.35,71.27-159,24.84a84.19,84.19,0,0,1-25.56-59,74.05,74.05,0,0,1,6.24-31c1.68-3.84,13.08-25.67,18.24-24.59-13.08-2.76-37.55,2.64-54.71,28.19-15.36,22.92-14.52,58.2-5,83.28a132.85,132.85,0,0,1-12.12-39.24c-12.24-82.55,43.31-153,94.31-170.51-27.48-24-96.47-22.31-147.71,15.36-29.88,22-51.23,53.16-62.51,90.36,1.68-20.88,9.6-52.08,25.8-83.88-17.16,8.88-39,37-49.8,62.88-15.6,37.43-21,82.19-16.08,124.79.36,3.24.72,6.36,1.08,9.6,19.92,117.11,122,206.38,244.78,206.38C392.77,503.42,504,392.19,504,255,503.88,250.48,503.76,245.92,503.52,241.48Z"&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;&amp;nbsp;
]
.aligncenter[
.mydropdown[ 
.mydropbtn[
&lt;svg viewBox="0 0 512 512" style="position:relative;display:inline-block;top:.1em;fill:white;height:0.8em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zM273 369.9l135.5-135.5c9.4-9.4 9.4-24.6 0-33.9l-17-17c-9.4-9.4-24.6-9.4-33.9 0L256 285.1 154.4 183.5c-9.4-9.4-24.6-9.4-33.9 0l-17 17c-9.4 9.4-9.4 24.6 0 33.9L239 369.9c9.4 9.4 24.6 9.4 34 0z"&gt;&lt;/path&gt;&lt;/svg&gt; 
]
.mydropdown-content[
[1. Introduction](../01_Intro/index.html)
[2. MCMC](../02_MCMC/index.html)
[3. Intro HTA](../03_Intro_HE/index.html)
[4. ILD](../04_ILD/index.html)
[5. ALD](../05_ALD/index.html)
[6. NMA](../06_NMA/index.html)
[7. Model uncertainty](../07_Model_uncertainty/index.html)
[8. Survival](../06_Survival/index.html)
[9. Markov models](../09_MM/index.html)
[10. Missing data](../10_Missing/index.html)
[11. VoI](../11_VoI/index.html)
[12. EVPPI](../12_EVPPI/index.html)
[13. Intro EVSI](../13_EVSI/index.html)
[14. Data EVSI](../14_Data_EVSI/index.html)
[15. EVSI MC](../15_EVSI_MC/index.html)
[16. EVSI Regression](../16_EVSI_regression/index.html)
]
]
&amp;#8291;2. Learning from data using MCMC and `BUGS` 
]
.alignright[
&amp;nbsp;&lt;a target="_self" href="../../index.html"; title="Back to the summer school website"&gt;&lt;svg viewBox="0 0 576 512" style="position:relative;display:inline-block;top:.1em;fill:#bcc0c4;height:1em;bottom:1em;" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;path d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;&amp;nbsp; &amp;nbsp; BMHE 
]
] 

---

# Summary 

- Already seen how to make predictions based on parameters with *known* uncertainty distributions

- Here we **learn** about parameters from **observed data** using *Bayes theorem*

- Simple example &amp;ndash; trial with binary outcome
   - *"conjugate"*: no simulation needed

- Simulating posterior distributions of unknowns given data, using **MCMC (Markov Chain Monte Carlo)** in `BUGS`
   - Practical issues: putting data in 
   - Convergence (knowing which / how many simulations to save)

- Expressing full uncertainty on any function / transformation of unknown parameters

&lt;span style="display:block; margin-top: 4rem ;"&gt;&lt;/span&gt;
.content-box-beamer[
### References 
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

&lt;svg viewBox="0 0 448 512" xmlns="http://www.w3.org/2000/svg" style="height:1em;fill:currentColor;position:relative;display:inline-block;top:.1em;"&gt;
  [ comment ]
  &lt;path d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"&gt;&lt;/path&gt;
&lt;/svg&gt; &lt;i&gt;The BUGS Book&lt;/i&gt;, chapters 3, 4 .button[&lt;img src="../img/routledge.png" width="10%"&gt; [Book website](https://www.routledge.com/The-BUGS-Book-A-Practical-Introduction-to-Bayesian-Analysis/Lunn-Jackson-Best-Thomas-Spiegelhalter/p/book/9781584888499)]

&lt;svg viewBox="0 0 448 512" xmlns="http://www.w3.org/2000/svg" style="height:1em;fill:currentColor;position:relative;display:inline-block;top:.1em;"&gt;
  [ comment ]
  &lt;path d="M448 360V24c0-13.3-10.7-24-24-24H96C43 0 0 43 0 96v320c0 53 43 96 96 96h328c13.3 0 24-10.7 24-24v-16c0-7.5-3.5-14.3-8.9-18.7-4.2-15.4-4.2-59.3 0-74.7 5.4-4.3 8.9-11.1 8.9-18.6zM128 134c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm0 64c0-3.3 2.7-6 6-6h212c3.3 0 6 2.7 6 6v20c0 3.3-2.7 6-6 6H134c-3.3 0-6-2.7-6-6v-20zm253.4 250H96c-17.7 0-32-14.3-32-32 0-17.6 14.4-32 32-32h285.4c-1.9 17.1-1.9 46.9 0 64z"&gt;&lt;/path&gt;
&lt;/svg&gt; &lt;i&gt;Bayesian Methods in Health Economics&lt;/i&gt;, chapters 2, 4 .button[&lt;img src="../img/routledge.png" width="7%"&gt; [Book website (CRC)](https://www-taylorfrancis-com.libproxy.ucl.ac.uk/books/9780429111396)] .button[&lt;i class="fab fa-firefox"&gt;&lt;/i&gt; [Book website](https://gianluca.statistica.it/bmhe)] .button[&lt;i class="fab fa-github"&gt;&lt;/i&gt; [Code](https://github.com/giabaio/BCEA)]
]

---

# Bayes Theorem 


## Updating beliefs with new evidence

- External evidence about unknown quantities `\(\theta\)` **that is not based on current data** is expressed as a **prior** probability distribution `\(p(\theta)\)`
- Evidence from available data `\(y\)` expressed as **sampling distribution**  `\(p(y\mid \theta)\)`

--

Two sources of evidence combined using **Bayes theorem**:

`$$p(\theta \mid y) = p(\theta) \times \frac{p(y\mid \theta)}{p(y)}$$` which is essentially

`$$p(\theta \mid y) \propto p(\theta) \times p(y\mid \theta)$$`

&lt;span style="display:block; margin-top: 2rem ;"&gt;&lt;/span&gt;
&lt;center style="color: #035AA6;"&gt; &lt;b&gt;Posterior&lt;/b&gt; \(\propto\) &lt;b&gt;Prior&lt;/b&gt; \(\times\) &lt;b&gt;Likelihood&lt;/b&gt; &lt;/center&gt;

&lt;!--
`$$\mbox{posterior} \propto \mbox{prior} \times \mbox{likelihood}$$`
--&gt;

&lt;span style="display:block; margin-top: 1rem ;"&gt;&lt;/span&gt;
... Posterior becomes your prior when next piece of evidence arrives...

---

# Bayesian methods for fitting models

## Practical benefits

- Ability to **synthesise** multiple datasets / sources of evidence in coherent manner

- ... Allows complexities about real-world data to be modelled (via MCMC methods)

- Naturally provides **predictions** of future events

- Full allowance for **uncertainty** in conclusions

- **Intuitive** communication
   - express uncertainty by probability statements about unknowns

--

&lt;span style="display:block; margin-top: 1 ;"&gt;&lt;/span&gt;
## Challenges

- Harder to implement than classical "frequentist" methods

- Extra source of information (the prior) &amp;ndash; can be tricky to specify...

---

# Choice of prior distributions

- **"Vague" priors**: typically distributions with big variances (*on a suitable scale*!), eg `mu ~ dnorm(0, 0.00001)` (**NB**: precision=0.00001 `\(\Rightarrow\)` variance=100000!)

.pull-left[
&lt;img src="./img/unnamed-chunk-2-1.png" style="display: block; margin: auto;" width="85%" title="This is a vague prior for the mean of a Normal distribution. It may be perfectly OK to assume that possible values span effectively from -1000 to 1000, depending on the meaning of the underlying variable. Or it may be way too vague (if, for instance, the underlying variable is the difference in the blood pressure between patients treated with a new drug or with placebo"&gt;
]
.pull-right[
&lt;img src="./img/unnamed-chunk-3-1.png" style="display: block; margin: auto;" width="85%" title="This is the same distribution, except this time is defined on the log sd of that Normal distribution. This would imply impossibly large values may still be reasonable, before we observe data and this may have implications especially if the information in the data is very limited"&gt;
]

&lt;span style="display:block; margin-top: -40px ;"&gt;&lt;/span&gt;
- **NB**: Beware of the implications of your prior &amp;ndash; are you assuming too much (unrealistic) variance?    
--

- More recent proposal: **Penalised Complexity** (PC) Priors 
   - Use "default" distributional assumptions; penalise deviations from (= added complexity in comparison to) a simpler, base model &amp;ndash; can be very hard to think about and construct
   - (**Very** technical) &lt;svg viewBox="0 0 384 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M224 136V0H24C10.7 0 0 10.7 0 24v464c0 13.3 10.7 24 24 24h336c13.3 0 24-10.7 24-24V160H248c-13.2 0-24-10.8-24-24zM64 72c0-4.42 3.58-8 8-8h80c4.42 0 8 3.58 8 8v16c0 4.42-3.58 8-8 8H72c-4.42 0-8-3.58-8-8V72zm0 64c0-4.42 3.58-8 8-8h80c4.42 0 8 3.58 8 8v16c0 4.42-3.58 8-8 8H72c-4.42 0-8-3.58-8-8v-16zm192.81 248H304c8.84 0 16 7.16 16 16s-7.16 16-16 16h-47.19c-16.45 0-31.27-9.14-38.64-23.86-2.95-5.92-8.09-6.52-10.17-6.52s-7.22.59-10.02 6.19l-7.67 15.34a15.986 15.986 0 0 1-14.31 8.84c-.38 0-.75-.02-1.14-.05-6.45-.45-12-4.75-14.03-10.89L144 354.59l-10.61 31.88c-5.89 17.66-22.38 29.53-41 29.53H80c-8.84 0-16-7.16-16-16s7.16-16 16-16h12.39c4.83 0 9.11-3.08 10.64-7.66l18.19-54.64c3.3-9.81 12.44-16.41 22.78-16.41s19.48 6.59 22.77 16.41l13.88 41.64c19.77-16.19 54.05-9.7 66 14.16 2.02 4.06 5.96 6.5 10.16 6.5zM377 105L279.1 7c-4.5-4.5-10.6-7-17-7H256v128h128v-6.1c0-6.3-2.5-12.4-7-16.9z"&gt;&lt;/path&gt;&lt;/svg&gt; [paper](https://projecteuclid.org/euclid.ss/1491465621)
   
---

# PC Priors (**optional**)

- Regularise inference while not forcing too strong information
   
- Penalise departure from a "base" model (eg parameter = some fixed value)

   - Prior tends to favour the base model `\(\Rightarrow\)` need fairly strong evidence to move away from it
   
   - Distance between the **base** model `\(\color{red}g(\xi)\)` and an **alternative**, more complex model `\(\color{blue}f(\xi)\)` is measured by

.myblue[   
`$$d(f,g) = \sqrt{2\kld(f,g)} \qquad {\style{font-family:inherit; font-size: 105%; color: black;}{\text{with}}} \qquad \href{https://en.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence}{\kld(f,g)} = \int f(\xi)\log\left(\frac{f(\xi)}{g(\xi)}\right)d\xi$$` 
]

--

   - Penalisation done at a constant rate 

.myblue[   
`$$p(d)=\lambda\exp(-\lambda d)\sim \dexp(\lambda) \qquad {\color{black}\Rightarrow} \qquad p(\xi)=\lambda e^{-\lambda d(\xi)}\left\lvert \frac{\partial d(\xi)}{\partial \xi} \right\rvert$$`
]

   - PC prior defined using probability statements on the model parameters (in the appropriate scale) to determine the value of `\(\lambda\)` using "reasonable" information

---

count: false
# PC Priors (**optional**)

.panelset[
.panel[.panel-name[Example]
.pull-left[

PC prior for a  **precision** `\(\tau=\sigma^{-2}\)`

- Base model: `\(\sigma=0\)`

- Set `\(\Pr(\sigma&gt;\sigma_0)=\alpha,\)` for some constants `\(\sigma_0\)` and `\(\alpha\)`
   
- This implies

   .myblue[
   $$p(\tau) = \frac{\lambda}{2}\tau^{-3/2}\exp\left(-\lambda\tau^{-1/2}\right) \sim \style{font-family:inherit;}{\text{type-2 Gumbel}} $$
   ]
   
   with 
   
   .myblue[
   `$$\lambda=-\frac{\log(\alpha)}{\sigma_0}$$`
   ]

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
- **NB**: The regularising constraint and the actual prior may be defined on **different scales**!
   - In this case, the resulting prior for the standard deviation is
   .myblue[
   `$$p(\sigma)\sim\dexp(\lambda)$$`
   ]
]



.pull-right[

&lt;span style="display:block; margin-top: -30px ;"&gt;&lt;/span&gt;
.center[
eg: setting `\(\sigma_0=2\)` and `\(\alpha=\)` 0.1 gives this  &lt;svg viewBox="0 0 512 512" style="position:relative;display:inline-block;fill:#00acee;height:1.5em;top:10px;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-143.6-28.9L288 302.6V120c0-13.3-10.7-24-24-24h-16c-13.3 0-24 10.7-24 24v182.6l-72.4-75.5c-9.3-9.7-24.8-9.9-34.3-.4l-10.9 11c-9.4 9.4-9.4 24.6 0 33.9L239 404.3c9.4 9.4 24.6 9.4 33.9 0l132.7-132.7c9.4-9.4 9.4-24.6 0-33.9l-10.9-11c-9.5-9.5-25-9.3-34.3.4z"&gt;&lt;/path&gt;&lt;/svg&gt;
]



&lt;center&gt;&lt;img src=./img/pc-prior-1.png width='90%' title=''&gt;&lt;/center&gt;
]
]

.panel[.panel-name[Proof]

Consider the two competing models for some parameter `\(\theta\)` (or data `\(y\)`) as a function of a **precision** `\(\tau\)`
`$${\color{blue}g(\tau)\sim \dnorm(0,\tau=\tau_0\rightarrow \infty)} \qquad \style{font-family:inherit;}{\text{and}} \qquad {\color{red}{f(\tau)\sim \dnorm(0,\tau), \tau\in(0,\infty)}}$$`

&lt;span style="display:block; margin-top: 50px ;"&gt;&lt;/span&gt;

Then 

- `\(\class{myblue}{\kld(f,g)=\frac{1}{2}\frac{\tau_0}{\tau}\left[ 1+\frac{\tau}{\tau_0}\log\left(\frac{\tau}{\tau_0}\right) -\frac{\tau}{\tau_0} \right] \rightarrow \frac{1}{2}\frac{\tau_0}{\tau}}\qquad {\color{black}\style{font-family:inherit;}{\text{if }}} \tau&lt;&lt;\tau_0\)` 

- `\(\class{myblue}{d(\tau)=\sqrt{2\kld(f,g)}=\sqrt{\frac{\tau_0}{\tau}}=\tau_0^{1/2}\tau^{-1/2}}\)`

&lt;span style="display:block; margin-top: 50px ;"&gt;&lt;/span&gt;
Assuming `\(\class{myblue}{p(d)=\lambda\exp(-\lambda d)}\)` then 
   
- `\(\class{myblue}{\left\lvert \frac{\partial d(\tau)}{\partial \tau} \right\rvert = \left\lvert -\frac{1}{2}\tau^{-3/2}\right\lvert = \frac{1}{2}\tau^{-3/2} \qquad \Rightarrow \qquad p(\tau)=\lambda\exp\left[-\lambda d(\tau)\right]\left\lvert \frac{\partial d(\tau)}{\partial \tau} \right\rvert = \frac{\lambda}{2}\tau^{-3/2} \exp\left( -\lambda \tau^{-1/2}\right)}\)`
   
- `\(\class{myblue}{\left\lvert\frac{\partial\tau}{\partial\sigma}\right\lvert=\left\lvert\frac{\partial\sigma^{-2}}{\partial\sigma}\right\lvert=\lvert -2\sigma^{-3}\lvert=2\sigma^{-3} \hspace{-8 mu}\qquad\!\! \Rightarrow \qquad p(\sigma=\tau^{-1/2})=\frac{\lambda}{2}\sigma^3\exp\left(-\lambda\sigma\right)\left\lvert\frac{\partial\tau}{\partial\sigma}\right\lvert=\lambda\exp(-\lambda\sigma)}\)`

]
]

---

# Choice of prior distributions

**Informative priors**: from previous studies, or *elicited* from experts  

- May be difficult to derive, so present sensitivity analysis to different choices    
- If exact choice of vague prior is influential `\(\Rightarrow\)` need more data or informative prior    
- Put prior on "**natural** scale" (as opposed to "*original* scale"!) parameters
   - Easier to include genuine information &amp;ndash; **more on this later!**
   - For instance: place a prior on `\(\theta \sim \style{font-family:inherit;}{\text{Gamma}}(\eta,\lambda)\)` ...

&lt;span style="display:block; margin-top: -24px ;"&gt;&lt;/span&gt;

.pull-left[
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
&lt;img src="./img/unnamed-chunk-5-1.png" style="display: block; margin: auto;" width="90%" title="It's generally hard to elicit prior on mathematical (original) parameters, such as rates, shapes, scales, because they don't have a physical meaning. In this case, we can make priors on the mean and sd of an underlying Gamma distribution, for which we may be able to elicit some relevant information and then turn them into the priors for the actual mathematical parameters that are required to model the Gamma distribution"&gt;
]

.pull-right[
&lt;span style="display:block; margin-top: -40px ;"&gt;&lt;/span&gt;

```r
&gt; # Simulates from priors for mu and sigma
&gt; mu=rlnorm(10000,5.2,.2)
&gt; sigma=rexp(10000,.35)
&gt; # Check interval estimates
&gt; quantile(mu,c(.025,.975))
```

```
    2.5%    97.5% 
122.7655 270.3207 
```

```r
&gt; quantile(sigma,c(.025,.975))
```

```
       2.5%       97.5% 
 0.07482428 10.83392815 
```

```r
&gt; # Simulates from priors for lambda and eta
&gt; lambda=sqrt(mu/sigma^2)
&gt; eta=mu*lambda
```
]

---

name: conjugacy 
# Bayesian modelling of binary data  

Suppose `\(\theta\)` is the true underlying success rate (proportion) of a drug

Assume a `\(\color{#FF851B}{\style{font-family:inherit;}{\text{Beta}}(a, b)}\)` prior distribution  for `\(\theta\)`

`$$\style{font-family:inherit;}{\text{Prior}}   \propto  \theta^{a -1} (1-\theta)^{ b - 1}$$`

If we observe `\(r\)` successes out of `\(n\)` trials, the Binomial distribution means that

$$\style{font-family:inherit;}{\text{Likelihood}}  \propto  \theta^{r} (1-\theta)^{n- r} $$

Then by Bayes theorem
`\begin{align}
\style{font-family:inherit;}{\text{Posterior}} &amp; \propto \theta^{a -1} (1-\theta)^{ b -1}
\theta^{r} (1-\theta)^{n-r}  \\
&amp; \propto \theta^{a+r-1} (1-\theta)^{b+n-r-1}  \\
&amp;  = \style{font-family:inherit;}{\text{Beta}}(a+r,b+  n-r)
\end{align}`

**Beta** prior + **Binomial** data = **Beta** posterior distribution 

&lt;span style="display:block; margin-top: 40px ;"&gt;&lt;/span&gt; 

.content-box-beamer[
### For example...

One of the Covid vaccines has been approved by the [FDA](https://www.fda.gov/) on the back of a Bayesian modelling procedure based on a [Binomial-Beta model](../../practical/02_mcmc/covid))

]

---

# Bayesian modelling of binary data 

### See &lt;a href="../01_Intro/#beta-tricks"&gt;Lecture 1&lt;/a&gt;

So: `\(\displaystyle\left\{ \begin{array}{l} \theta\sim \style{font-family:inherit;}{\text{Beta}}(0,0) \\ y_0\sim \style{font-family:inherit;}{\text{Binomial}}(\theta,n_0)\end{array} \right. \qquad \Rightarrow\qquad  \theta\mid y_0,n_0 \sim \style{font-family:inherit;}{\text{Beta}}(y_0, n_0-y_0)\)`

- Intuition: a `\(\style{font-family:inherit;}{\text{Beta}}(0,0)\)` prior essentially implies you have truly no knowledge whatsoever about the parameter `\(\theta\)` (even **less** than 0 successes in 0 trials!)

--


- **BUT**: this is an **improper** prior, because it does not integrate/sum to 1 (which is a fundamental property of probability distributions)

`$$\theta \sim\style{font-family:inherit;}{\text{Beta}}(0,0) \Rightarrow \int_0^1 p(\theta\mid\alpha=0,\beta=0)d\theta \propto \int_0^1 \frac{1}{\theta(1-\theta)}d\theta \rightarrow \infty$$`

- It is possible that when using improper priors, the posterior also does not integrate to 1, which means you **cannot** make probabilistic assessment of your output &amp;ndash; in that case, Bayesian inference is not valid 

--


- It is still OK to consider this intuition and set up to validate the idea that a Beta prior can be formed to encode a thought experiment with `\(y_0\)` "successes" out of `\(n_0\)` "trials"

---

# Example: Drug

## Recap from last lecture

- Consider a drug to be given for relief of chronic pain


- Experience with similar compounds has suggested that annual response rates between 0.2 and 0.6 could be feasible


- Interpret this as a distribution with mean = 0.4, standard deviation 0.1


- `\(\rightarrow\)` Beta(9.2,13.8) **prior** distribution 

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

We actually do the study and **observe** 15 successes out of 20 patients

- Predict whether `\(&gt;\)` 25 successes in next 40 patients

---

# Example: Drug 
### Prior distribution


&lt;img src="./img/unnamed-chunk-8-1.png" style="display: block; margin: auto;" width="40%" title="Once again, this graph shows the Beta with parameters a=9.2 and b=13.8, encoding the assumption that the prior average for the probability of success for the drug is about 40%, with 95% interval between 20 and 60%"&gt;

Beta(9.2, 13.8) prior distribution supporting response rates between 0.2 and 0.6

---

count: false
# Example: Drug 
### Likelihood


&lt;img src="./img/unnamed-chunk-10-1.png" style="display: block; margin: auto;" width="40%" title="This is the likelihood function (proportional to the sampling distribution) and describes the contribution produced by the observed data"&gt;
Likelihood arising from a Binomial observation of 15 responders out of 20 patients given the drug
`\(\displaystyle\mathcal{L}(\theta) = \theta^{15}(1-\theta)^{(20-15)} \Rightarrow \style{font-family:inherit;}{\text{MLE = 15/20}} =\)` 0.75

---

count: false
# Example: Drug 
### Posterior distribution


&lt;img src="./img/unnamed-chunk-12-1.png" style="display: block; margin: auto;" width="40%" title="The posterior can be fully described by combining the values of the prior parameters a and b with the summary statistics (the observed number of successes y and the observed sample size n. This is an example of conjugate analysis"&gt;
Parameters of the Beta distribution are updated to `\(\displaystyle (a+15, b+20-15) = (24.2, 18.8)\)`: **posterior mean**: 24.2/(24.2+18.8) = 0.56.

---

# Conjugate distributions

This is a case of **conjugate analysis**, when the posterior distribution is in the same **family** as the prior distribution

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
Other examples:

- Gamma prior for **rate** parameter of a Poisson likelihood (for count data, e.g. number of people arriving at emergency department)

- Normal prior for mean of a Normal likelihood

- Gamma prior for precision (1/variance) of a Normal likelihood

&lt;span style="display:block; margin-top: 2rem ;"&gt;&lt;/span&gt;
Advantage: don't need simulation to determine posterior

... But real situations usually more complex `\(\Rightarrow\)` software like `BUGS` needed

---

&lt;style type="text/css"&gt;
/* No background */
.remark-slide tr:nth-child(even) {
  background: #FFFFFF; /*#e2e2f9*/
}
.remark-slide tr:nth-child(odd) {
  background: #FFFFFF; /*#e2e2f9*/
}
/* No borders */
.remark-slide table {
  margin: auto;
  border-top: 0px solid #666;
  border-bottom: 0px solid #666;
}
&lt;/style&gt;

# Drug (continued)

### Learning from data using  Markov chain Monte-Carlo (MCMC) methods

Assume we observed 15 successes out of 20 subjects, and wish to predict whether we will get `\(&gt;\)` 25 successes in next 40 patients

The model can be written  
&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: #035AA6 !important;"&gt; \(\theta\sim\style{font-family:inherit;}{\text{Beta}}(a,b)\) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; prior distribution &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: #035AA6 !important;"&gt; \(y\sim\style{font-family:inherit;}{\text{Binomial}}(\theta,m)\) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; sampling distribution &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: #035AA6 !important;"&gt; \(y_{\rm pred} \sim\style{font-family:inherit;}{\text{Binomial}}(\theta,n)\) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; predictive distribution &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: #035AA6 !important;"&gt; \(P_{\rm crit}=\Pr(y_{\rm pred}\ge n_{\rm crit})\) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; probability of exceeding critical threshold &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


```r
model {
  theta ~ dbeta(a, b)                     # prior distribution
  y ~ dbin(theta, m)                      # sampling distribution
  y.pred ~ dbin(theta, n)                 # predictive distribution
  P.crit &lt;- step(y.pred - n.crit + 0.5)   # =1 if y.pred &gt;= ncrit
                                          # =0 otherwise
}
```

---

# Graphical model

## (Equivalent to BUGS code)

&lt;center&gt;&lt;img src=./img/BUGS_model.png width='600px' title='INCLUDE TEXT HERE'&gt;&lt;/center&gt;

- "Parent" nodes (start of arrow) generate "child" nodes (end of arrow)    
   - data `y` generated by model with parameter `theta`
   - parameter `theta` generated by its "parents" `a,b`
- `BUGS` samples from **posterior** of `theta` &amp;ndash; formed by combining data `y` (**likelihood**) and **prior** parameters `a,b`    
- Evidence flows **up and down** arrows: Knowing about  child `y` tells you about parent `theta`, just as information on `theta` used to predict child `y.pred`

---

# MCMC (Markov chain Monte Carlo)

## Gibbs sampling

- Uses a **Markov chain** (type of random walk &amp;ndash; distribution for the next simulated value depends only on current value)

- Give **initial values** to parameters `\(\theta_1,\ldots,\theta_P\)`

- **Update** parameter values by repeatedly sampling from **full-conditional** posterior distribution of

`$$(\theta_1 \mid  \style{font-family:inherit;}{\text{ current }}\, \theta_p: p \neq 1)$$`
`$$(\theta_2 \mid  \style{font-family:inherit;}{\text{ current }}\, \theta_p: p \neq 2)$$`
`$$\ldots$$`
`$$(\theta_P \mid  \style{font-family:inherit;}{\text{ current }}\, \theta_p: p \neq P)$$`

&amp;emsp; then repeat the cycle until **convergence** (more on this later!)

- Should converge to sampling from **joint posterior** of all unknown quantities `\(\theta_p\)` of interest

- Summarize marginal posterior of `\(\theta_p\)` using converged sample:
   - Use sample mean as estimate of posterior mean
   - Draw smoothed **histogram** to estimate shape of posterior

---

count: false
name: semi-conjugated-example
# MCMC (Markov chain Monte Carlo) 

## Gibbs sampling

**(Convenient) Example: semi-conjugated Normal model**

Assume    
- `\(\displaystyle y_i \stackrel{iid}{\sim} \style{font-family:inherit;}{\text{Normal}}(\mu,\sigma^2), \qquad\)` with `\(i=1,\ldots,n\Rightarrow\)` observed data    
- `\(\displaystyle \mu \mid \sigma^2 \sim \style{font-family:inherit;}{\text{Normal}}(\mu_0,\sigma^2_0) \qquad \left(\style{font-family:inherit;}{\text{e.g. }} \sigma^2_0=\frac{\sigma^2}{\kappa}\right)\qquad \style{font-family:inherit;}{\text{ and }} \qquad\tau=\frac{1}{\sigma^2} \sim \style{font-family:inherit;}{\text{Gamma}}(\alpha_0,\beta_0)\)`    
for fixed `\(\mu_0,\sigma^2_0, \alpha_0, \beta_0\)`

This implies that:
- **Conditionally on** `\(\sigma^2\)`, `\(\mu\)` has a **conjugate prior** (Normal)
- **Marginally**, `\(\tau\)` has a **conjugate prior** (Gamma)    

--

Can prove that under these assumptions 
&lt;span style="display:block; margin-top: -40px ;"&gt;&lt;/span&gt;
`$$\displaystyle\mu\mid\sigma^2,\boldsymbol{y} \sim \style{font-family:inherit;}{\text{Normal}}(\mu_1,\sigma^2_1) \qquad \style{font-family:inherit;}{\text{with: }} \,\mu_1=\sigma^2_1\left(\frac{\mu_0}{\sigma^2_0}+\frac{n\bar{y}}{\sigma^2}\right) \quad \style{font-family:inherit;}{\text{ and }} \quad \sigma^2_1=\left(\frac{1}{\sigma^2_0}+\frac{n}{\sigma^2}\right)^{-1}$$` 
`$$\displaystyle\tau\mid\mu,\boldsymbol{y} \sim \style{font-family:inherit;}{\text{Gamma}}(\alpha_1,\beta_1) \qquad \style{font-family:inherit;}{\text{with: }}\,\alpha_1=\alpha_0+\frac{n}{2}\quad \style{font-family:inherit;}{\text{ and }} \quad \beta_1 = \beta_0 + \frac{1}{2}\sum_{i=1}^n (y_i-\mu)^2$$`

---

name: mcmc0
# MCMC 
### Gibbs sampling &amp;ndash; convergence




&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;

&lt;center&gt;&lt;img src=./img/unnamed-chunk-16-1.png width='55%' title='This graph shows the true joint posterior for the mean and the sd and the starting point (initialisation). In reality, we do not know the true posterior and we want to sample using the MCMC algorithm to approximate it'&gt;&lt;/center&gt;
---

count: false
# MCMC 
### Gibbs sampling &amp;ndash; convergence



&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;

&lt;center&gt;&lt;img src=./img/unnamed-chunk-17-1.png width='55%' title='Then we first simulate from the posterior of the mean given the current value of the sd (we know what this distribution is because of semi-conjugacy) and we move along the x-axis to the new value. Then we simulate from the posterior of the sd given this new value for the mean and make the move to the new point'&gt;&lt;/center&gt;

---

count: false
# MCMC 
### Gibbs sampling &amp;ndash; convergence



&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;

&lt;center&gt;&lt;img src=./img/unnamed-chunk-18-1.png width='55.5%' title='We repeat the process and move first along the x-axis and then along the y-axis to reach the new point'&gt;&lt;/center&gt;

---

count: false
# MCMC 
### Gibbs sampling &amp;ndash; convergence



&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;

&lt;center&gt;&lt;img src=./img/unnamed-chunk-19-1.png width='55%' title='... and we continue to repeat - the graph shows the situation after 10 iterations'&gt;&lt;/center&gt;

---

count: false
# MCMC 
### Gibbs sampling &amp;ndash; convergence



&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;

&lt;center&gt;&lt;img src=./img/unnamed-chunk-20-1.png width='55%' title='... and after 30 iterations. In this case, we can see that we are indeed covering a lot of the target distribution'&gt;&lt;/center&gt;

---

count: false
# MCMC 
### Gibbs sampling &amp;ndash; convergence



&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;

&lt;center&gt;&lt;img src=./img/unnamed-chunk-21-1.png width='55%' title='... After 500 iterations, we've completely covered the target portion of the parametric space, although we started off from a very distant point (marked as 0)'&gt;&lt;/center&gt;

---

count: false
# MCMC 
### Gibbs sampling &amp;ndash; convergence

&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;

&lt;center&gt;&lt;img src=./img/convergence.png width='600px' title='To assess convergence, we can start 2 independent chains from very difference points. If the process works, after a while they will go on top of each other, meaning that both are visiting the same part of the parametric space (the target distribution'&gt;&lt;/center&gt;

---

count: false
exclude: true
# MCMC 
### Gibbs sampling &amp;ndash; convergence

&lt;center&gt;&lt;img src=./img/iter1_1.png width='58%' title=''&gt;&lt;/center&gt;
---

count: false
exclude: true
# MCMC 
### Gibbs sampling &amp;ndash; convergence

&lt;center&gt;&lt;img src=./img/iter1_2.png width='58%' title='INCLUDE TEXT HERE'&gt;&lt;/center&gt;
---

count: false
exclude: true
# MCMC .subtitle[Gibbs sampling &amp;ndash; convergence]

&lt;center&gt;&lt;img src=./img/iter1_3.png width='58%' title=''&gt;&lt;/center&gt;

---

count: false
exclude: true
# MCMC .subtitle[Gibbs sampling &amp;ndash; convergence]

&lt;center&gt;&lt;img src=./img/iter1_4.png width='58%' title='INCLUDE TEXT HERE'&gt;&lt;/center&gt;

---

name: caterpillar
# Checking convergence: monitoring samples

.pull-left[
&lt;center&gt;&lt;img src=./img/trace_all_draft.jpg width='380px' title='These are examples of traceplots for hypothetical analyses. We would like to see a 'fat hairy caterpillar' indicating convergence'&gt;&lt;/center&gt;
]
.pull-right[

- Convergence (or lack of) can be apparent from one chain
   - Want **fat hairy caterpillars** (b) &amp;ndash; not twisting  snakes

- Chain may have converged but be slow to **mix** (c)
   - Run chain for longer (d) to get more precise estimates

- One chain may get "stuck" in some area due to extreme initial value (a)
   - Run multiple chains from different initial values (e): check all end up in same place

- Parameterisation may make a big difference (see manual)
]

---

# Formal convergence tests

Formal diagnostics exist to check if multiple chains end up in essentially same place, eg Brooks-Gelman-Rubin (often referred to as Potential Scale Reduction, PSR) statistic

&lt;span style="display:block; margin-top: 3rem ;"&gt;&lt;/span&gt;

.pull-left[
&lt;center&gt;&lt;img src=./img/bgr.jpg width='500px' title='The PSR statistic shows the ratio of the within to between chain variation. When these two things are similar, it means that the chains are varying more or less at the same rate and over the same part of the parametric space, to indicate convergence'&gt;&lt;/center&gt;
]

.pull-right[

- Based on ratio of between to within variances of multiple chains: (ANOVA)

- `OpenBUGS` produces plots of
   - Average 80% interval within-chains (blue) and pooled 80% interval between-chains (green)
   - Ratio green/blue should converge to 1 (red) as iterations increase

]

&lt;span style="display:block; margin-top: 1em ;"&gt;&lt;/span&gt;

- `coda` and `R2OpenBUGS` packages for `R` contain many other diagnostics    
- **NB** This is only a heuristic measure &amp;ndash; more recent work suggests alternative ways    
   - [https://avehtari.github.io/rhat_ess/rhat_ess.html](https://avehtari.github.io/rhat_ess/rhat_ess.html)      
   - [http://cknudson.com/Presentations/BayesComp2020.pdf](http://cknudson.com/Presentations/BayesComp2020.pdf)

---

# How many iterations after convergence?

- How many significant figures do you need in your estimates: **your decision**

- Easiest strategy: run chains until the posterior summaries of interest don't change

- Monte Carlo Standard Error (MCSE) says how accurate the posterior mean is

- **Autocorrelated** samples need to be longer to get the same accuracy, compared to independent samples
   - Some theory (Raftery &amp; Lewis, see `BUGS` Book for further details) suggests that to get 95% posterior quantiles with true 94.5-95.5% coverage need MCSE/posterior SD &lt; 0.01, or **effective sample size** &gt; 4000

---

# Supplying data to `BUGS`


1. Rectangular format &amp;ndash; traditional "spreadsheet"-shaped data

```r
n[] r[]
 47  0
148 18
...
360 24
END
```


&lt;ol style="counter-reset: my-awesome-counter 1;"&gt;
&lt;li&gt;&lt;span style="font-family:Inconsolata;"&gt;R&lt;/span&gt; / &lt;span style="font-family:Inconsolata;"&gt;S-Plus&lt;/span&gt; style "lists"&lt;/li&gt;&lt;/ol&gt;

```r
list(N=12,n = c(47,148,119,810,211,196,
          148,215,207,97,256,360),
     r = c(0,18,8,46,8,13,9,31,14,8,29,24))
```


List format more flexible, can specify constants alongside data &amp;ndash; useful for complex multilevel models with variables of different lengths

(**NB** If using `R` interfaces to `BUGS`, just supply data in `R` list object &amp;ndash; see later...)

---

# Supplying data to `BUGS`

## Drugs example

Data can be written after the model description, or held in a separate `.txt` or `.odc` file

```r
list(
  a=9.2, b=13.8,    # prior parameters
  y=15,             # number of successes 
  m=20,             # number of trials
  n=40,             # future number of trials
  ncrit=25          # critical value of future successes
)
```

Alternatively, put all data and constants into model description: 

```r
model{
  theta ~ dbeta(9.2, 13.8)      # prior distribution
  y ~ dbin(theta, 20)           # sampling distribution
  y.pred ~ dbin(theta, 40)      # predictive distribution
  P.crit &lt;- step(y.pred - 24.5) # =1 if y.pred &gt;= ncrit,
                                # =0 otherwise
  y &lt;- 15                       # observed successes
}
```

---

# Initial values

- `BUGS` simulates from posterior &amp;ndash; combination of prior and evidence from data &amp;ndash;  by MCMC

- Posterior **unknown** to start with &amp;ndash; need to **initialize** simulation

- `BUGS` can automatically generate initial values for the simulation using `gen.inits` &amp;ndash; simulates from the prior
   - We have seen this in the practical for forward sampling

- Fine if have informative prior information

- If have fairly "vague" priors, better to provide reasonable values in an initial-values list

Initial values list can be after model description or in a separate file

```r
list(theta=0.1)
```

---

# `OpenBUGS` output and exact answers


```r
node       mean     sd     MC error   2.5%   median   97.5%  start sample
theta     0.5633  0.07458  4.292E-4   0.4139  0.5647  0.7051  1001 30000
y.pred   22.52    4.278    0.02356   14.0    23.0    31.0     1001 30000
P.crit    0.3273  0.4692   0.002631   0.0     0.0     1.0     1001 30000
```

Exact answers from conjugate analysis:

- `\(\theta\)`:  mean 0.563 and standard deviation  0.075

- `\(Y^{\rm pred}\)`:  mean 22.51 and standard deviation  4.31

- Probability of at least 25:  0.329

MCMC results are within Monte Carlo error of the true values

.small[.alignright[&lt;svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;" xmlns="http://www.w3.org/2000/svg"&gt;  &lt;path d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm-28.9 143.6l75.5 72.4H120c-13.3 0-24 10.7-24 24v16c0 13.3 10.7 24 24 24h182.6l-75.5 72.4c-9.7 9.3-9.9 24.8-.4 34.3l11 10.9c9.4 9.4 24.6 9.4 33.9 0L404.3 273c9.4-9.4 9.4-24.6 0-33.9L271.6 106.3c-9.4-9.4-24.6-9.4-33.9 0l-11 10.9c-9.5 9.6-9.3 25.1.4 34.4z"&gt;&lt;/path&gt;&lt;/svg&gt; [Next lecture](../03_Intro_HE/index.html)]]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url("../assets/beamer/UCL-beamer.png");
  background-size: 14% 7%;
  background-repeat: no-repeat;
  position: absolute;
  top:  2.625%; /* 2.65%em */
  left: 85%;
  width: 100%;
  height: 100%;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)' +
    ':not(.thankyou-michelle)' +
    ':not(.thankyou-barney)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>


<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
