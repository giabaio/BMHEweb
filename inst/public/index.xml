<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian methods in health economics</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Bayesian methods in health economics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 24 Jun 2022 14:15:00 +0000</lastBuildDate>
    <image>
      <url>/media/logo_hu1963fc62d5b8fe503cce6274f5cb00c3_9765_300x300_fit_lanczos_3.png</url>
      <title>Bayesian methods in health economics</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Practical 1. Monte Carlo in BUGS - SOLUTIONS</title>
      <link>/practical/01_monte-carlo/solutions/</link>
      <pubDate>Mon, 20 Jun 2022 11:15:00 +0000</pubDate>
      <guid>/practical/01_monte-carlo/solutions/</guid>
      <description>


&lt;div id=&#34;coins-example-see-lectures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. “Coins” example (see lectures)&lt;/h2&gt;
&lt;p&gt;The first thing we need to do is to run the programme from the script
provided in the file &lt;code&gt;coins-script.odc&lt;/code&gt;. The commands in the script make
automatic the process of pointing-and-clicking that we would otherwise
need to perform the analysis from &lt;tt&gt;BUGS&lt;/tt&gt;. We
need to be careful in telling &lt;tt&gt;BUGS&lt;/tt&gt; what the
“working directory” is. In the script, the default path is set as
&lt;tt&gt;c:/bayes-hecourse/1_monte-carlo&lt;/tt&gt;, which grants a couple of comments.&lt;/p&gt;
&lt;p&gt;Firstly, you may not have saved the script file onto this directory
(which in fact may not even exist in your computer!). So you need to
make sure you update this to the actual path to the folder in which the
file &lt;code&gt;coins-script.odc&lt;/code&gt; is stored, or else
&lt;tt&gt;BUGS&lt;/tt&gt; will complain that it can’t find the
file. Secondly, the default text uses &lt;tt&gt;Unix&lt;/tt&gt; notation, where folders are
separated using a “&lt;code&gt;/&lt;/code&gt;” symbol. Under other operating systems (notably
&lt;tt&gt;MS Windows&lt;/tt&gt;), this is no longer valid and paths are indicated using a
“&lt;code&gt;\&lt;/code&gt;” symbol. You then need to be careful in providing the correct
string. For instance, suppose your file under &lt;tt&gt;MS Windows&lt;/tt&gt; is saved in
the directory &lt;tt&gt;N:\DesktopSetting\Desktop\STAT0019&lt;/tt&gt;, then you would need
to copy and paste this string into the model script.&lt;/p&gt;
&lt;p&gt;In order to run the script, you need to click anywhere on the file
&lt;code&gt;coins-script.odc&lt;/code&gt; (assuming you have opened it in
&lt;tt&gt;BUGS&lt;/tt&gt;) and then click on the menu &lt;tt&gt;Model
&lt;svg viewBox=&#34;0 0 448 512&#34; style=&#34;height:1em;position:relative;display:inline-block;top:.1em;&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt; &lt;path d=&#34;M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Script&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;NB&lt;/strong&gt;: we use the notation
&lt;tt&gt;Command1 &lt;i class=&#34;fas fa-arrow-right&#34;&gt;&lt;/i&gt; Command2&lt;/tt&gt; to indicate that you need to click on the menu
labelled as &lt;tt&gt;Command1&lt;/tt&gt; and then on the sub-menu labelled as &lt;tt&gt;Command2&lt;/tt&gt;).
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Unlike the older version (&lt;tt&gt;WinBUGS&lt;/tt&gt;), &lt;tt&gt;OpenBUGS&lt;/tt&gt; does not open the log
file automatically. Thus, if there is an error, only a message in the
bottom-left corner of the window will appear (but it may not be very
noticeable). To open the log file you can click on &lt;tt&gt;Info &lt;svg viewBox=&#34;0 0 448 512&#34; style=&#34;height:1em;position:relative;display:inline-block;top:.1em;&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt; &lt;path d=&#34;M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;
Open Log&lt;/tt&gt;. If you do this, before running the script, remember to click
back on the part of the window occupied by the script file and then
click on &lt;tt&gt;Model &lt;svg viewBox=&#34;0 0 448 512&#34; style=&#34;height:1em;position:relative;display:inline-block;top:.1em;&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt; &lt;path d=&#34;M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Script&lt;/tt&gt;. In this case, you will see on
the log file messages from &lt;tt&gt;OpenBUGS&lt;/tt&gt; informing you of what is
happening. For instance, if all works, these messages will be printed.&lt;/p&gt;
&lt;pre echo=&#34;TRUE,eval=FALSE&#34;&gt;&lt;code&gt;model is syntactically correct
model compiled
initial values generated, model initialized
model is updating
1000 updates took 0 s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, two more windows will automatically open, showing the
output of the &lt;tt&gt;BUGS&lt;/tt&gt;procedure. The first one
shows the “Node statistics, i.e. the summary of the simulations
performed for the node monitored (you can check that the script
instructs &lt;tt&gt;BUGS&lt;/tt&gt;to monitor the nodes &lt;code&gt;Y&lt;/code&gt; and
&lt;code&gt;P8&lt;/code&gt; with the commands&lt;/p&gt;
&lt;pre echo=&#34;TRUE,eval=FALSE&#34;&gt;&lt;code&gt;samplesSet(Y)
samplesSet(P8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second window shows the graphs of the posterior densities for the
nodes monitored. In this case, you will see the histograms of &lt;code&gt;P8&lt;/code&gt; and
&lt;code&gt;Y&lt;/code&gt;. The latter represents the predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, the
number of coin tosses showing up “heads”. In line with the summary
statistics, this distribution has a mean of approximately 5 and most of
the probability mass (in fact, 95%) is between 2 and 8. As for &lt;span class=&#34;math inline&#34;&gt;\(P_8\)&lt;/span&gt;,
which represents the probability of observing 8 or more “heads”, the
histogram has mostly 0s, to indicate that in all the simulations it is
most likely that this event does &lt;em&gt;not&lt;/em&gt; happen, than it does. Again, this
is in line with the summary statistics, showing that the sample mean
computed from the simulations
&lt;span class=&#34;math display&#34;&gt;\[\mbox{E}[P_8] = \frac{1}{S}\sum_{s=1}^S P_8\]&lt;/span&gt; is just
above 5%. Notice that because of how the node &lt;code&gt;P8&lt;/code&gt; is defined in the
model code (see Lecture notes), it either takes value 0 (if the event is
not true) or 1 (otherwise). Thus the mean effectively represents the
estimated probability that the event (i.e. at least 8 “heads”) is true.&lt;/p&gt;
&lt;p&gt;We can also run the model using the point-and-click facility of
&lt;tt&gt;BUGS&lt;/tt&gt; (this can be done by simply following
all the steps in the practical question). The output will be identical
to the one just described.&lt;/p&gt;
&lt;p&gt;Finally, we can modify the original model code to encode the assumption
that the coin is actually unbalanced (with a probaility of “heads” equal
to 0.7). To do so, we simply need to modify the model code as follows.&lt;/p&gt;
&lt;pre echo=&#34;TRUE,&#34; eval=&#34;FALSE&#34;&gt;&lt;code&gt;model {
  Y ~ dbin(0.7, 30)
  P15 &amp;lt;- step(15.5 - Y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then re-run the &lt;tt&gt;BUGS&lt;/tt&gt;procedure. Of course
you can either overwrite the original file and then run the script, or
save the new code to another file, update the command &lt;tt&gt;modelCheck&lt;/tt&gt; to
provide the new file name, or simply use the point-and-click procedure
(this latter option does not require you to save the file).&lt;/p&gt;
&lt;p&gt;The predictive distribution for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; in this new setting is centered
around 20, with most of the mass between 16 and 26, while the
probability of observing at most 15 “heads” is estimated to be around
1.7%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drug-example-see-lectures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. “Drug” example (see lectures)&lt;/h2&gt;
&lt;p&gt;To run the model, you can again use the script provided or simply
point-and-click to run the model code file, also provided. As discussed
in the class, the model code instructs
&lt;tt&gt;BUGS&lt;/tt&gt;on the assumptions underlying the model.&lt;/p&gt;
&lt;pre echo=&#34;TRUE,eval=FALSE&#34;&gt;&lt;code&gt;model{
  theta ~ dbeta(9.2, 13.8)
  y ~ dbin(theta, 20)
  P.crit &amp;lt;- step(y - 14.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the model is pretty much the same as in the previous
example — except that in this case, we are not willing to assume a
“fixed” and known value for the probability of the event of interest
(e.g. “heads” in the previous example, or that the drug is successful,
in the current one). So, instead of assigning it a value (e.g. 0.5 or
0.7), we describe our knowledge using a distribution. Given that we
believe the “true” success rate to be between 0.2 and 0.6, we can
express this using a Beta distribution with parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha=9.2\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(\beta=13.8\)&lt;/span&gt;. As discussed in class, by the mathematical properties of
the Beta distribution, this implies that we believe
&lt;span class=&#34;math display&#34;&gt;\[\mbox{E}[\theta] = \frac{\alpha}{\alpha+\beta}=\frac{9.2}{23}=0.4\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[\mbox{Var}[\theta] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}=0.01\]&lt;/span&gt;
(which implies &lt;span class=&#34;math inline&#34;&gt;\(\mbox{sd}[\theta]=\sqrt{0.001}=0.1\)&lt;/span&gt;, which in turn
implies that, &lt;em&gt;approximately&lt;/em&gt;, 95% of the mass for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is included
in &lt;span class=&#34;math inline&#34;&gt;\(0.4\pm 2(0.1)=[0.2; 0.6]\)&lt;/span&gt;, as required).&lt;/p&gt;
&lt;p&gt;In the next part of the exercise, you are required to modify the prior
distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; to a Uniform(0,1). This effectively means that
you are not implying any specific prior knowledge on the “true” success
rate, apart from the fact that it can take a value between 0 and 1.
Graphically, a Uniform distribution in (0,1) looks like in the following
graph.
&lt;img src=&#34;/practical/01_monte-carlo/solutions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;35%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This prior distribution is “non informative” in the sense that it does
not provide much information or clue as to what value of the probability
of success is more likely to generate the data. Consequently, it is not
surprising that the resulting predictive distribution for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is spread
over the entire range of possible values (with a mean of around 10 and a
95% interval covering 0 to 20 — i.e. the whole range). Basically, if all
you’re prepared to say before seeing any data is that the true success
rate is something between 0 and 1, then the prediction of the number of
successes cannot be anything else than anything between 0 (no
successes) and 20 (all patients are cured)…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-functions-of-random-quantities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Simulating functions of random quantities&lt;/h2&gt;
&lt;p&gt;One of the main features of Bayesian analysis using simulation methods
(such as those underpinning &lt;tt&gt;BUGS&lt;/tt&gt;) is that it
is possible to obtain, almost as a simple by-product of the estimation
procedure, simulations for any function of the main model parameters. We
shall see later that this is very helpful when using complex models,
e.g. for economic evaluation.&lt;/p&gt;
&lt;p&gt;In this case, we need to write a model for a variable
&lt;span class=&#34;math inline&#34;&gt;\(y \sim \mbox{Normal}(\mbox{mean}=0, \mbox{ sd}=1)\)&lt;/span&gt;. As suggested in the
practical exercise, we need to be careful and remember that
&lt;tt&gt;BUGS&lt;/tt&gt;parameterise the Normal distribution in
terms of mean and &lt;strong&gt;precision&lt;/strong&gt; (=1/variance). If we are imposing a sd
of 1, then this is irrelevant because if &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt;, then
&lt;span class=&#34;math inline&#34;&gt;\(\mbox{variance} = \mbox{precision} = \frac{1}{\mbox{variance}} = 1\)&lt;/span&gt;.
The model can be written in &lt;tt&gt;BUGS&lt;/tt&gt;language as&lt;/p&gt;
&lt;pre echo=&#34;TRUE,eval=FALSE&#34;&gt;&lt;code&gt;model {
  y ~ dnorm(0, 1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and run using the same procedure described above (for such a simple
model, we can just use the point-and-click approach).&lt;/p&gt;
&lt;p&gt;This produces simulations from a Normal variable with 0 mean and unit
variance. The summary statistics of this distribution are as follows
(your output may vary, due to pseudo-random variation).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    mean       sd     2.5%      25%      50%      75%    97.5% 
-0.00153  1.00520 -1.95402 -0.67965  0.00295  0.66825  1.98802 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;based on a sample of 10000 simulations. The resulting density looks like
the following (you can produce the graph in
&lt;tt&gt;BUGS&lt;/tt&gt;using the &lt;tt&gt;density&lt;/tt&gt; button in the
&lt;tt&gt;Sample monitor tool&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/practical/01_monte-carlo/solutions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;35%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we want to assume that
&lt;span class=&#34;math inline&#34;&gt;\(y\sim\mbox{Normal}(\mbox{mean}=1, \mbox{ sd=2})\)&lt;/span&gt;, then this implies
that precision &lt;span class=&#34;math inline&#34;&gt;\(=1/2^2=0.25\)&lt;/span&gt;. So we need to modify the model code to&lt;/p&gt;
&lt;pre echo=&#34;TRUE,eval=FALSE&#34;&gt;&lt;code&gt;model {
  y ~ dnorm(1, 0.25)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can run this new model and obtain summaries and graphs for the
predictive distribution of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; (notice that the term “predictive” here
highlights the fact that we have not actually observed &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; — we might in
the future, but not just yet. In fact the most appropriate term here
would be “&lt;em&gt;prior&lt;/em&gt; predictive” distribution, again to highlight the fact
that this is only based on our prior knowledge about the parameters,
i.e. mean and standard deviation).&lt;/p&gt;
&lt;p&gt;The output of the analysis is as follows (again, if you see slight
differences, these would be down to simulation error).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  mean     sd   2.5%    25%    50%    75%  97.5% 
 0.997  2.010 -2.909 -0.359  1.006  2.336  4.975 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/01_monte-carlo/solutions_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;35%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is reasonable, the second case (with a larger sd) gives estimates
that are more spread out (as the variability is higher, by definition).&lt;/p&gt;
&lt;p&gt;Now, we are asked to create a new variable &lt;span class=&#34;math inline&#34;&gt;\(Z=Y^3\)&lt;/span&gt; and estimate its
distribution. We also need to compute the probability that &lt;span class=&#34;math inline&#34;&gt;\(Z&amp;gt;10\)&lt;/span&gt;. This
is extremely easy in &lt;tt&gt;BUGS&lt;/tt&gt; — the model code
needs to be modified simply to the following.&lt;/p&gt;
&lt;pre echo=&#34;TRUE,eval=FALSE&#34;&gt;&lt;code&gt;model {
  y ~ dnorm(1, 0.25)
  z &amp;lt;- pow(y, 3)
  # Alternatively, we can write
  # z &amp;lt;- y * y * y
  above10 &amp;lt;- step(z - 10)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          mean    sd   2.5%     25%  50%   75%  97.5%
y        0.997  2.01  -2.91 -0.3593 1.01  2.34   4.98
z       13.089 39.75 -24.61 -0.0464 1.02 12.75 123.20
above10  0.283  0.45   0.00  0.0000 0.00  1.00   1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/01_monte-carlo/solutions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;35%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/practical/01_monte-carlo/solutions_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;35%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 2. Markov Chain Monte Carlo - SOLUTIONS</title>
      <link>/practical/02_mcmc/solutions/</link>
      <pubDate>Mon, 20 Jun 2022 14:00:00 +0000</pubDate>
      <guid>/practical/02_mcmc/solutions/</guid>
      <description>


&lt;div id=&#34;understanding-gibbs-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Understanding Gibbs sampling&lt;/h2&gt;
&lt;p&gt;Most of this exercise is actually fairly easy as it is performed directly in &lt;tt&gt;Excel&lt;/tt&gt;. In fact, the model does not need MCMC to be analysed and we could obtain analytic estimates for the two variables &lt;span class=&#34;math inline&#34;&gt;\((y_1,y_2)\)&lt;/span&gt;. Some clarifications are perhaps useful, though.&lt;/p&gt;
&lt;p&gt;The main point is perhaps to realise that correlation across the model parameters may be a crucial factor in terms of convergence and the speed at which this may be reached when running MCMC. This is due to the fact that larger correlation implies that the joint distribution of the parameters has a ``narrower’’ shape. The figure below clarifies this point in the simplest two-dimensional, bi-variate Normal case — but the situation generalises to higher dimensions and different distributional shapes.&lt;/p&gt;
&lt;div style=&#34;overflow:hidden;&#34;&gt;
&lt;div style=&#34;float: left;margin-left: 20px;margin-top: 0px;margin-bottom:0px; width: 48%;&#34;&gt;
&lt;img src=&#34;/practical/02_mcmc/solutions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;80%&#34; /&gt;
&lt;center&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Low correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho=0.08\)&lt;/span&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div style=&#34;float: left;margin-left: 20px;margin-top: 0px;margin-bottom:0px; width: 48%;&#34;&gt;
&lt;img src=&#34;/practical/02_mcmc/solutions_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;80%&#34; /&gt;
&lt;center&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;High correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho=0.98\)&lt;/span&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The graphs show the first 30 simulations in a Gibbs sampling process for the same set up as in the &lt;tt&gt;Excel&lt;/tt&gt; spreadsheet; the “true” underlying mean is &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta=(0,0)\)&lt;/span&gt; and the “true” underlying variances are &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\sigma^2=(1,1)\)&lt;/span&gt;. In the plot (panel a) we assume correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho=0.08\)&lt;/span&gt; (i.e.~very low). This generates a joint distribution for (y_1,y_2)$ that is like a “ball”, with a wide radius. This means that even a relatively low number of simulations (and more importantly long moves across the parametric space) is almost sufficient to cover the underlying true joint distribution.&lt;/p&gt;
&lt;p&gt;In panel (b), however, because we are assuming a large correlation (&lt;span class=&#34;math inline&#34;&gt;\(\rho=0.98\)&lt;/span&gt;), then the resulting distribution is very narrow and thus 30 simulations cannot cover the target portion of the space. In fact, in this case, the initial values make a difference — starting from values that are far awy from the bulk of the target distribution will generally mean that a longer running time is necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-in-bugs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MCMC in &lt;tt&gt;BUGS&lt;/tt&gt;&lt;/h2&gt;
&lt;p&gt;As discussed in the lecture, the model is very similar to the one seen in Practical 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model {
   theta    ~ dbeta(a,b)         # prior distribution
   y        ~ dbin(theta,m)      # sampling distribution
   y.pred   ~ dbin(theta,n)      # predictive distribution
   P.crit  &amp;lt;- step(y.pred - ncrit + 0.5) # =1 if y.pred &amp;gt;= ncrit,
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we model our uncertainty on the underlying success rate for the drug using an informative, conjugate Beta prior distribution, which implies a mean of around 0.4 and a 95% interval spanning from 0.2 to 0.6. This time, however, we do have observed data on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; — effectively now the objective of our analysis is estimate the &lt;em&gt;posterior&lt;/em&gt; distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(\theta\mid y)\)&lt;/span&gt; and to predict the outcome of the next round of the trial, when we assume to observed &lt;span class=&#34;math inline&#34;&gt;\(n=40\)&lt;/span&gt; patients alltogether.&lt;/p&gt;
&lt;p&gt;Notice that in the model code, we are making the assumption that the new patients are effectively drawn from the same population of those that we have already observed. This is expressed by assuming that they have the same distributional form — both &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y.pred&lt;/code&gt; are modelled using a sampling distribution that is assumed to be Binomial&lt;span class=&#34;math inline&#34;&gt;\((\theta, \mbox{sample size})\)&lt;/span&gt; and effectively all that we assume to vary is the sample size (&lt;span class=&#34;math inline&#34;&gt;\(m=20\)&lt;/span&gt; for the observed &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n=40\)&lt;/span&gt; for the new patients).&lt;/p&gt;
&lt;p&gt;The actual &lt;code&gt;.odc&lt;/code&gt; file contains &lt;em&gt;all&lt;/em&gt; the relevant bits that &lt;tt&gt;BUGS&lt;/tt&gt; needs, in one place. In addition to the model code, it also contains the data, in the following format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list(
a = 9.2, b = 13.8, # prior parameters
y = 15,            # number of successes
m = 20,            # number of trials
n = 40,            # future number of trials
ncrit = 25)        # critical value of future successes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a “list” (we shall see a lot more of this when we start working in &lt;tt&gt;R&lt;/tt&gt;). As mentioned in the class, it is generally a good idea to keep the code general and then pass values that can be changed separately, rather than “hard-coding” them in the model code. In this simple case, however, you could “fix” the values of the parameters into the model code, as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model {
   theta ~ dbeta(a,b)                   # prior distribution
   y ~ dbin(theta,m)                    # sampling distribution
   y.pred ~ dbin(theta,n)               # predictive distribution
   P.crit &amp;lt;- step(y.pred - ncrit + 0.5) # =1 if y.pred &amp;gt;= ncrit,
   y &amp;lt;- 15                              #  0 otherwise
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you wanted to use this latter format, you would not need to pass &lt;tt&gt;BUGS&lt;/tt&gt; any data, because all the relevant numbers would be included in the model code. Notice &lt;tt&gt;BUGS&lt;/tt&gt; accepts a `&lt;code&gt;double&#39;&#39; definition for the node&lt;/code&gt;y` — first as a random variable associated with a Binomial distribution and then as a fixed number (15, indicating the observed number of successess in the current trial). In general, however, you need to be careful — for example it is not possible to define a node twice with the same nature (i.e. twice as observed data, or twice with two different probabilistic assignments).&lt;/p&gt;
&lt;p&gt;Finally, the &lt;code&gt;odc&lt;/code&gt; file contains the list for the initial values. In a &lt;tt&gt;BUGS&lt;/tt&gt; file, all variables that are associated with a probabilistic statement (i.e. a “&lt;code&gt;~&lt;/code&gt;” symbol after the variable name) &lt;strong&gt;and&lt;/strong&gt; are &lt;em&gt;not&lt;/em&gt; observed need to be initialised so that &lt;tt&gt;BUGS&lt;/tt&gt; can run the Gibbs sampler. So, in this particular case, although &lt;code&gt;y&lt;/code&gt; is defined as a random variable, because it is also observed, then there is no uncertainty about its value and so we should not initialise it. Conversely, although &lt;code&gt;y.pred&lt;/code&gt; has exactly the same nature as &lt;code&gt;y&lt;/code&gt;, because it is not observed, then we need to initialise it. In the &lt;code&gt;odc&lt;/code&gt; file we actually let &lt;tt&gt;BUGS&lt;/tt&gt; do this (effectively when clicking the &lt;code&gt;gen init&lt;/code&gt; button in the &lt;code&gt;Specification tool&lt;/code&gt;), but we do provide initial values for the other unobserved, random node, &lt;code&gt;theta&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the same reason mentioned above, &lt;tt&gt;BUGS&lt;/tt&gt; does not let us monitor the node &lt;code&gt;y&lt;/code&gt; — it has been already observed, so, as said above, there is nothing more to learn about it. It can be only used to learn about &lt;code&gt;theta&lt;/code&gt;, &lt;code&gt;y.pred&lt;/code&gt; and &lt;code&gt;P.crit&lt;/code&gt;, which will all be associated with a posterior distribution, given the evidence provided by the fact that &lt;span class=&#34;math inline&#34;&gt;\(y=15\)&lt;/span&gt; out of &lt;span class=&#34;math inline&#34;&gt;\(m=20\)&lt;/span&gt; trials.&lt;/p&gt;
&lt;p&gt;When we actually run the model, we can monitor the relevant nodes and produce the following summary statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;         mean     sd   2.5%    25%    50%    75%  97.5% Rhat n.eff
y.pred 22.578 4.2649 14.000 20.000 23.000 25.000 31.000    1 10000
theta   0.563 0.0749  0.414  0.512  0.563  0.614  0.706    1 10000
P.crit  0.330 0.4701  0.000  0.000  0.000  1.000  1.000    1 10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These indicate that, given current evidence, we revise our uncertainty about the true success rate of the drug from a distribution centered on 0.4 and with most of the mass in [0.2; 0.6] to another centered around 0.563 and with most of the probability mass included in [0.414; 0.706].&lt;/p&gt;
&lt;p&gt;In fact, because we are using a Beta prior combined with a Binomial sampling distribution, then the model is &lt;em&gt;conjugated&lt;/em&gt; and so we &lt;strong&gt;know&lt;/strong&gt; that the form of the posterior distribution is another Beta (as seen in class), where the update from prior to posterior only involves the value of its parameters. This implies that we don’t need to worry about convergence of the MCMC model. This is confirmed by the inspection of the traceplots.&lt;/p&gt;
&lt;div style=&#34;overflow:hidden;&#34;&gt;
&lt;div style=&#34;float: left;margin-left: 20px;margin-top: 0px;margin-bottom:0px; width: 48%;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/solutions_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;float: left;margin-left: 20px;margin-top: 0px;margin-bottom:0px; width: 48%;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/solutions_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In all cases, already at iteration 1 of the MCMC the two chains that we’ve run in parallel are on top of each other and evidently visiting the same portion of the parametric space.&lt;/p&gt;
&lt;p&gt;As for &lt;code&gt;y.pred&lt;/code&gt; and &lt;code&gt;P.crit&lt;/code&gt; we can see that the expected number of successes in the next phase of the trial is around 23 and the probability of exceeding the critical threshold (25) is 32.97.&lt;/p&gt;
Notice finally that we can also look at the convergence statistics, &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; (&lt;code&gt;Rhat&lt;/code&gt;, the Potential Scale Reduction, also known as “Gelman-Rubin Statistic”) and the effective sample size &lt;span class=&#34;math inline&#34;&gt;\(n_{\text{eff}}\)&lt;/span&gt; (&lt;code&gt;n.eff&lt;/code&gt;). The former has a value of 1 for all the nodes — again, given the fact that the model is conjugate, this is not surprising. The model does not present problems in terms of autocorrelation. The following graphs show the autocorrelation plot for the monitored nodes.
&lt;div style=&#34;overflow:hidden;&#34;&gt;
&lt;div style=&#34;float: left;margin-left: 20px;margin-top: 0px;margin-bottom:0px; width: 48%;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/solutions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;float: left;margin-left: 20px;margin-top: 0px;margin-bottom:0px; width: 48%;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/solutions_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The way in which we need to interpret these is to look at the level of autocorrelation across different lags. These indicate how much correlation exists across consecutive, lagged simulations in the MCMC procedure. It is expected that consecutive iterations be relatively correlated (if you recall how the Gibbs sampling is constructed, you should realise that the &lt;span class=&#34;math inline&#34;&gt;\(s-\)&lt;/span&gt;th simulated value for the &lt;span class=&#34;math inline&#34;&gt;\(p-\)&lt;/span&gt;th parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_p^{(s)}\)&lt;/span&gt; depends on the distribution at the &lt;span class=&#34;math inline&#34;&gt;\((s-1)-\)&lt;/span&gt;th run for all the other parameters). But it is also expected that this level of correlation should fade away as you consider simulations that are further apart. So if the autocorrelation graph looks like the ones above, you can say that actually the simulations are close to a series of independent values because the autocorrelation is very low, in fact at low lags as well.&lt;/p&gt;
&lt;p&gt;In our case, we that the model converges (because of conjugacy), so it is not surprising to see these graphs. Numerically, when the effective sample size is “close enough” the the nominal sample size (i.e. the number of iterations you use to draw your inference), then autocorrelation is not a problem. In this case, we have considered 2 chains, each with 5000 iterations (so a total of 10000). The value of &lt;code&gt;n.eff&lt;/code&gt; is 10000 for &lt;code&gt;y.pred&lt;/code&gt;, 10000 for &lt;code&gt;theta&lt;/code&gt; and 10000 for &lt;code&gt;P.crit&lt;/code&gt;. Clearly, the ratio of effective to nominal sample size is then 1, 1 and 1, which clearly indicate no issues.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Covid vaccine: Bayesian modelling</title>
      <link>/practical/02_mcmc/covid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/practical/02_mcmc/covid/</guid>
      <description>


&lt;div id=&#34;description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Description&lt;/h2&gt;
&lt;p&gt;In the wake of the COVID-19 pandemic, several biotechnology and pharmaceutical companies have started collaborating on the development of a vaccine, in an unprecedented effort to deliver vital innovation in a short amount of time. The first vaccine to be approved by the FDA in December 2020 was the one developed jointly by German biotech company BioNTech and US pharmaceutical giant Pfizer. The development of the vaccine was based on a Phase I/II/III multicenter, placebo-controlled trial to evaluate the safety, tolerability, immunogenicity and efficacy of the candidate vaccine.&lt;/p&gt;
&lt;p&gt;The Phase II/III study was designed using a Bayesian approach based on the following setup. There are two arms: one is randomised to receive two doses of a placebo, while the active arm receives two doses of the candidate vaccine. The data relevant for the efficacy analysis are the number of individuals who in each arm are confirmed COVID-19 cases, over the total number of individuals randomised to the specific arm. This can be formalised as &lt;span class=&#34;math inline&#34;&gt;\(y_{\rm plac}\sim \mbox{Bin}(\pi_{\rm plac}, n_{\rm plac})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{\rm vac}\sim \mbox{Bin}(\pi_{\rm vac}, n_{\rm vac})\)&lt;/span&gt;, respectively. The actual measure of vaccine efficacy is defined as &lt;span class=&#34;math inline&#34;&gt;\(\mbox{VE}=\displaystyle\left(1-\frac{\pi_{\rm vac}}{\pi_{\rm plac}}\right)\)&lt;/span&gt;, assuming a 1:1 allocation ratio in the two arms and thus &lt;span class=&#34;math inline&#34;&gt;\(n_{\rm plac}=n_{\rm vac}=N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;However, the analysis is based on a reformulation of the problem: the experimenters actually model &lt;span class=&#34;math inline&#34;&gt;\(y=y_{\rm vac}=\)&lt;/span&gt; number of COVID-19 cases among the individuals in the treatment arm over &lt;span class=&#34;math inline&#34;&gt;\(n=(y_{\rm vac}+y_{\rm plac})=\)&lt;/span&gt; the total number of COVID-19 cases in the two arms. This is modelled as &lt;span class=&#34;math inline&#34;&gt;\(y \sim \mbox{Bin}(\theta,n)\)&lt;/span&gt;, where
&lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray}
\theta&amp;amp; = &amp;amp; \displaystyle\frac{\pi_{\rm vac}}{\pi_{\rm vac}+\pi_{\rm plac}} \nonumber \\
&amp;amp; = &amp;amp; \frac{1-\mbox{VE}}{2-\mbox{VE}}.
\end{eqnarray}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The reason for this seemingly overly-complex (and obscure) setup is that it allows the experimenters to only specify one observational model and, crucially a single prior distribution for the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; — of course, once &lt;span class=&#34;math inline&#34;&gt;\(p(\theta\mid {\rm data})\)&lt;/span&gt; is available, using the equation above, it is straightforward (e.g. through Monte Carlo simulation) to obtain directly the posterior distribution for the main parameter of interest &lt;span class=&#34;math inline&#34;&gt;\(p(\mbox{VE}\mid {\rm data})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-size-calculation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample size calculation&lt;/h2&gt;
&lt;p&gt;The determination of the sample size calculation is based on a simulation approach. The experimenter looked to determine a sample size large enough to be able to provide a probability exceeding 90% to conclude that &lt;span class=&#34;math inline&#34;&gt;\(\mbox{VE}&amp;gt;30\%\)&lt;/span&gt; with a “high probability” (we note here that the study protocol is vague on the actual threshold selected).&lt;/p&gt;
&lt;p&gt;The simulation excercise proceeds in two steps. Firstly, the experimenters make assumptions about some of the features of the “data generating process”. For instance, in the study protocol, they stipulate a “true” vaccine efficacy &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\mbox{VE}} =0.6\)&lt;/span&gt; (i.e. a reduction by 40% in the infection rate in the vaccine arm, in comparison to the placebo population). This implies that the “true” proportion of COVID-19 cases in the vaccine arm over the total of cases is
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta=\displaystyle\frac{1-\widehat{\mbox{VE}}}{2-\widehat{\mbox{VE}}}=0.2857.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using these assumption, we can simulate a large number &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; (say, 100,000) of potential trial data from the alleged generating process &lt;span class=&#34;math inline&#34;&gt;\(y^{(s)}\sim \mbox{Bin}(\hat\theta,n)\)&lt;/span&gt;, where the superscript &lt;span class=&#34;math inline&#34;&gt;\((s)\)&lt;/span&gt; indicates the &lt;span class=&#34;math inline&#34;&gt;\(s-\)&lt;/span&gt;th simulated dataset and given a fixed value of the overall number of cases &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Typically, we repeat the simulation for a grid of possible values of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; (e.g. &lt;span class=&#34;math inline&#34;&gt;\(n=[10,20,30,40,50,60,70,\ldots]\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Once the hypothetical trial data have been generated, the second step consists in analysing them according to the statistical analysis plan defined in the protocol. In this case, the full model specification is required, which involves defining a prior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For simplicity, the experimenters set a minimally informative Beta prior &lt;span class=&#34;math inline&#34;&gt;\(\theta \sim \mbox{Beta}(\alpha_0,\beta_0)\)&lt;/span&gt;, where the parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; are selected to express the limited amount of information available a priori (recall that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; represents the proportion of COVID-19 cases occurred in the vaccine group). &lt;a href=&#34;../../../slides/02_BUGS/index.html#27&#34;&gt;We know&lt;/a&gt; that &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}[\theta]=\displaystyle\frac{\alpha_0}{\alpha_0+\beta_0}\)&lt;/span&gt;. If we fix &lt;span class=&#34;math inline&#34;&gt;\(\beta_0=1\)&lt;/span&gt;, then we can solve for &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt; so that the prior mean for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is equal to some pre-specified value — in particular, the experimenters had chosen a threshold of &lt;span class=&#34;math inline&#34;&gt;\(\mbox{VE}=30\%\)&lt;/span&gt; as the minimum level of efficacy they were prepared to entertain. This value can be mapped to the scale of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \frac{1-0.3}{2-0.3}=0.4117\)&lt;/span&gt; and thus solving
&lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray*}
\frac{\alpha_0}{\alpha_0+1} = 0.4117
\end{eqnarray*}\]&lt;/span&gt;
gives &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0=0.700102\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because of &lt;a href=&#34;../../../slides/03_MCMC/#18&#34;&gt;conjugacy&lt;/a&gt;, for each simulated dataset we can easily update the Beta&lt;span class=&#34;math inline&#34;&gt;\((0.700102,1)\)&lt;/span&gt; prior to a Beta&lt;span class=&#34;math inline&#34;&gt;\((\alpha_1,\beta_1)\)&lt;/span&gt; posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1=0.700102+y^{(s)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1=1+n-y^{(s)}\)&lt;/span&gt;. Moreover, for each simulation &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, we can compute &lt;em&gt;analytically&lt;/em&gt; any tail-area probability from &lt;span class=&#34;math inline&#34;&gt;\(p(\theta\mid{\rm data})\)&lt;/span&gt;. Once again, we are really interested in &lt;span class=&#34;math inline&#34;&gt;\(\Pr(\mbox{VE}&amp;gt;0.3\mid{\rm data})\)&lt;/span&gt;, but using the deterministic relationship linking VE to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we can re-express this as
&lt;span class=&#34;math display&#34;&gt;\[\begin{eqnarray*}
\Pr(\mbox{VE}&amp;gt;0.3 \mid{\rm data}) &amp;amp; = &amp;amp; \Pr\left(\frac{1-2\theta}{1-\theta}&amp;gt;0.3\mid{\rm data}\right) \\
&amp;amp; = &amp;amp; \Pr(\theta&amp;lt;0.4117 \mid {\rm data}),
\end{eqnarray*}\]&lt;/span&gt;
which can be computed for each simulation &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. This produces a large number of simulations that can be used to determine the “power” for a given sample size, as the proportion of times in which this computed probability exceeds a set threshold (i.e. is “large enough” in the phrasing of the study protocol).&lt;/p&gt;
&lt;p&gt;The experimenters compute &lt;span class=&#34;math inline&#34;&gt;\(n=164\)&lt;/span&gt; as the optimal number of total COVID-19 cases that are necessary to be able to ascertain that &lt;span class=&#34;math inline&#34;&gt;\(\mbox{VE}&amp;gt;30\%\)&lt;/span&gt; with a large probability.&lt;/p&gt;
&lt;p&gt;Then, it is necessary to determine the overall sample size (i.e. the total number of individuals to be recruited in the study) so that a total of 164 cases is likely to be observed within the required time frame. Once again, it is necessary to make some assumption about the data generating process; specifically the experimenters consider a 1.3% illness rate per year in the placebo group. Because the study aims at accruing 164 cases within 6 months, this essentially amounts to assuming that &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\rm plac} \approx 0.013/2\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\rm vac} \approx (\pi_{\rm plac}\times 0.4)/2\)&lt;/span&gt; (recall that we are assuming a 60% vaccine efficacy, or that &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\rm vac}=0.4\times \pi_{\rm plac}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;We can once again resort to simulations to estimate what sample size in each arm &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is necessary so that we can expect &lt;span class=&#34;math inline&#34;&gt;\(y_{\rm vac}+y_{\rm plac}\geq 164\)&lt;/span&gt; — this returns an optimal sample size of &lt;span class=&#34;math inline&#34;&gt;\(N=17\,600\)&lt;/span&gt; per group. Finally, considering an attrition rate of 20% (indicating that such proportion of individuals would not generate an evaluable outcome could be observed), the experimenters inflate the sample size to obtain &lt;span class=&#34;math inline&#34;&gt;\(N^*=\displaystyle\frac{N}{0.8}=21\,999\)&lt;/span&gt; per group (or a total of 43,998 individuals).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data analysis&lt;/h2&gt;
&lt;p&gt;At the end of the actual study, there were &lt;span class=&#34;math inline&#34;&gt;\(y_{\rm vac}=8\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{\rm plac}=162\)&lt;/span&gt; confirmed COVID-19 cases in the vaccine and the placebo group, respectively. These imply that &lt;span class=&#34;math inline&#34;&gt;\(y=8\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n=(8+162)=170\)&lt;/span&gt;. However, there is a slight extra complication: the setup describe above implies the assumption of 1:1 allocation (i.e. &lt;span class=&#34;math inline&#34;&gt;\(n_{\rm plac}=n_{\rm vac}\)&lt;/span&gt;) — this is crucial to ensure the simple deterministic relationship between VE and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. In actual fact, the placebo group had a slightly higher number of individuals, by the time of the data analysis (there were 17,411 and 17,511 individuals with valid data in the vaccine and placebo group, respectively). Thus, we need to rescale the observed data, which can be done by considering &lt;span class=&#34;math inline&#34;&gt;\(y_{\rm vac}^*=y_{\rm vac}\displaystyle\frac{17\,461}{17\,411}=8.02297\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{\rm plac}^*=y_{\rm plac}\displaystyle\frac{17\,461}{17\,511}=161.53743\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(17\,461 = \displaystyle\frac{17\,411+17\,511}{2}\)&lt;/span&gt; and thus &lt;span class=&#34;math inline&#34;&gt;\(y^*=8.02297\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n^*=169.5604\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These data can be used to update the prior distribution Beta&lt;span class=&#34;math inline&#34;&gt;\((0.700102,1)\)&lt;/span&gt; into a Beta&lt;span class=&#34;math inline&#34;&gt;\((8.723072,162.5374)\)&lt;/span&gt; posterior for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Finally, we can rescale this posterior to determine the relevant posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\mbox{VE}\mid {\rm data})\)&lt;/span&gt;, for instance using Monte Carlo to simulate a large number &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; of values &lt;span class=&#34;math inline&#34;&gt;\(\theta^{(s)}\sim \mbox{Beta}(8.723072,162.5374)\)&lt;/span&gt; and then computing &lt;span class=&#34;math inline&#34;&gt;\(\mbox{VE}^{(s)}=\displaystyle\frac{1-2\theta^{(s)}}{1-\theta^{(s)}}\)&lt;/span&gt; and then use these to characterise the uncertainty around the vaccine efficacy.&lt;/p&gt;
&lt;p&gt;In this case, the posterior distributions &lt;span class=&#34;math inline&#34;&gt;\(p(\theta\mid {\rm data})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(\mbox{VE}\mid {\rm data})\)&lt;/span&gt; can be visualised below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha.0=0.700102
beta.0=1
y=c(8,162)
n=c(17411,17511)
# So needs to reproportion as if the treatment arms had the same sample size (to be in line with model assumptions!)
y=y*mean(n)/n
# Now can update the Beta prior with the observed data
alpha.1=alpha.0+y[1]
beta.1=beta.0+y[2]

theta=rbeta(100000,alpha.1,beta.1)
ve=(1-2*theta)/(1-theta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/covid_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;/practical/02_mcmc/covid_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also use the resulting samples from the posterior distribution to estimate the 95% interval of &lt;span class=&#34;math inline&#34;&gt;\([0.9034; 0.9761]\)&lt;/span&gt; for the vaccine efficacy, indicating that we expect the vaccine to perform extremely well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How do MCMC and Gibbs sampling really work?</title>
      <link>/practical/02_mcmc/tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/practical/02_mcmc/tutorial/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This short tutorial will guide you through the example of Gibbs sampling shown in class.&lt;/p&gt;
&lt;p&gt;As a quick reminder, the example does not really require Gibbs sampling or any form of MCMC to estimate the joint posterior distribution for the parameters. However, because of its specific assumptions it is very helpful because essentially we can determine analytically the full conditional distributions for each parameter (details later). This means that we can directly and repeatedly sample from them (which is a pre-requisite of Gibbs sampling and what &lt;tt&gt;BUGS&lt;/tt&gt; does).&lt;/p&gt;
&lt;p&gt;This document also includes the R code used to obtain the output.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up&lt;/h2&gt;
&lt;p&gt;As discussed in class, we assume a set up such as the following.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
y_i &amp;amp; \stackrel{iid}{\sim} \mbox{Normal}(\mu,\sigma^2),\qquad \mbox{with } i=1,\ldots,n \\
\mu &amp;amp; \sim \mbox{Normal}(\mu_0,\sigma^2_0) \\
\tau = \frac{1}{\sigma^2} &amp;amp; \sim \mbox{Gamma} (\alpha_0,\beta_0)
\end{align}\]&lt;/span&gt;
Under these assumptions we can &lt;em&gt;prove&lt;/em&gt; analyticaly that the full conditional distributions for the two parameters &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; are
&lt;span class=&#34;math display&#34;&gt;\[\mu\mid \sigma^2,\boldsymbol{y} \sim \mbox{Normal}(\mu_1,\sigma^2_1) \qquad \mbox{with: } \mu_1=\sigma^2_1\left( \frac{\mu_0}{\sigma^2_0} + \frac{n\bar{y}}{\sigma^2}\right) \qquad \mbox{and: } \sigma^2_1=\left(\frac{1}{\sigma^2_0}+\frac{n}{\sigma^2}\right)^{-1}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\tau\mid\mu,\boldsymbol{y} \sim \mbox{Gamma}(\alpha_1,\beta_1) \qquad \mbox{with: }\,\alpha_1=\alpha_0+\frac{n}{2}\quad \mbox{and: } \quad \beta_1 = \beta_0 + \frac{1}{2}\sum_{i=1}^n (y_i-\mu)^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that the full conditionals are &lt;strong&gt;not&lt;/strong&gt; the target distributions for Bayesian inference.
What we really want is the marginal posterior distributions &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid \boldsymbol{y} )\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(\tau \mid \boldsymbol{y})\)&lt;/span&gt;, which
can be simply obtained from the marginal joint posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\mu, \tau \mid \boldsymbol{y} )\)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;However, in this case, for each of the two parameters, conditionally on (i.e. given)
the other, the posterior is in the same family of the prior — the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is
still a Normal and the posterior for &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is still a Gamma; what changes is the value of
the “hyper-parameters” (i.e. the parameters of these distributions), which move from
(&lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0^2)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((\mu_1 , \sigma_1^2 )\)&lt;/span&gt; and from &lt;span class=&#34;math inline&#34;&gt;\((\alpha_0, \beta_0 )\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((\alpha_1 , \beta_1)\)&lt;/span&gt;. For this reason, this model is called
“semi-conjugated”.&lt;/p&gt;
&lt;p&gt;These relationships clarify that the (posterior) distribution of the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; depends
(among other things) on the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1/\tau\)&lt;/span&gt; as well as that the posterior for the
precision &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; depends (among other things) on the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Crucially, &lt;strong&gt;&lt;em&gt;in this example&lt;/em&gt;&lt;/strong&gt;,
this dependence is known in closed form. A very useful implication of this set up is that
it means that we can directly determine not just the distributional form of the posteriors
(Normal for the mean and Gamma for the precision), but also the numerical value of
the “hyper-parameters” (&lt;span class=&#34;math inline&#34;&gt;\(\mu_1, \sigma_1^2 , \alpha_1,\beta_1 )\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;what-do-we-really-need-to-do&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What do we really need to do?&lt;/h3&gt;
&lt;p&gt;The point of this exercise is to create a routine that can simulate sequentially from these
distributions, with the aim of obtaining a sample from the joint posterior distribution
&lt;span class=&#34;math inline&#34;&gt;\(p(\mu, \tau \mid \boldsymbol{y})\)&lt;/span&gt;. So, once the data y have been observed and we have fixed the values for
the parameters of the prior distributions (&lt;span class=&#34;math inline&#34;&gt;\(\mu_0, \sigma_0^2,\alpha_0,\beta_0)\)&lt;/span&gt;, we can use, for instance &lt;tt&gt;R&lt;/tt&gt;, to
simulate from the resulting full conditionals.&lt;/p&gt;
&lt;p&gt;At each iteration, we will update sequentially the values of the two parameters: first
we update &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; by simulating from its full conditional, then we set the value of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; to the
one we have just simulated and update the value of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; by randomly drawing from its
full conditional. And we repeat this process “until convergence”.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running the example&lt;/h2&gt;
&lt;div id=&#34;data-and-fixed-quantities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data and fixed quantities&lt;/h3&gt;
&lt;p&gt;Suppose we have observed a sample of &lt;span class=&#34;math inline&#34;&gt;\(n = 30\)&lt;/span&gt; data points, for example, in &lt;tt&gt;R&lt;/tt&gt; you may
input the data onto your workspace using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Vector of observed data
y = c(1.2697,7.7637,2.2532,3.4557,4.1776,6.4320,-3.6623,7.7567,5.9032,
7.2671,-2.3447,8.0160,3.5013,2.8495,0.6467,3.2371,5.8573,-3.3749,
4.1507,4.3092,11.7327,2.6174,9.4942,-2.7639,-1.5859,3.6986,2.4544,
-0.3294,0.2329,5.2846)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assume also that you want to set the value of the parameters for the priors as &lt;span class=&#34;math inline&#34;&gt;\(\mu_0 = 0\)&lt;/span&gt;,
&lt;span class=&#34;math inline&#34;&gt;\(\sigma_0^2 = 10000\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0 = \beta_0 = 0.1\)&lt;/span&gt;. In &lt;tt&gt;R&lt;/tt&gt; we could define these using the following
commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# &amp;quot;Hyper-parameters&amp;quot; (ie parameters for the prior distributions)
mu_0 = 0
sigma2_0 = 10000
alpha_0 = 0.01
beta_0 = 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may also want to define some “utility” variables, that can be used later on, for
instance the sample size and observed sample mean, which we can input in R as in the
following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sample size and sample mean of the data
n = length(y)
ybar = mean(y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The actual values of these variables are now stored in the respective “objects” and can
be accessed at any point, for instance the commands:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 30&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ybar&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.343347&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will output the sample size and mean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Initial values&lt;/h3&gt;
&lt;p&gt;As seen in the lecture, in order to run the Gibbs sampling algorithm, we need to initialise
the Markov chains. This essentially means telling the computer what values should be
used at iteration 0 of the process for anything that is a) associated with a probability
distribution (i.e. it is not known with absolute certainty); and b) has not been observed.&lt;/p&gt;
&lt;p&gt;We can do this in &lt;tt&gt;R&lt;/tt&gt; using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sets the &amp;quot;seed&amp;quot; (for reproducibility). With this command, you will
# *always* get the exact same output
set.seed(13)
# Initialises the parameters
mu = tau = numeric()
sigma2 = 1/tau
mu[1] = rnorm(1,0,3)
tau[1] = runif(1,0,3)
sigma2[1] = 1/tau[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we set the “random seed” (in this case to the value 13), which ensures replicability of the results. If you run this code on any machine, you will always and invariably obtain the same results.&lt;/p&gt;
&lt;p&gt;Then, we define the parameters mu and tau to be vectors, which in &lt;tt&gt;R&lt;/tt&gt; we do by using the &lt;tt&gt;R&lt;/tt&gt; built-in command &lt;code&gt;numeric()&lt;/code&gt;. Basically, the command &lt;code&gt;mu = tau = numeric()&lt;/code&gt; instructs &lt;tt&gt;R&lt;/tt&gt; to expect these two objects to be vectors of (as yet) unspecified length (if you used the command &lt;code&gt;x = numeric(5)&lt;/code&gt; we would define a vector of length 5).&lt;/p&gt;
&lt;p&gt;Finally, we set the first value of &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;tau&lt;/code&gt; to be randomly generated, respectively
from a Normal(mean = 0, sd = 3) and a Uniform(0, 3). &lt;tt&gt;R&lt;/tt&gt; has built-in commands to
draw (pseudo-)random numbers — typically these are constructed using the prefix &lt;code&gt;r&lt;/code&gt;
(for “random”) and a string of text describing the distribution (e.g. &lt;code&gt;norm&lt;/code&gt; for “Normal”,
&lt;code&gt;unif&lt;/code&gt; for “Uniform”, &lt;code&gt;bin&lt;/code&gt; for “Binomial”, etc.). The first argument (input) to a call to a
&lt;code&gt;rxxxx(...)&lt;/code&gt; command is the number of values you want to simulate. So, for instance,
&lt;code&gt;rnorm(1000,0,6)&lt;/code&gt; instructs &lt;tt&gt;R&lt;/tt&gt; to simulate 1000 values from a Normal distribution with
mean 0 and standard deviation 6 — notice that, unlike &lt;tt&gt;BUGS&lt;/tt&gt;, &lt;tt&gt;R&lt;/tt&gt; parameterises the Normal in terms of mean and sd (instead of the precision).
You can check the values that have been selected to initialise your Markov chain by
simply typing the name of the variables (or some suitable function thereof).&lt;/p&gt;
&lt;p&gt;You can check the values that have been selected to initialise your Markov chain by
simply typing the name of the variables (or some suitable function thereof).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.662981&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(sigma2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9249339&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-gibbs-sampling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running the Gibbs sampling&lt;/h3&gt;
&lt;p&gt;Generally speaking, the actual Gibbs sampling is really simple (if the full conditionals
are known analytically!) and reduces to code such as the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sets the number of iterations (nsim)
nsim = 1000
# Loops over to sequentially update the parameters
for (i in 2:nsim) {
  # 1. Updates the sd of the full conditional for mu
  sigma_1 = sqrt(1/(1/sigma2_0 + n/sigma2[i-1]))
  # 2. Updates the mean of the full conditional for mu
  mu_1 = (mu_0/sigma2_0 + n*ybar/sigma2[i-1])*sigma_1^2
  # 3. Samples from the updated full conditional for mu
  mu[i] = rnorm(1,mu_1,sigma_1)
  
  # 4. Updates the 1st parameter of the full conditional for tau
  alpha_1 = alpha_0+n/2
  # 5. Updates the 2nd parameter of the full conditional for tau
  beta_1 = beta_0 + sum((y-mu[i])^2)/2
  # 6. Samples from the updated full conditional for tau
  tau[i] = rgamma(1,alpha_1,beta_1)
  # 7. Re-scales the sampled value on the variance scale
  sigma2[i] = 1/tau[i]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you check the code above, you should be able to see that it matches perfectly the
mathematical expressions defined for the (updated) parameters of the full posterior
distributions. The loop goes from 2 to &lt;code&gt;nsim&lt;/code&gt; — the first value of the vectors &lt;code&gt;mu&lt;/code&gt;, &lt;code&gt;tau&lt;/code&gt; and
&lt;code&gt;sigma2&lt;/code&gt; are filled at initialisation.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that the code above produces simulations from the full conditional of the precision
&lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; , which are then rescaled to produce a vector of simulations from the joint posterior
distribution for the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; . It is of course very easy to also rescale these further to
obtain a sample of values from the posterior of the standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, for example
using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma = sqrt(sigma2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this code has been executed in &lt;tt&gt;R&lt;/tt&gt; your output is made by vectors that, if the procedure has worked (i.e. it has converged), are drawn, at least with a very good degree of approximation, from the joint posterior distribution of the parameters.&lt;/p&gt;
&lt;p&gt;You can also visualise a traceplot — in this particular case, convergence is not an issue
(as the model is “semi-conjugated”) and even with a single chain, you can get a sense
that all the traceplots are &lt;a href=&#34;/slides/02_mcmc/#34&#34;&gt;“fat, hairy caterpillars”&lt;/a&gt;, which indicates all is well.&lt;/p&gt;
&lt;p&gt;A simple code needed to produce these two plots is the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Histograms from the posterior distributions
hist(mu)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/tutorial_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/tutorial_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Traceplots
plot(mu,t=&amp;quot;l&amp;quot;,bty=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/tutorial_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sigma,t=&amp;quot;l&amp;quot;,bty=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/tutorial_files/figure-html/unnamed-chunk-11-2.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once again, notice how powerful MCMC is: technically, you may not model directly a
(highly non-linear) function of the main parameters; for example, all the computation
is made in terms of the precision &lt;code&gt;tau&lt;/code&gt;, although you may be much more interested in
learning the standard deviation sigma. MCMC lets you obtain all the relevant informa-
tion on the latter by simply creating simulations from the posterior distributions using
the relevant inverse function &lt;code&gt;sigma = 1/sqrt(tau)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can also depict the trajectories of the MCMC samples in the first 10 iterations of the proecess. The arrows follow
the moves of the Markov chain during the updates of the parameters, from the initial&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/practical/02_mcmc/tutorial_files/figure-html/unnamed-chunk-19-1.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The main intuition here is to do with the basic properties of conditional probabilities. Recall that given two events &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, then by definition &lt;span class=&#34;math inline&#34;&gt;\(\Pr(A \mid B) = \frac{\Pr(A,B)}{\Pr(B)}\)&lt;/span&gt;. Now we can extend this to the case of three events &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, by adding &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; to the conditioning set (i.e. to the right of the “&lt;span class=&#34;math inline&#34;&gt;\(\mid\)&lt;/span&gt;” symbol throughout the equation) and get &lt;span class=&#34;math inline&#34;&gt;\(\Pr(A \mid B, C ) = \frac{\Pr(A,B\mid C)}{Pr(B\mid C)}\)&lt;/span&gt;. If we replace &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; ,&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; and a probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\cdot)\)&lt;/span&gt; for the probability associated with an event &lt;span class=&#34;math inline&#34;&gt;\(\Pr(\cdot)\)&lt;/span&gt;, we can write &lt;span class=&#34;math inline&#34;&gt;\(p(\mu \mid \sigma^2, \boldsymbol{y} ) = \frac{p(\mu,\sigma^2\mid \boldsymbol{y})}{p(\sigma^2\mid \boldsymbol{y})}\propto p(\mu, \sigma^2 \mid \boldsymbol{y} )\)&lt;/span&gt;. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(p(\sigma^2 \mid \mu, \boldsymbol{y}) = \frac{p(\mu,\sigma^2\mid\boldsymbol{y})}{p(\mu\mid\boldsymbol{y})} \propto p(\mu,\sigma^2\mid\boldsymbol{y})\)&lt;/span&gt;. We can then see that each full conditional is proportional to the target distribution (i.e. the joint posterior of all the parameters). Thus, sampling repeatedly from all the full conditionals essentially gives us something that is, broadly speaking, proportional to our target joint posterior distribution. Formal theorems ensure that
if we do this long enough, we are guaranteed to actually approximate the target to an arbitrary degree of precision.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt;: there is a slight confusion in the terminology: usually, we refer the initialisation of the process as iteration 0. However, &lt;tt&gt;R&lt;/tt&gt; does not let you index a vector with the number 0, i.e. the first element of a vector is indexed by the number 1. Thus, what in the &lt;a href=&#34;/slides/02_mcmc/#27&#34;&gt;Lecture 2&lt;/a&gt; was indicated as &lt;span class=&#34;math inline&#34;&gt;\(\mu^{(0)}\)&lt;/span&gt; is actually &lt;code&gt;mu[1]&lt;/code&gt; in the &lt;tt&gt;R&lt;/tt&gt; code. Similarly, the updated value for &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; at iteration 2 is indicated as &lt;span class=&#34;math inline&#34;&gt;\(\mu^{(2)}\)&lt;/span&gt; in the slides, but as &lt;code&gt;mu[3]&lt;/code&gt; in the &lt;tt&gt;R&lt;/tt&gt; code.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 3. Introduction to R and cost-effectiveness analysis using BCEA - SOLUTIONS</title>
      <link>/practical/03_bcea/solutions/</link>
      <pubDate>Mon, 20 Jun 2022 16:30:00 +0000</pubDate>
      <guid>/practical/03_bcea/solutions/</guid>
      <description>


&lt;div id=&#34;introduction-to-r-and-cost-effectiveness-analysis-using-bcea-solutions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to &lt;tt&gt;R&lt;/tt&gt; and cost-effectiveness analysis using &lt;tt&gt;BCEA&lt;/tt&gt; — SOLUTIONS&lt;/h2&gt;
&lt;div id=&#34;mcmc-in-rbugs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MCMC in &lt;tt&gt;R&lt;/tt&gt;/&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/h3&gt;
&lt;p&gt;The model file is fairly simple in this case and it is coded as in the
following&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Model for Laplace&amp;#39;s analysis of birth&amp;#39;s data
model {
    y ~ dbin(theta,n)
    theta ~ dunif(0,1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which you can save on a file &lt;code&gt;ModelLaplace.txt&lt;/code&gt;. Of course, the naming
convention is completely up to you and you can use any name or file
extension, for that matter, e.g. something like &lt;code&gt;model.bug&lt;/code&gt; is perfectly
acceptable to &lt;tt&gt;BUGS&lt;/tt&gt;, as long as all is
consistent in your call from &lt;tt&gt;R&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;The next step consists into inputting the data in the
&lt;tt&gt;R&lt;/tt&gt; workspace. Assuming you have opened
&lt;tt&gt;R&lt;/tt&gt;, you can point it in the “working”
directory, i.e. the folder in which you have stored the relevant files,
say something like &lt;code&gt;C:\gianluca&lt;/code&gt; if you are under &lt;code&gt;MS Windows&lt;/code&gt;.
&lt;tt&gt;R&lt;/tt&gt; uses &lt;code&gt;Unix&lt;/code&gt; notation and thus paths to
folder are characterised by a slash (not a forward slash, as in
&lt;code&gt;MS Windows&lt;/code&gt;). Thus you can point &lt;tt&gt;R&lt;/tt&gt; to the
relevant directory using the &lt;code&gt;setwd&lt;/code&gt; command, for example as in the
following.&lt;/p&gt;
&lt;p&gt;An alternative way of specifying a path in
&lt;tt&gt;R&lt;/tt&gt; is to use “double forward slash”, as in the
following&lt;/p&gt;
&lt;p&gt;To &lt;tt&gt;R&lt;/tt&gt; , both commands have the same meaning.&lt;br /&gt;
Now we can start inputting the data in the workspace. Because the
example is so simple, we can simply write down the values we want to
associated with each of the relevant variables, for example as in the
following commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y=241945
n=493527
data=list(y=y,n=n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first two commands define new variables &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; and assign them
the relevant values. The third line creates a new
&lt;tt&gt;R&lt;/tt&gt; object — that is a &lt;em&gt;list&lt;/em&gt; in which we store
&lt;code&gt;y&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;. Lists are very helpful because they act as boxes in which
you can put variables of different nature (scalars, matrices, strings,
numbers, etc.). The way &lt;tt&gt;R&lt;/tt&gt; defines variables
and lists in particular is rather straightforward: notice however that
you can assign names to elements of a list — we do this here by using
the option &lt;code&gt;y=y,n=n&lt;/code&gt;.&lt;br /&gt;
You can inspect the list by using various
&lt;tt&gt;R&lt;/tt&gt; commands, for example as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;y&amp;quot; &amp;quot;n&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 241945&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command &lt;code&gt;names&lt;/code&gt; returns the names of the objects included inside the
data list, while the “$” operator allows us to access objects contained
inside other objects. So the command &lt;code&gt;data$y&lt;/code&gt; looks inside the object
&lt;code&gt;data&lt;/code&gt; and returns the value of the object &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The next step we need to follow before we can run our model using
&lt;tt&gt;BUGS&lt;/tt&gt;is to set up the environment and main
variables for the MCMC. We do this using the
&lt;tt&gt;R&lt;/tt&gt; commands below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filein=&amp;quot;ModelLaplace.txt&amp;quot;
params=&amp;quot;theta&amp;quot;
inits_det=list(list(theta=.1),list(theta=.9))
inits_ran=function(){list(theta=runif(1))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first one creates a string variable with the path to the model file.
In this case, we assume that the &lt;code&gt;.txt&lt;/code&gt; file is stored in the working
directory and so all that is necessary is a pointer to the file name. If
the &lt;code&gt;.txt&lt;/code&gt; were stored somewhere, e.g. in the folder
&lt;code&gt;C:\gianluca\myfiles&lt;/code&gt;, then we would need to specify the full path to
the file, e.g. &lt;code&gt;filein=C:/gianluca/myfiles/ModelLaplace.txt&lt;/code&gt;. Of course,
there is nothing special about the name &lt;code&gt;filein&lt;/code&gt; — and you can use any
name you like, provided you are then consistent in the call to
&lt;tt&gt;BUGS&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The second command defines the parameters to be monitored. In this case,
the model is so simple that it only has one parameter, so we simply
define a string variable &lt;code&gt;params&lt;/code&gt;. If we had more than one parameter, we
would need to create a vector of names — in
&lt;tt&gt;R&lt;/tt&gt; we can do this by using the “collection”
operator, for example as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;params=c(&amp;quot;theta1&amp;quot;,&amp;quot;theta2&amp;quot;,&amp;quot;theta3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The third and fourth commands show two different ways of creating
initial values to pass to &lt;tt&gt;BUGS&lt;/tt&gt;. The first
one uses deterministic initial values, in the sense that you are passing
specific values to &lt;tt&gt;R&lt;/tt&gt; , which it in turn will
send to &lt;tt&gt;BUGS&lt;/tt&gt;. If you want to do this, you
need to create a “list of lists”. In other words, first you define a
variable (in this case &lt;code&gt;inits_det&lt;/code&gt;) as a list. Then, you create other
lists inside it — each of this list should contain a specific value for
each of the variables you need to initialise. The number of lists inside
the upper-level list is defined by the number of Markov chain you want
to run (see below and the lecture slides). You can explore the list you
have just created by simply calling its name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inits_det&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[[1]]$theta
[1] 0.1


[[2]]
[[2]]$theta
[1] 0.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This may be a bit daunting at first glance, but once you get your head
around it, the way in which &lt;tt&gt;R&lt;/tt&gt; manages its
objects is actually very informative and very much linked with its
object-oriented programming nature. The output is characterised by a
series of “tags”. For example, the tag &lt;code&gt;[[1]]&lt;/code&gt; indicates the first
element (i.e. the first list included) in the main object (i.e. the
variable &lt;code&gt;inits_det&lt;/code&gt;). So for example, you can access elements inside an
object using the “‘double square bracket’’ and/or the “$” notation, for
example as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inits_det[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$theta
[1] 0.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inits_det[[1]]$theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The lower level tag &lt;code&gt;[1]&lt;/code&gt; is used to indicate the variables stored
inside the upper-level element of the object (e.g. the value &lt;code&gt;0.1&lt;/code&gt; that
we have assigned to &lt;code&gt;theta&lt;/code&gt; in the first list inside &lt;code&gt;inits_det&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The fourth command defines “random” initial values. To do so, instead of
you passing a specific value, you can create a
&lt;tt&gt;R&lt;/tt&gt; function that simulates them from a
suitable probability distribution. In this case, we know that &lt;code&gt;theta&lt;/code&gt; is
defined in our model code to be a probability and so it has to range
between 0 and 1. As we have seen in the classes, a Uniform distribution
may be a good candidate and thus in this case we set up a little
function that creates a list and puts inside it a number generated
randomly from a Uniform distribution. In &lt;tt&gt;R&lt;/tt&gt;,
built-in functions such as &lt;code&gt;runif&lt;/code&gt; have default values. So a call&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runif(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1834258&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;randomly samples 1 value from a Uniform distribution defined in the
default range &lt;span class=&#34;math inline&#34;&gt;\([0;1]\)&lt;/span&gt;. You can get more information on
&lt;tt&gt;R&lt;/tt&gt; functions typing the command
&lt;code&gt;help(name_of_the_function)&lt;/code&gt; to your
&lt;tt&gt;R&lt;/tt&gt; terminal.&lt;/p&gt;
&lt;p&gt;At this point we are finally ready to call
&lt;tt&gt;BUGS&lt;/tt&gt;in background and do the MCMC simulation
for us. We can do so using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(R2OpenBUGS)
model= bugs(data=data,inits=inits_det,parameters.to.save=params,model.file=filein,
              n.chains=2,n.iter=10000,n.burnin=4500,n.thin=1,DIC=TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Firstly we load the package &lt;code&gt;R2OpenBUGS&lt;/code&gt;, which allows us to link
&lt;tt&gt;R&lt;/tt&gt; to &lt;tt&gt;BUGS&lt;/tt&gt;. Then
we define a new object &lt;code&gt;model&lt;/code&gt;, which will collect the output of the
call to the function &lt;code&gt;bugs&lt;/code&gt;. This takes several arguments. In this case,
we are using the deterministic initial values and considering 2 chains.
We ask &lt;tt&gt;BUGS&lt;/tt&gt;to do a burn-in (cfr. slides as
well as &lt;em&gt;BMHE&lt;/em&gt; and &lt;em&gt;The BUGS Book&lt;/em&gt;) of 4500 iterations and then simulate
for another 10000 iterations after that. We do not require thinning (or,
equivalently, a thinning of 1, which simply means that all the 10000
simulations after burn-in are stored for our analysis). Finally, we
instruct &lt;tt&gt;BUGS&lt;/tt&gt;to compute the DIC (more on
this later — do not worry about it for now).&lt;/p&gt;
&lt;p&gt;The commands&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;n.chains&amp;quot;        &amp;quot;n.iter&amp;quot;          &amp;quot;n.burnin&amp;quot;        &amp;quot;n.thin&amp;quot;         
 [5] &amp;quot;n.keep&amp;quot;          &amp;quot;n.sims&amp;quot;          &amp;quot;sims.array&amp;quot;      &amp;quot;sims.list&amp;quot;      
 [9] &amp;quot;sims.matrix&amp;quot;     &amp;quot;summary&amp;quot;         &amp;quot;mean&amp;quot;            &amp;quot;sd&amp;quot;             
[13] &amp;quot;median&amp;quot;          &amp;quot;root.short&amp;quot;      &amp;quot;long.short&amp;quot;      &amp;quot;dimension.short&amp;quot;
[17] &amp;quot;indexes.short&amp;quot;   &amp;quot;last.values&amp;quot;     &amp;quot;isDIC&amp;quot;           &amp;quot;DICbyR&amp;quot;         
[21] &amp;quot;pD&amp;quot;              &amp;quot;DIC&amp;quot;             &amp;quot;model.file&amp;quot;     &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model$n.iter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will return the list of names for all the elements included in the
&lt;code&gt;model&lt;/code&gt; object; and access the value of the element &lt;code&gt;n.iter&lt;/code&gt; included
inside the object &lt;code&gt;model&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R2OpenBUGS&lt;/code&gt; has a “print” method — meaning that we can display the
results of our model in a tabular form, simply using the following
command (the option &lt;code&gt;digits=3&lt;/code&gt; defines the number of significant figures
to be displayed).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(model,digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;ModelLaplace.txt&amp;quot;, 
Current: 2 chains, each with 10000 iterations (first 4500 discarded)
Cumulative: n.sims = 11000 iterations saved
           mean    sd   2.5%   25%   50%    75%  97.5%  Rhat n.eff
theta     0.490 0.001  0.489  0.49  0.49  0.491  0.492 1.001 11000
deviance 14.562 1.427 13.560 13.66 14.02 14.870 18.740 1.002  3200

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 1.002 and DIC = 15.560
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a table with summary statistics from the posterior
distribution of the nodes we have instructed
&lt;tt&gt;R&lt;/tt&gt; and &lt;tt&gt;BUGS&lt;/tt&gt;to
monitor. Notice that &lt;tt&gt;BUGS&lt;/tt&gt;will automatically
monitor a node &lt;code&gt;deviance&lt;/code&gt; — see Lecture 5. For now, let us not worry
about it!&lt;/p&gt;
&lt;p&gt;In this case, we can easily assess that the estimate for the probability
of a girl being born is on average 0.490 and we can approximate a 95%
&lt;em&gt;credible&lt;/em&gt; interval by considering as lower and upper limits the 2.5%
and 97% quantiles of the posterior distribution. In this case, the
interval estimate is 0.489 to 0.492. Because the entire interval is
below the threshold of 0.5, Laplace concluded in his analysis that it
was “morally certain” that a boy birth was more likely than a girl (see
&lt;em&gt;BMHE&lt;/em&gt;, chapter 2).&lt;/p&gt;
&lt;p&gt;In any MCMC analysis, convergence is a crucial point. Because the
Uniform distribution (that we have used as prior for &lt;code&gt;theta&lt;/code&gt;) can be
considered as a special case of the Beta distribution, we know from
Lecture 3 that actually ours is a case of Binomial-Beta conjugate model.
Thus, we do not really have issues with convergence and in fact the
&lt;tt&gt;BUGS&lt;/tt&gt;model is not even really a MCMC exercise
(the simulations are still obtained but all the calculations are done in
close-form because of conjugacy). This is reflected in the value of the
potential scale reduction factor (or Gelman-Rubin statistic &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt;),
described in the column headed &lt;code&gt;Rhat&lt;/code&gt;, which show values well below the
threshold of 1.1. Similarly, autocorrelation is not an issue as the
effective sample size (described in the column headed &lt;code&gt;neff&lt;/code&gt;) is exactly
identical with the actual sample size. In particular, notice that: we
have 2 chains, we run the model for 10000 iterations per chain after a
burn-in of 4500 iterations per chain that are discarded. This makes the
total number of simulations used for our analysis
&lt;span class=&#34;math inline&#34;&gt;\(2\times (10000-4500)=11000\)&lt;/span&gt;. This number is stored also in the object
&lt;code&gt;model$n.sims&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A handy workaround that allows us to avoid using the “$” or “double
square bracket” notation is to “attach” an object to the workspace. This
means that we effectively bypass the upper-level object and make its
elements available directly to &lt;tt&gt;R&lt;/tt&gt; and we can
do this by using the command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach.bugs(model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt;: It is possible that this command does not produce the required effect (in newer versions of &lt;code&gt;R2OpenBUGS&lt;/code&gt;). An alternative that should work is&lt;/p&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach.all(model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(i.e. use the function &lt;code&gt;attach.all&lt;/code&gt;, still from the package &lt;code&gt;R2OpenBUGS&lt;/code&gt;, where the argument in brackets is the name of the &lt;code&gt;BUGS&lt;/code&gt; model for which you want to make all the output directly available on your &lt;code&gt;R&lt;/code&gt; workspace).

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Notice that this operation comes with some risks — every variable that
is currently present in the &lt;tt&gt;R&lt;/tt&gt; workspace with
the same name as a variable included in the object &lt;code&gt;model&lt;/code&gt; will be now
overwritten. Nevertheless, we can now use all the elements of &lt;code&gt;model&lt;/code&gt;
directly; for example we can produce a histogram of the posterior
distribution for &lt;code&gt;theta&lt;/code&gt; simply using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(theta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/03_bcea/solutions_files/figure-html/inspect.bugs4-1.png&#34; width=&#34;460.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
which confirms that, effectively, there is no chance that &lt;code&gt;theta&lt;/code&gt; is
greated than 0.5.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;health-economic-evaluation-in-r-using-bcea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Health economic evaluation in &lt;tt&gt;R&lt;/tt&gt; using &lt;tt&gt;BCEA&lt;/tt&gt;&lt;/h2&gt;
&lt;p&gt;First we load a dataset that has been previously saved. There are
several format to which you can save &lt;tt&gt;R&lt;/tt&gt; data
or &lt;tt&gt;R&lt;/tt&gt; workspaces — in this case we use the
format &lt;code&gt;.RData&lt;/code&gt;. We can load a &lt;code&gt;.RData&lt;/code&gt; file simply using the built-in
&lt;tt&gt;R&lt;/tt&gt; command &lt;code&gt;load&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;Vaccine.RData&amp;quot;)
names(he)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;n_sim&amp;quot;         &amp;quot;n_comparators&amp;quot; &amp;quot;n_comparisons&amp;quot; &amp;quot;delta_e&amp;quot;      
 [5] &amp;quot;delta_c&amp;quot;       &amp;quot;Kmax&amp;quot;          &amp;quot;k&amp;quot;             &amp;quot;ib&amp;quot;           
 [9] &amp;quot;eib&amp;quot;           &amp;quot;kstar&amp;quot;         &amp;quot;best&amp;quot;          &amp;quot;ref&amp;quot;          
[13] &amp;quot;comp&amp;quot;          &amp;quot;step&amp;quot;          &amp;quot;interventions&amp;quot; &amp;quot;delta.e&amp;quot;      
[17] &amp;quot;delta.c&amp;quot;      &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file &lt;code&gt;Vaccine.RData&lt;/code&gt; contains the object &lt;code&gt;he&lt;/code&gt;, which in turns has
several elements in it.&lt;/p&gt;
&lt;p&gt;We can now load the package &lt;tt&gt;BCEA&lt;/tt&gt; (assuming
you have already installed it) and use it to post-process &lt;code&gt;he&lt;/code&gt; and
produce relevant summaries/analyses.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    The practical uses the &lt;strong&gt;current&lt;/strong&gt; version of &lt;tt&gt;BCEA&lt;/tt&gt;, which is available from &lt;a href=&#34;https://cran.r-project.org/web/packages/BCEA/index.html&#34;&gt;CRAN&lt;/a&gt;. However, the &lt;a href=&#34;https://github.com/giabaio/BCEA/tree/dev&#34;&gt;development version&lt;/a&gt; is in the “beta-testing” version and will soon replace the current stable version. As this will be a &lt;strong&gt;major&lt;/strong&gt; update, some of the commands below &lt;em&gt;may&lt;/em&gt; break in the future — differences will be minor and changes will be easy to address.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(BCEA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;BCEA&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:graphics&amp;#39;:

    contour&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ICER=mean(he$delta.c)/mean(he$delta.e) 
ICER&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 20097.59&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing we need to do is to compute the ICER. If you check with
your slides for Lecture 3, you will see that
&lt;span class=&#34;math display&#34;&gt;\[\mbox{ICER} = \frac{\mbox{E}[\Delta_c]}{\mbox{E}[\Delta_e]}\]&lt;/span&gt; and so
we create a new variable &lt;code&gt;ICER&lt;/code&gt; to which we assign as value the ratio of
the mean of the element &lt;code&gt;he$delta.c&lt;/code&gt; to the mean of the element
&lt;code&gt;he$delta.e&lt;/code&gt;. Its value is returned to be 20097.59. The only difficulty
of this part is to realise how to translate the correct formula for the
ICER into &lt;tt&gt;R&lt;/tt&gt; commands (i.e. you need to take
the ratio of the means, not the mean of the ratio!) and to access the
elements &lt;code&gt;delta.e&lt;/code&gt; and &lt;code&gt;delta.c&lt;/code&gt; from inside the main object &lt;code&gt;he&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Secondly, we are asked to compute the Expected Incremental Benefit (EIB)
for a value of the willingness to pay threshold &lt;span class=&#34;math inline&#34;&gt;\(k=30000\)&lt;/span&gt;. Now, from the
Lecture we know that
&lt;span class=&#34;math display&#34;&gt;\[\mbox{EIB} = k\mbox{E}[\Delta_e] - \mbox{E}[\Delta_c]\]&lt;/span&gt; and so we can
easily compute this using the following
&lt;tt&gt;R&lt;/tt&gt; commmands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k=30000
EIB=k*mean(he$delta.e)-mean(he$delta.c)
EIB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2.48131&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because for this willingness to pay threshold &lt;span class=&#34;math inline&#34;&gt;\(\mbox{EIB}&amp;gt;0\)&lt;/span&gt;, then the
new treatment &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt; is more cost-effective than the comparator &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next, we are required to display the cost-effectiveness plane, using the
&lt;tt&gt;BCEA&lt;/tt&gt; built-in command &lt;code&gt;ceplane.plot&lt;/code&gt;. This
gives the following result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ceplane.plot(he,wtp=10000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/03_bcea/solutions_files/figure-html/bcea3-1.png&#34; width=&#34;652.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The reason why &lt;tt&gt;BCEA&lt;/tt&gt; says “ICER=NULL” in the
top-right corner of the graph is because the object &lt;code&gt;he&lt;/code&gt; has been
modified from the standard &lt;tt&gt;BCEA&lt;/tt&gt; output
(effectively, the variable &lt;code&gt;ICER&lt;/code&gt; is computed within a
&lt;tt&gt;BCEA&lt;/tt&gt; object. But because you were required to
compute the ICER yourselves, this has been removed and so &lt;code&gt;ceplane.plot&lt;/code&gt;
does not know what value to use. If you are bothered by this, you can
simply define in &lt;tt&gt;R&lt;/tt&gt; &lt;code&gt;he$ICER=ICER&lt;/code&gt; and re-run
the &lt;code&gt;ceplane.plot&lt;/code&gt; command. Now &lt;tt&gt;BCEA&lt;/tt&gt; will
print on the graph the value of the ICER.&lt;/p&gt;
&lt;p&gt;In any case, because we have selected a different threshold (&lt;span class=&#34;math inline&#34;&gt;\(k=10\,000\)&lt;/span&gt;
instead of &lt;span class=&#34;math inline&#34;&gt;\(k=30,000\)&lt;/span&gt; as before), this time the ICER (the red dot in the
graph) is not included in the “sustainability area” (the grey area in
the plane). For this reason, we can conclude that &lt;em&gt;at this new
threshold&lt;/em&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt; is more cost-effective.&lt;/p&gt;
&lt;p&gt;If we now execute the following command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eib.plot(he, plot.cri=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/03_bcea/solutions_files/figure-html/bcea4-1.png&#34; width=&#34;652.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
we obtain a plot of the EIB for different values of the willingness to
pay. The interpretation of this graph is that for willingness to pay up
to about 20100, then &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt; is the most cost-effective treatment (because
EIB&lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;0\)&lt;/span&gt;). After that, &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt; becomes more cost-effective and EIB&lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;0\)&lt;/span&gt;
indicating that the new treatment has a greater utility than the
comparator.&lt;/p&gt;
&lt;p&gt;Finally, we can use &lt;tt&gt;BCEA&lt;/tt&gt; to show a contour
plot of the economic results, by using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;contour(he)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/03_bcea/solutions_files/figure-html/bcea5-1.png&#34; width=&#34;652.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
The probability that the new intervention (vaccination) is dominated by
the reference (status quo) is estimated by the proportion if points in
the cost-effectiveness plane that lie in the NW quadrant. This is
reported by the graph as 0.169.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear regression Tutorial</title>
      <link>/practical/04_ild/tutorial-regression/</link>
      <pubDate>Tue, 21 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/practical/04_ild/tutorial-regression/</guid>
      <description>


&lt;p&gt;When dealing with individual level data, &lt;em&gt;regression&lt;/em&gt; models are arguably one of the most commonly used tools in statistical analysis. As discussed &lt;a href=&#34;https://gianluca.statistica.it/teaching/intro-stats/regression-to-the-mean.html&#34;&gt;here&lt;/a&gt; (which gives more details and background), the term “regression” was introduced by Francis Galton.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Historical background&lt;/strong&gt; Galton was a very controversial figure. He was a polyscientist, who made contributions to Statistics, Psychology, Sociology, Anthropology and many other sciences. He was the half-cousin of Charles Darwin and was inspired by his work on the origin of species to study hereditary traits in humans, including height, which he used to essentially invent regression. He also established and then financed the “Galton Laboratory” at UCL (that is the first incarnation of the UCL Department of Statistical Science), which Karl Pearson (another important and controversial figure) went on to lead. Alas, both Galton and Pearson wer also a major proponent of eugenics (in fact, Galton is credited with the invention of the term) and has thus left a troubling legacy behind them. You can read more about UCL’s own enquiry &lt;a href=&#34;https://www.ucl.ac.uk/provost/inquiry-history-eugenics-ucl&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Galton worked to study hereditary traits. In particular, he collected data on &lt;span class=&#34;math inline&#34;&gt;\(n=898\)&lt;/span&gt; children from 197 families. The original data are stored in the file &lt;a href=&#34;Galton.txt&#34;&gt;&lt;code&gt;Galton.txt&lt;/code&gt;&lt;/a&gt;. The data comprise the height of each child &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, as well as the height of the father &lt;span class=&#34;math inline&#34;&gt;\(X_{1i}\)&lt;/span&gt; and the mother &lt;span class=&#34;math inline&#34;&gt;\(X_{2i}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt;, all measured in inches.&lt;/p&gt;
&lt;p&gt;Galton’s objective was to find out whether there was some consistent relationship between the outcome and the predictor and, if so, to quantify the strength of this relationship. His original analysis was based on &lt;a href=&#34;https://gianluca.statistica.it/teaching/intro-stats/regression-to-the-mean.html#fn5&#34;&gt;&lt;em&gt;least squares fitting&lt;/em&gt;&lt;/a&gt;. But in general terms, regression can be framed as a statistical model; in its simplest form (which we are using here), we can assume that the sampling variability underlying the observed data can be described by a Normal distribution. This amount to assuming that
&lt;span class=&#34;math display&#34; id=&#34;eq:regmodstats2&#34;&gt;\[\begin{align}
y_i &amp;amp; \sim \mbox{Normal}(\mu_i,\sigma^2) \\
\mu_i &amp;amp; = \beta_0 + \beta_1 X_{1i} + \ldots + \beta_K X_{Ki}.\tag{1}
\end{align}\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,n\)&lt;/span&gt; (and assuming that there are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; “covariates” &lt;span class=&#34;math inline&#34;&gt;\(X_1,\ldots,X_K\)&lt;/span&gt;). This notation highlights the probabilistic nature of the assumed regression relationship between &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X}\)&lt;/span&gt;: we are assuming that the &lt;strong&gt;linear predictor&lt;/strong&gt; of Equation &lt;a href=&#34;#eq:regmodstats2&#34;&gt;(1)&lt;/a&gt; is &lt;em&gt;on average&lt;/em&gt; the best estimate of the outcome, given a specific “profile”, i.e. a set of values for the observed covariates. But we do not impose determinism on this relationship: there is a variance, which is determined by the sampling variability in the observed data and expressed by the population parameter &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;An alternative (but entirely equivalent!) way of writing the regression model (which is perhaps more common in Econometrics than it is in Statistics) is to consider&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}y_i = \beta_0 + \beta_1 X_{1i} + \ldots + \beta_K X_{Ki} + \varepsilon_i,\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i \sim \mbox{Normal}(0,\sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This formulation explicitly describes the assumption mentioned above. The quantity &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt; represents some kind of “white noise”, or, in other words, a random error, that is centered on 0 and whose variance basically depends on the population variability.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;We can follow the model specification described &lt;a href=&#34;https://gianluca.statistica.it/teaching/intro-stats/linearregressionsec.html&#34;&gt;here&lt;/a&gt;. The data can be loaded up in the &lt;tt&gt;R&lt;/tt&gt; workspace using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load the whole dataset
data=read.table(&amp;quot;tutorial-regression/Galton.txt&amp;quot;,header=TRUE)

# Creates the covariates matrix. Notice that the first column is made by &amp;#39;1&amp;#39; and the father&amp;#39;s and mother&amp;#39;s heights are &amp;quot;centered&amp;quot;
X=cbind(rep(1,nrow(data)),scale(data$Father,scale=F),scale(data$Mother,scale=F))
colnames(X)=c(&amp;quot;Intercept&amp;quot;,&amp;quot;Father&amp;quot;,&amp;quot;Mother&amp;quot;)

# Creates the outcome (children&amp;#39;s height)
y=data$Height&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The assumptions about the model for the data and the priors described &lt;a href=&#34;https://gianluca.statistica.it/teaching/intro-stats/linearregressionsec.html&#34;&gt;here&lt;/a&gt; can be coded up into the following &lt;code&gt;BUGS&lt;/code&gt; model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model {
   for(i in 1:n) {
      y[i] ~ dnorm(mu[i],tau)
      # If you are using R2jags, you can take advantage of the &amp;#39;%*%&amp;#39; operator 
      mu[i] &amp;lt;- X[i,]%*%beta
      # If you are using R2OpenBUGS, you need to specify the full linear predictor
      # mu[i] &amp;lt;- X[i,1]*beta[1] + X[i,2]*beta[2] + X[i,3]*beta[3]
   }
   for(k in 1:K) {
      beta[k] ~ dnorm(mu.beta[k],tau.beta[k])
   }
   tau ~ dgamma(a,b)
   sigma &amp;lt;- pow(tau,-.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can save this model into a &lt;code&gt;.txt&lt;/code&gt; file (say, &lt;code&gt;model.txt&lt;/code&gt;) and use it to run &lt;code&gt;BUGS&lt;/code&gt; or &lt;code&gt;JAGS&lt;/code&gt;. Notice that if you’re doing the former, you will need to use the specification of the linear predictor &lt;code&gt;mu[i]&lt;/code&gt; that is currently commented out (ie it is prefixed by a “&lt;code&gt;#&lt;/code&gt;” in the code above). This is because &lt;code&gt;JAGS&lt;/code&gt; implements the operator “&lt;code&gt;%*%&lt;/code&gt;”, which is the sum of the products between the covariate &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; and its coefficient &lt;span class=&#34;math inline&#34;&gt;\(\beta_k\)&lt;/span&gt;. So the notation&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu[i] &amp;lt;- X[i,]%*%beta&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is equivalent to
&lt;span class=&#34;math display&#34;&gt;\[\sum_{k=1}^3 X_{ik}\beta_k\]&lt;/span&gt;
(assuming that &lt;code&gt;X&lt;/code&gt; is a matrix with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; rows and &lt;span class=&#34;math inline&#34;&gt;\(K=3\)&lt;/span&gt; columns and &lt;code&gt;beta&lt;/code&gt; is a vector with length &lt;span class=&#34;math inline&#34;&gt;\(K=3\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;You now need to define the data list, the parameters to be monitored and the other relevant quantities (number of chains, iterations, burn-in — as seen in class). For example, you can use the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Defines the data list
dataList &amp;lt;- list(
  y=y,        # outcome (children&amp;#39;s heights)
  X=X,        # covariates matrix (including a column of &amp;#39;1&amp;#39; for the intercept)
  n=nrow(X),  # number of data points (= rows of X)
  K=ncol(X),  # number of covariates (= columns of X)
  # &amp;quot;hyper-parameters&amp;quot;
  a=0.1,b=0.1,mu.beta=c(65,0,0),tau.beta=c(20,10,10)^-2
)

# MCMC-related variables
n.chains=2                             # 2 chains
n.iter=10000                           # 10000 iterations
n.burnin=5000                          # 5000 to be discarded as burn-in
parameters.to.save=c(&amp;quot;beta&amp;quot;,&amp;quot;sigma&amp;quot;)   # monitored parameters&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, you can use &lt;code&gt;R2OpenBUGS&lt;/code&gt; or &lt;code&gt;R2jags&lt;/code&gt; to run the model. If you’re using the former, the following code would do the job&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(R2OpenBUGS)
model=bugs(data=dataList,model.file=&amp;quot;model.txt&amp;quot;,n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while if you want to use &lt;code&gt;R2jags&lt;/code&gt; you can simply replace the command &lt;code&gt;library(R2OpenBUGS)&lt;/code&gt; with &lt;code&gt;library(R2jags)&lt;/code&gt; and call &lt;code&gt;jags(...)&lt;/code&gt; instead of &lt;code&gt;bugs(...)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With this setup, you should be able to replicate the results shown in the &lt;a href=&#34;https://gianluca.statistica.it/teaching/intro-stats/linearregressionsec.html&#34;&gt;notes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can also use &lt;a href=&#34;galton_regression.R&#34;&gt;this&lt;/a&gt; script, which is annotated with instructions and will allow you to replicate the whole analysis presented in the notes (including the graphs and the post-processing). For simplicity, the &lt;code&gt;BUGS&lt;/code&gt; code is also stored &lt;a href=&#34;model_galton.txt&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt;: &lt;a href=&#34;https://gianluca.statistica.it/teaching/intro-stats/regression.html&#34;&gt;This&lt;/a&gt; document shows a more detailed introduction to the basics of regression and &lt;a href=&#34;https://gianluca.statistica.it/teaching/intro-stats/regression-to-the-mean.html&#34;&gt;this&lt;/a&gt; also describes the issue related with centering covariates in a regression model.&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;The impact of covariate centering on Bayesian computation is also presented in Chapter 2 of &lt;a href=&#34;https://gianluca.statistica.it/book/bmhe/&#34;&gt;BMHE&lt;/a&gt;.

  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Practical 4. Cost-effectiveness analysis with individual level data — SOLUTIONS</title>
      <link>/practical/04_ild/solutions/</link>
      <pubDate>Tue, 21 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/practical/04_ild/solutions/</guid>
      <description>


&lt;p&gt;This document comments more in details the &lt;tt&gt;R&lt;/tt&gt; script used to perform
the whole analysis. Similar results can be obtained using the &lt;tt&gt;BUGS&lt;/tt&gt;
interface directly — but as we mentioned in the class, this is usually
less efficient and so we focus here on the &lt;tt&gt;R&lt;/tt&gt; script.&lt;/p&gt;
&lt;p&gt;The first thing to do is to make sure that you are in the correct
folder. You can check the location of your
&lt;tt&gt;R&lt;/tt&gt; workspace by typing the following command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getwd()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which would return something like&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;[1] &amp;quot;/home/gianluca&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;— the symbol &lt;code&gt;[1]&lt;/code&gt; here indicates the the answer is a vector and the
current rows starts with its first (and, in this case, only) element.&lt;/p&gt;
&lt;p&gt;You can move to different directories by using a command like the
following&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setwd(&amp;quot;/home/gianluca/Mystuff&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which, in this case, would move the workspace in the subfolder
&lt;code&gt;Mystuff&lt;/code&gt;. In &lt;code&gt;Rstudio&lt;/code&gt; you can also use the menus under &lt;code&gt;Session&lt;/code&gt; to
move across the folders on your computer.&lt;/p&gt;
&lt;p&gt;The next step is to load in the &lt;tt&gt;R&lt;/tt&gt; workspace
the relevant “packages”. In this case, this is done using the following
command (that are typeset in red).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(R2OpenBUGS)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can of course load any other package you require at any point in
your script.&lt;/p&gt;
&lt;div id=&#34;question-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 1&lt;/h2&gt;
&lt;p&gt;We now proceed to load the data on costs, using the following
&lt;tt&gt;R&lt;/tt&gt; command (here the hash symbol “&lt;code&gt;#&lt;/code&gt;” indicates a comment)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loads the data on costs only into R from the txt file (list originally prepared for BUGS)
cost.data=source(&amp;quot;cost-data.txt&amp;quot;)$value&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;tt&gt;R&lt;/tt&gt; command &lt;code&gt;source&lt;/code&gt; executes into the
&lt;tt&gt;R&lt;/tt&gt; workspace the commands included in the file
that is used as its argument. In this case, the file &lt;code&gt;cost-data.txt&lt;/code&gt;
contains a &lt;em&gt;list&lt;/em&gt; of values for a set of different variables and the
above comment assigns this list to a new
&lt;tt&gt;R&lt;/tt&gt; variable named &lt;code&gt;cost.data&lt;/code&gt;. Notice here
that we also add the suffix &lt;code&gt;$value&lt;/code&gt; to the &lt;code&gt;source&lt;/code&gt; command. The reason
for this is that in this way, &lt;tt&gt;R&lt;/tt&gt; can strip the
values for the elements of the list contained in the &lt;code&gt;.txt&lt;/code&gt; file from
all the “meta-data”” (i.e. external information that is irrelevant to
us). For example, compare the following output&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost.data=source(&amp;quot;cost-data.txt&amp;quot;)$value
names(cost.data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;N1&amp;quot;    &amp;quot;N2&amp;quot;    &amp;quot;cost1&amp;quot; &amp;quot;cost2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with the one obtained by omitting the &lt;code&gt;$value&lt;/code&gt; suffix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost.data=source(&amp;quot;cost-data.txt&amp;quot;)
names(cost.data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;value&amp;quot;   &amp;quot;visible&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(cost.data$value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;N1&amp;quot;    &amp;quot;N2&amp;quot;    &amp;quot;cost1&amp;quot; &amp;quot;cost2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the first case, the elements of the object &lt;code&gt;cost.data&lt;/code&gt; are the
variables that we need to use as data for the
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;model. In the second one, these are
actually “hidden” inside the object &lt;code&gt;values&lt;/code&gt; that is stored inside the
object &lt;code&gt;cost.data&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We are now ready to start setting everything up to run
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;remotely from our
&lt;tt&gt;R&lt;/tt&gt; session. The first things to do are to
define the relevant inputs to the &lt;tt&gt;BUGS&lt;/tt&gt; function that will do just
that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Runs the BUGS model from R
model.file=&amp;quot;normal-mod.txt&amp;quot;
params &amp;lt;- c(&amp;quot;mu&amp;quot;,&amp;quot;ss&amp;quot;,&amp;quot;ls&amp;quot;,&amp;quot;delta.c&amp;quot;,&amp;quot;dev&amp;quot;,&amp;quot;total.dev&amp;quot;)
n.chains=2
n.burnin=1000
n.iter=2000
n.thin=1
debug=FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first command here defines a string with the path to the file in
which we have saved the model code. This will instruct
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;about where to read the
model assumptions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The second line defines the parameters to be monitored and stores
them (as strings of names) into the vector &lt;code&gt;params&lt;/code&gt; — of course you
can use any naming convention you like; so this vector may be called
&lt;code&gt;parameters&lt;/code&gt;, or &lt;code&gt;x&lt;/code&gt; instead.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The third line defines a numeric variable in which we specify the
number of Markov chains we want to run — in this case 2. It is
usually a good idea to run more than one chain, because by starting
them from different initial values, this helps assessing convergence
to the target posterior distributions of all the relevant variables
in our model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The fourth line defines the number of iterations to be used as
“burn-in”, i.e. to get closer to the core of the target
posterior distributions. In this case, we are selecting 1000 — there
is no reason why 1000 should &lt;em&gt;always&lt;/em&gt; be sufficient, so we should
assess convergence very carefully (more on this later).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The fifth line defines the total number of simulations to be run,
which will be used to obtain the samples that we will use for the
analysis after discarding the burn-in. In this case, we set
&lt;code&gt;n.iter&lt;/code&gt;=2000, which means that, in total, we will run the MCMC for
&lt;span class=&#34;math inline&#34;&gt;\(2\times(2000)=4000\)&lt;/span&gt; simulations (because we have selected
2 chains). Of these, the first &lt;span class=&#34;math inline&#34;&gt;\(2\times 1000=2000\)&lt;/span&gt; will be discarded
as burn-in, which means that the summary statistics will be computed
on a sample of &lt;span class=&#34;math inline&#34;&gt;\(2000\)&lt;/span&gt; simulations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The sixth line defines the “thinning” of the chains. In this case,
&lt;code&gt;n.thin&lt;/code&gt;=1, which means that we are actually storing every single
iteration after the burn-in for our analysis. In general,
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;will save 1 every &lt;code&gt;n.thin&lt;/code&gt;
simulations, so using a thinning level above 1 means that some of
the simulations will be discarded (and consequently, to have the
same overall sample size we will need a longer run of the MCMC).
This may help reduce autocorrelation in our results (see the lecture
slides and both &lt;em&gt;BMHE&lt;/em&gt; and &lt;em&gt;The BUGS Book&lt;/em&gt;, for more details).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the seventh line defines a logical variable &lt;code&gt;debug&lt;/code&gt;. In
this case we set it to the value &lt;code&gt;FALSE&lt;/code&gt;, which means that
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;will be called remotely and we will
not see it in action. If we set &lt;code&gt;debug=TRUE&lt;/code&gt;, then
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;would forcibly take over from
&lt;tt&gt;R&lt;/tt&gt; and we would see it opening its windows
and spitting out its output. This works &lt;strong&gt;under &lt;code&gt;MS Windows&lt;/code&gt; only&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point we are finally ready to call
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;, which we do using the following
command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m=bugs(data=cost.data,inits=NULL,model.file=model.file,parameters.to.save=params,
       n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T,debug=debug)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model is run by &lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;and while this is
happening, we lose access to the &lt;tt&gt;R&lt;/tt&gt; session
(i.e. you cannot use &lt;tt&gt;R&lt;/tt&gt; to make other
calculations while &lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;is running). When
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;is finished, then
&lt;tt&gt;R&lt;/tt&gt; takes over again and if everything has
worked, the object &lt;code&gt;m&lt;/code&gt; is stored in our workspace. We can inspect it by
using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;n.chains&amp;quot;        &amp;quot;n.iter&amp;quot;          &amp;quot;n.burnin&amp;quot;        &amp;quot;n.thin&amp;quot;         
 [5] &amp;quot;n.keep&amp;quot;          &amp;quot;n.sims&amp;quot;          &amp;quot;sims.array&amp;quot;      &amp;quot;sims.list&amp;quot;      
 [9] &amp;quot;sims.matrix&amp;quot;     &amp;quot;summary&amp;quot;         &amp;quot;mean&amp;quot;            &amp;quot;sd&amp;quot;             
[13] &amp;quot;median&amp;quot;          &amp;quot;root.short&amp;quot;      &amp;quot;long.short&amp;quot;      &amp;quot;dimension.short&amp;quot;
[17] &amp;quot;indexes.short&amp;quot;   &amp;quot;last.values&amp;quot;     &amp;quot;isDIC&amp;quot;           &amp;quot;DICbyR&amp;quot;         
[21] &amp;quot;pD&amp;quot;              &amp;quot;DIC&amp;quot;             &amp;quot;model.file&amp;quot;     &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are many variables inside the object &lt;code&gt;m&lt;/code&gt; and we can use the “$”
operator in &lt;tt&gt;R&lt;/tt&gt; to access them. For example,
the command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m$n.sims&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;returns the total number of simulations used by
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;to compute the posterior distributions.&lt;/p&gt;
&lt;p&gt;The first thing to do once the model has run is to check the summary
statistics for the posterior distributions, together with the
convergence diagnostics. We can do this using the &lt;code&gt;print&lt;/code&gt; function. For
example, the command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m,digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;normal-mod.txt&amp;quot;, 
Current: 2 chains, each with 2000 iterations (first 1000 discarded)
Cumulative: n.sims = 2000 iterations saved
             mean    sd    2.5%     25%     50%     75%   97.5% Rhat n.eff
mu[1]       22.72  1.18   20.46   21.90   22.71   23.51   25.10    1  2000
mu[2]       24.53  1.28   22.09   23.69   24.52   25.39   27.12    1  2000
ss[1]      486.18 38.60  418.00  459.65  483.50  509.12  568.61    1  2000
ss[2]      550.47 41.47  477.48  520.80  548.55  576.90  634.80    1  2000
ls[1]        3.09  0.04    3.02    3.06    3.09    3.12    3.17    1  2000
ls[2]        3.15  0.04    3.08    3.13    3.15    3.18    3.23    1  2000
delta.c      1.81  1.74   -1.62    0.63    1.83    3.04    5.06    1  2000
dev[1]    2995.63  2.02 2994.00 2994.00 2995.00 2996.00 3001.00    1  1900
dev[2]    3064.17  2.07 3062.00 3063.00 3064.00 3065.00 3070.00    1  2000
total.dev 6059.77  2.81 6056.00 6058.00 6059.00 6061.00 6067.00    1  2000
deviance  6059.77  2.81 6056.00 6058.00 6059.00 6061.00 6067.00    1  2000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 3.90 and DIC = 6064.00
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;shows the summary table for the nodes (variables) we have chosen to
monitor. The optional input &lt;code&gt;digits=2&lt;/code&gt; instructs
&lt;tt&gt;R&lt;/tt&gt; to show the results using 2 significant
figures. The table reports the mean, standard deviation and selected
quantiles of the posterior distributions. We can typically use the 2.5%
and the 97.5% quantiles to approximate a 95% &lt;em&gt;credible&lt;/em&gt; interval. In
addition, the table reports the values for &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt;, the potential
scale reduction (or Gelman-Rubin statistic) and the effective sample
size (&lt;code&gt;n.eff&lt;/code&gt;) — see the lecture slides and both &lt;em&gt;BMHE&lt;/em&gt; and &lt;em&gt;The BUGS
Book&lt;/em&gt;, for more details.&lt;/p&gt;
&lt;p&gt;Given this analysis, the mean cost is 22.72 for the control arm and
24.53 for the intervention arm. The mean difference in costs is 1.81 —
recall that costs are entered in
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;math inline&#34;&gt;\(\times 1000\)&lt;/span&gt;. We can also manipulate
the simulations to produce further analyses. For example, we can use the
code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(
    m$sims.list$mu[,1],
    m$sims.list$mu[,2],
    xlab=&amp;quot;Population average cost in arm 1 (x 1000)&amp;quot;,
    ylab=&amp;quot;Population average cost in arm 2 (x 1000)&amp;quot;,
    pch=20,
    cex=.6,
    main=&amp;quot;Joint distribution of mean costs&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to display (in the graph below) the posterior joint distribution of the
population average costs in the two arms (the
&lt;tt&gt;R&lt;/tt&gt; script provided for the practical provides
also some description of the graphical parameters used in this call).&lt;/p&gt;
&lt;p&gt;Notice again the use of the “$” operator to access elements of the
object &lt;code&gt;m&lt;/code&gt;. In this case, the element &lt;code&gt;sims.list&lt;/code&gt; is a list containing
&lt;code&gt;all&lt;/code&gt; the simulated values for all the monitored nodes. You can actually
inspect them with the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(m$sims.list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;mu&amp;quot;        &amp;quot;ss&amp;quot;        &amp;quot;ls&amp;quot;        &amp;quot;delta.c&amp;quot;   &amp;quot;dev&amp;quot;       &amp;quot;total.dev&amp;quot;
[7] &amp;quot;deviance&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(m$sims.list$mu[,1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 21.77 23.83 20.31 22.58 23.79 23.76&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;tt&gt;R&lt;/tt&gt; command &lt;code&gt;head&lt;/code&gt; shows by default the
first 6 values of a variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/practical/04_ild/solutions_files/figure-html/inspectBUGS44-1.png&#34; width=&#34;460.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice here the interesting fact that
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;effectively adds a dimension to each
node. So the node &lt;code&gt;mu&lt;/code&gt; is defined in the model code as a vector with 2
values (one for arm 1 and the other for arm 2). But because when
processed by &lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;we record for each of these
two elements &lt;code&gt;n.sims&lt;/code&gt; simulations, then the resulting object turns into
a matrix with &lt;code&gt;n.sims&lt;/code&gt; = 2000 rows and 2 columns. So, in reference to
the code used to generate the plot of the joint distribution of the mean
costs, &lt;code&gt;m$sims.list$mu[,1]&lt;/code&gt; indicates all the rows and the first column
and &lt;code&gt;m$sims.list$mu[,2]&lt;/code&gt; indicates all the rows and the second column of
the matrix &lt;code&gt;m$mu&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;question-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 2&lt;/h2&gt;
&lt;p&gt;We have already included in the vector &lt;code&gt;params&lt;/code&gt; the nodes related to the
deviance — these are &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;total.dev&lt;/code&gt;, which are coded in the
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;model to compute manually the deviance
associated with the underlying Normal model. In practice you do not need
to do this and &lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;will calculate (and
monitor) the deviance automatically — assuming that there is at least
one observed variable (you can think of why this is!).&lt;/p&gt;
&lt;p&gt;Going back to the summary statistics table, we can see that the overall
model deviance is, on average, 6059.77. This is the same value, whether
computed manually (in the node &lt;code&gt;total.dev&lt;/code&gt;) or automatically (in the
node &lt;code&gt;deviance&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;question-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 3&lt;/h2&gt;
&lt;p&gt;The first thing to do now is to load the new dataset, which includes
data on costs as well as utilities. Then we can setup our call to
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;pointing to the correct model file.
Finally, because this new model (based on Gamma-Gamma structure — see
the lecture slides) is more complex, we need to provide
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;with a set of suitable initial values
(identified using a trial-and-error procedure). We do this using the
following &lt;tt&gt;R&lt;/tt&gt; commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loads the data on costs only into R from the txt file (list originally prepared for BUGS)
cost.utility=source(&amp;quot;cost-util-data.txt&amp;quot;)$value

# Specifies the new model file
model.file=&amp;quot;cgeg-mod.txt&amp;quot;

# Loads the 3 sets of initial values from the .txt files
inits1=source(&amp;quot;cgeg-inits1.txt&amp;quot;)$value
inits2=source(&amp;quot;cgeg-inits2.txt&amp;quot;)$value
inits3=source(&amp;quot;cgeg-inits3.txt&amp;quot;)$value
# And combines them into a single list
inits=list(inits1,inits2,inits3)

# Re-defines the list of parameters to be monitored
params=c(&amp;quot;mu.c&amp;quot;,&amp;quot;mu.e&amp;quot;,&amp;quot;delta.c&amp;quot;,&amp;quot;delta.e&amp;quot;,&amp;quot;shape.c&amp;quot;,&amp;quot;shape.e&amp;quot;,&amp;quot;beta&amp;quot;,&amp;quot;INB&amp;quot;,&amp;quot;CEAC&amp;quot;)

# Re-defines the burn-in, number of simulations and number of chains
n.burnin=1000
n.iter=4000         # NB: this adds 3000 simulations to the 1000 of burnin
n.chains=3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we need to be consistent in the definition of the number of
chains and the set up of the &lt;code&gt;inits&lt;/code&gt;. So if we select 3 chains, then
&lt;code&gt;inits&lt;/code&gt; needs to be a list comprising of 3 lists (as in this case) —
failure to do this will result in
&lt;tt&gt;R&lt;/tt&gt; complaining and stopping with an error
message.&lt;/p&gt;
&lt;p&gt;After that, we are ready to call &lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;and run
the new model, which we do using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2=bugs(data=cost.utility,inits=inits,model.file=model.file,parameters.to.save=params,
    n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T,debug=debug)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results can be again printed in the form of a summary table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m2,digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;cgeg-mod.txt&amp;quot;, 
Current: 3 chains, each with 4000 iterations (first 1000 discarded)
Cumulative: n.sims = 9000 iterations saved
              mean    sd    2.5%     25%     50%     75%   97.5% Rhat n.eff
mu.c[1]      23.47  1.99   20.02   22.09   23.31   24.69   27.83    1   700
mu.c[2]      25.99  2.02   22.47   24.57   25.84   27.24   30.38    1  3000
mu.e[1]      74.70  7.13   61.77   69.81   74.28   79.17   90.15    1  1300
mu.e[2]      81.05  8.14   66.62   75.26   80.63   86.45   97.78    1  6100
delta.c       2.52  2.80   -3.02    0.66    2.55    4.38    7.94    1   660
delta.e      -6.34 10.74  -27.39  -13.84   -6.27    1.02   14.81    1  1800
shape.c[1]    1.18  0.09    1.01    1.12    1.18    1.24    1.36    1  3300
shape.c[2]    1.67  0.13    1.43    1.58    1.67    1.76    1.95    1  9000
shape.e[1]    0.41  0.03    0.35    0.39    0.41    0.43    0.46    1  9000
shape.e[2]    0.37  0.03    0.32    0.36    0.37    0.39    0.43    1  9000
beta[1]       0.16  0.02    0.12    0.14    0.16    0.18    0.21    1  1100
beta[2]       0.17  0.02    0.13    0.15    0.17    0.18    0.21    1  3100
INB[1]       -2.52  2.80   -7.94   -4.38   -2.55   -0.66    3.02    1   660
INB[2]       -3.15  3.57  -10.17   -5.55   -3.18   -0.74    3.81    1   730
INB[3]       -3.79  4.46  -12.59   -6.81   -3.82   -0.74    4.87    1   810
INB[4]       -4.42  5.43  -14.98   -8.15   -4.41   -0.68    6.01    1   900
INB[5]       -5.06  6.42  -17.57   -9.50   -5.05   -0.62    7.23    1   970
INB[6]       -5.69  7.44  -20.19  -10.84   -5.68   -0.61    8.70    1  1000
INB[7]       -6.33  8.47  -23.01  -12.18   -6.36   -0.54   10.07    1  1100
INB[8]       -6.96  9.51  -25.67  -13.52   -6.93   -0.46   11.42    1  1100
INB[9]       -7.59 10.56  -28.38  -14.90   -7.60   -0.39   12.83    1  1200
INB[10]      -8.23 11.61  -31.06  -16.23   -8.20   -0.27   14.26    1  1200
INB[11]      -8.86 12.67  -33.72  -17.60   -8.83   -0.18   15.60    1  1300
CEAC[1]       0.18  0.39    0.00    0.00    0.00    0.00    1.00    1   680
CEAC[2]       0.19  0.39    0.00    0.00    0.00    0.00    1.00    1   810
CEAC[3]       0.20  0.40    0.00    0.00    0.00    0.00    1.00    1   910
CEAC[4]       0.21  0.41    0.00    0.00    0.00    0.00    1.00    1  1200
CEAC[5]       0.22  0.41    0.00    0.00    0.00    0.00    1.00    1  1300
CEAC[6]       0.23  0.42    0.00    0.00    0.00    0.00    1.00    1  2200
CEAC[7]       0.23  0.42    0.00    0.00    0.00    0.00    1.00    1  2800
CEAC[8]       0.23  0.42    0.00    0.00    0.00    0.00    1.00    1  2600
CEAC[9]       0.24  0.43    0.00    0.00    0.00    0.00    1.00    1  2000
CEAC[10]      0.24  0.43    0.00    0.00    0.00    0.00    1.00    1  2200
CEAC[11]      0.24  0.43    0.00    0.00    0.00    0.00    1.00    1  2300
deviance   9756.35  4.44 9750.00 9753.00 9756.00 9759.00 9766.00    1  2600

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 9.93 and DIC = 9766.00
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As is possible to see, all values for &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; are essentially 1,
indicating that the model has converged. Autocorrelation is somewhat
large, as confirmed by the fact that the effective sample size is
relatively small in comparison to the actual sample size (9000
simulations) for many if not all the nodes. In real-world analyses, this
would grant further analysis — for example by inspecting the traceplots
of the nodes and possibly running the model for longer or using thinning
(see &lt;em&gt;BMHE&lt;/em&gt;, chapter 4 for &lt;tt&gt;R&lt;/tt&gt; code to create
traceplots).&lt;/p&gt;
&lt;p&gt;The next part consists in manipulating the object &lt;code&gt;m2&lt;/code&gt; in order to
obtain more advanced analyses. Notice that in fact, we can use &lt;code&gt;BCEA&lt;/code&gt; to
do all these, but for the sake of practice, we use our own
&lt;tt&gt;R&lt;/tt&gt; code, in this case.&lt;/p&gt;
&lt;p&gt;The first thing to do is a cost-effectiveness plot. Recall that this is
obtained by plotting the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(\Delta_e,\Delta_c\)&lt;/span&gt;.
Thus, it is sufficient to access the nodes &lt;code&gt;delta.e&lt;/code&gt; and &lt;code&gt;delta.c&lt;/code&gt;
inside the object &lt;code&gt;m2&lt;/code&gt; and the &lt;tt&gt;R&lt;/tt&gt; function
&lt;code&gt;plot&lt;/code&gt;, as shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(m2$sims.list$delta.e,m2$sims.list$delta.c,pch=20,cex=.8,xlab=&amp;quot;Effectiveness differential&amp;quot;,
    ylab=&amp;quot;Cost differential&amp;quot;,main=&amp;quot;Cost-effectiveness plane&amp;quot;)
abline(v=0,lwd=2,col=&amp;quot;gray&amp;quot;)
abline(h=0,lwd=2,col=&amp;quot;gray&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/04_ild/solutions_files/figure-html/inspectBUGS7-1.png&#34; width=&#34;460.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The command &lt;code&gt;abline&lt;/code&gt; can be used to add a line to the graph. The input
&lt;code&gt;v=0&lt;/code&gt; indicates a vertical line at 0, while the input &lt;code&gt;h=0&lt;/code&gt; specifies a
horizontal line at 0. The extra parameters &lt;code&gt;lwd&lt;/code&gt; and &lt;code&gt;col&lt;/code&gt; specify the
width of the line and the color used, respectively (you can use
&lt;code&gt;help(plot)&lt;/code&gt; and &lt;code&gt;help(colours)&lt;/code&gt; in your
&lt;tt&gt;R&lt;/tt&gt; terminal to get more information).&lt;/p&gt;
&lt;p&gt;We can use this graph to assess the uncertainty around the overall
cost-effectiveness analysis; for example, we can visually assess the
proportion of points lying in each of the four quadrants — for instance,
the vast majority seems to be in the NW quadrant, where the new
intervention is more expensive and less effective.&lt;/p&gt;
&lt;p&gt;Next we turn to the analysis of the Incremental Net Benefit (INB). We
are asked to assess its value for a willingness to pay of
&lt;span class=&#34;math inline&#34;&gt;\(k=\)&lt;/span&gt;£500. So, using the code provided in the model file, we
can use the following &lt;tt&gt;R&lt;/tt&gt; commands to identify
the corresponding index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;K=numeric()
K.space=0.1
for (j in 1:11) {
    K[j]=(j-1)*K.space
}
idx=which(K==0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Firstly, we recreate in the &lt;tt&gt;R&lt;/tt&gt; workspace the
vector of possible willingness to pay thresholds, &lt;code&gt;K&lt;/code&gt;. Unlike
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;, &lt;tt&gt;R&lt;/tt&gt; requires
us to define any non-scalar quantity before we can use it, e.g. inside a
loop. We can do this using the command &lt;code&gt;K=numeric()&lt;/code&gt;, which effectively
creates a new variable &lt;code&gt;K&lt;/code&gt; and tells &lt;tt&gt;R&lt;/tt&gt; to
expect a numeric vector (which can also be length 1, i.e. be a scalar).
Next, we set the step of 0.1 (=£100) in the variable &lt;code&gt;K.space&lt;/code&gt; and use
it to fill in the vector &lt;code&gt;K&lt;/code&gt;. Notice that in
&lt;tt&gt;R&lt;/tt&gt; we create loops using the &lt;code&gt;for&lt;/code&gt; function,
which takes as arguments&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the index (in this case &lt;code&gt;j&lt;/code&gt;);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the starting point (in this case 1);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the ending point (in this case 11).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;tt&gt;R&lt;/tt&gt; notation is fairly straightforward as
basically instructs the computer to move &lt;code&gt;j&lt;/code&gt; “in 1 to 11” — this means
that &lt;tt&gt;R&lt;/tt&gt; will repeat the commands specified
inside the loop (delimited by the two curly brackets “{” and “}”) upon
varying the index &lt;code&gt;j&lt;/code&gt; from 1 to 2, 3, …, 11.&lt;/p&gt;
&lt;p&gt;The resulting vector is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;K&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the variable &lt;code&gt;idx&lt;/code&gt; is computed as the element of &lt;code&gt;K&lt;/code&gt; that is exactly
equal to 0.5, as defined in the function &lt;code&gt;which&lt;/code&gt; above. Notice that in
logical functions (e.g. &lt;code&gt;if&lt;/code&gt;, &lt;code&gt;while&lt;/code&gt;, &lt;code&gt;which&lt;/code&gt;),
&lt;tt&gt;R&lt;/tt&gt; requires that the equality condition is
represented by “&lt;code&gt;==&lt;/code&gt;” (notice the double sign of equality).&lt;/p&gt;
&lt;p&gt;At this point, we can use the &lt;tt&gt;R&lt;/tt&gt; built-in
functions &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;quantile&lt;/code&gt; to estimate the average and 95% interval
for the INB at &lt;span class=&#34;math inline&#34;&gt;\(k=\mbox{\pounds}500\)&lt;/span&gt;. We can do this using the following
commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(m2$sims.list$INB[,idx])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] -5.691401&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(m2$sims.list$INB[,idx],.025)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     2.5% 
-20.19075 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(m2$sims.list$INB[,idx],.975)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  97.5% 
8.70215 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice again that &lt;code&gt;INB&lt;/code&gt; is defined as a vector in the
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;model (with one value for each value of
&lt;code&gt;K&lt;/code&gt;). This means that resulting object from the
&lt;span&gt;&lt;span&gt;&lt;tt&gt;BUGS&lt;/tt&gt;&lt;/span&gt;&lt;/span&gt;simulations is turned into a matrix
(increased by one dimension) and what we need is to access all the rows
of the &lt;code&gt;idx&lt;/code&gt;-th column (which is associated with
&lt;span class=&#34;math inline&#34;&gt;\(k=\mbox{\pounds}500\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;We can manipulate the simulation further to obtain an estimate of the
probability that the INB is positive at this threshold. This can be
easily obtained by computing the proportion of simulations for which
this is true, which we do using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(m2$sims.list$INB[,idx]&amp;gt;0)/m2$n.sims&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.227&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logical expression &lt;code&gt;m2$sims.list$INB[,idx]&amp;gt;0&lt;/code&gt; checks whether each of
the &lt;code&gt;m2$n.sims&lt;/code&gt; simulations in the &lt;code&gt;idx&lt;/code&gt;-th column of the matrix
&lt;code&gt;m2$sims.list$INB&lt;/code&gt; is positive. If so, it returns a 1, while if not, it
will return a 0. Thus to sum over all these 1s and 0s and then divide by
the total number of simulations, will provide an estimate of the
probability that INB is positive.&lt;/p&gt;
&lt;p&gt;Finally, we can compute and plot the Cost-Effectiveness Acceptability
Curve (CEAC), using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CEAC=numeric()
for (i in 1:length(K)) {
    CEAC[i]=mean(m2$sims.list$CEAC[,i]) 
}
plot(K,CEAC,xlab=&amp;quot;Willingness to pay (x 1000)&amp;quot;,ylab=&amp;quot;Cost-effectiveness acceptability curve&amp;quot;,
     main=&amp;quot;&amp;quot;,t=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/04_ild/solutions_files/figure-html/inspectBUGS12-1.png&#34; width=&#34;460.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Firstly, recall that the CEAC is in fact the probability that INB is
positive, for each selected value of the willingness to pay. So we
define a vector &lt;code&gt;CEAC&lt;/code&gt;, in which we want to store the best estimate
(i.e. the mean of the posterior distribution) from our model. To do
this, we create a &lt;code&gt;for&lt;/code&gt; loop. Notice that this time, we are being a bit
clever and instead of specifying the ending point of the loop, we let
&lt;tt&gt;R&lt;/tt&gt; compute it itself by defining it equal to
&lt;code&gt;length(K)&lt;/code&gt; — that is, obviously, the length of the vector &lt;code&gt;K&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once the vector &lt;code&gt;CEAC&lt;/code&gt; has been filled in, we can simply plot it agains
the values of the willingness to pay using the &lt;code&gt;plot&lt;/code&gt; function. Notice
that, by default, &lt;code&gt;plot&lt;/code&gt; uses open dots to display the selected values.
To overwrite this behaviour, we need to specify the option &lt;code&gt;t=l&lt;/code&gt;, which
instructs &lt;tt&gt;R&lt;/tt&gt; to use a line (curve) instead.&lt;/p&gt;
&lt;p&gt;We can link the CEAC with the Cost-Effectiveness plane by noticing that the former is essentially the proportion of “futures” (i.e. simulated points in the latter) that lie below the line &lt;span class=&#34;math inline&#34;&gt;\(\Delta_e=k\Delta_c\)&lt;/span&gt;, for a given willingness to pay threshold, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/practical/04_ild/solutions_files/figure-html/inspectBUGS13-1.png&#34; width=&#34;460.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
For example, the graph above plots the line &lt;span class=&#34;math inline&#34;&gt;\(\Delta_e=0.4\Delta_c\)&lt;/span&gt;, which implies we’re considering &lt;span class=&#34;math inline&#34;&gt;\(k=0.4\)&lt;/span&gt; (recall that the costs are scaled by £1000, so in fact, we mean &lt;span class=&#34;math inline&#34;&gt;\(k=\)&lt;/span&gt;£400!). As is possible to see, most of the points lie &lt;strong&gt;above&lt;/strong&gt; the willingness to pay. In other words, it is much more likely that the intervention &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt; cost-effective. The proportion of points below the line (&lt;a href=&#34;https://gianluca.statistica.it/book/bmhe/&#34;&gt;BMHE&lt;/a&gt; refers to this as the “sustainability area”) is below 0.5 and it indicates, for that given willingness to pay, the CEAC.&lt;/p&gt;
&lt;p&gt;In reality, we do not really need to perform the economic evaluation “by hand”, ie programming the code above to compute the various health economic summaries and graphical representations. This is the point of &lt;code&gt;BCEA&lt;/code&gt;!… In this case, the output of the &lt;code&gt;BUGS&lt;/code&gt; model does contain the population average measures of costs and effectiveness — these are the parameters &lt;code&gt;mu.c&lt;/code&gt; and &lt;code&gt;mu.e&lt;/code&gt;. We can simply pass these as “input” to &lt;code&gt;bcea&lt;/code&gt; and obtain all the necessary and relevant economic summaries and plots. For instance, we can use the following code to create two matrices &lt;code&gt;e&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;, which we can then feed to &lt;code&gt;BCEA&lt;/code&gt; as input.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Defines the population average &amp;quot;effectiveness measures&amp;quot; (NB: days in hospital are bad so take -e!)
e = -m2$sims.list$mu.e
# Defines the population average &amp;quot;cost measures&amp;quot;
c = m2$sims.list$mu.c
# Calls BCEA
library(BCEA)
# Runs the function `bcea` to obtain the health economics summary
he=bcea(
  # Defines the inputs
  e,c,
  # Labels for the two intervention groups
  interventions=c(&amp;quot;Standard case management&amp;quot;,&amp;quot;Intensive case management&amp;quot;),
  # Defines the &amp;quot;reference&amp;quot; interventions (the one that is evaluated)
  ref=2,
  # Selects the maximum value for the willingness to pay threshold
  Kmax=1000
)
# Now can summarise the decision problem
summary(he)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NB: k (wtp) is defined in the interval [0 - 1000]

Cost-effectiveness analysis summary 

Reference intervention:  Intensive case management
Comparator intervention: Standard case management

Standard case management dominates for all k in [0 - 1000] 


Analysis for willingness to pay parameter k = 1000

                          Expected utility
Standard case management            -74728
Intensive case management           -81073

                                                          EIB    CEAC    ICER
Intensive case management vs Standard case management -6345.2 0.28222 -0.3973

Optimal intervention (max expected utility) for k = 1000: Standard case management
           
EVPI 1834.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Or plot the results
ceplane.plot(he) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/04_ild/solutions_files/figure-html/hebcea-1.png&#34; width=&#34;55%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-model-using-r2jags&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running the model using R2jags&lt;/h1&gt;
&lt;p&gt;In general, there aren’t many differences in running a model using &lt;code&gt;JAGS&lt;/code&gt; or &lt;code&gt;BUGS&lt;/code&gt;. In this particular case, however, a “vanilla” run of the code in the file &lt;a href=&#34;IPD_analysis.R&#34;&gt;&lt;code&gt;IPD_analysis.R&lt;/code&gt;&lt;/a&gt; may give some interesting inconsistency.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    You do &lt;strong&gt;NOT&lt;/strong&gt; need to run the model using &lt;code&gt;R2jags&lt;/code&gt; — this is just so you are aware of the potential issues. Or, of course, in case you &lt;em&gt;are&lt;/em&gt; running &lt;code&gt;JAGS&lt;/code&gt; and have encountered this first hand!
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;The script basically is identical — the only differences are that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You load the package &lt;code&gt;R2jags&lt;/code&gt; instead of &lt;code&gt;R2OpenBUGS&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;You call the function &lt;code&gt;jags&lt;/code&gt; instead of the function &lt;code&gt;bugs&lt;/code&gt;. Specifically, the call to &lt;code&gt;jags&lt;/code&gt; should &lt;strong&gt;not&lt;/strong&gt; include the option &lt;code&gt;debug&lt;/code&gt;, which is specific to &lt;code&gt;R2OpenBUGS&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, if you run the script and call&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.file=&amp;quot;normal-mod.txt&amp;quot;
params &amp;lt;- c(&amp;quot;mu&amp;quot;,&amp;quot;ss&amp;quot;,&amp;quot;ls&amp;quot;,&amp;quot;delta.c&amp;quot;,&amp;quot;dev&amp;quot;,&amp;quot;total.dev&amp;quot;)
n.chains=2
n.burnin=1000
n.iter=2000
n.thin=1
m=jags(data=cost.data,inits=NULL,model.file=model.file,parameters.to.save=params,
       n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;interestingly the summary table looks like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m,digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;normal-mod.txt&amp;quot;, fit using jags,
 2 chains, each with 2000 iterations (first 1000 discarded)
 n.sims = 2000 iterations saved
           mu.vect sd.vect     2.5%      25%      50%      75%    97.5% Rhat n.eff
delta.c       1.63    9.35   -16.96    -4.54     1.86     7.93    19.73 1.01  2000
dev[1]     3583.82  378.94  2994.04  3200.21  3815.87  3925.86  3934.52 4.76     2
dev[2]     3891.97   67.47  3749.52  3836.38  3928.69  3942.13  3972.89 3.48     3
ls[1]         4.36    0.71     3.07     3.77     4.81     4.98     4.99 3.77     2
ls[2]         4.87    0.10     4.65     4.79     4.93     4.95     5.00 3.46     3
mu[1]        22.70    5.93    10.01    19.78    22.66    25.39    36.14 1.22   110
mu[2]        24.33    7.23    10.15    19.65    24.39    29.46    38.22 1.09   390
ss[1]     12042.81 9274.28   463.50  1888.71 15634.44 21181.73 21746.46 4.19     2
ss[2]     17410.39 3360.54 10968.97 14388.13 19129.83 19906.17 21902.75 3.50     3
total.dev  7475.79  339.34  6924.63  7142.32  7658.96  7760.59  7862.98 3.87     2
deviance   7475.79  339.34  6924.63  7142.32  7658.96  7760.59  7862.98 3.87     2

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 16814.4 and DIC = 24290.2
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exact same model, with the exact same data, shows different results with evidence of &lt;strong&gt;very&lt;/strong&gt; poor mixing in the chains (high values of &lt;code&gt;Rhat&lt;/code&gt; and very low values for &lt;code&gt;n.eff&lt;/code&gt;)!&lt;/p&gt;
&lt;p&gt;The reason for this is in the different way in which &lt;code&gt;OpenBUGS&lt;/code&gt; and &lt;code&gt;JAGS&lt;/code&gt; handle the generation of initial values: the former uses a random draw from the prior distribution, while the latter uses values that are restricted to be in the proximity of the mean of the prior distribution&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this case, &lt;code&gt;R2OpenBUGS&lt;/code&gt; does a better job at selecting initial values that are closer to the main mass in the posterior distribution, which means that the
process converges much more easily and quickly. The process can be “saved” either by running the chains for longer, or by selecting “better” initial values. For instance, running the code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m=jags(data=cost.data,inits=NULL,model.file=model.file,parameters.to.save=params,
       n.chains=n.chains,n.iter=10000,n.burnin=9000,n.thin=n.thin,DIC=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(which creates 10000 simulations, discards the first 9000 and thus saves 1000 per chain — or 2000 in total). The summary statistics look &lt;em&gt;much&lt;/em&gt; better now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m,digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;normal-mod.txt&amp;quot;, fit using jags,
 2 chains, each with 10000 iterations (first 9000 discarded)
 n.sims = 2000 iterations saved
          mu.vect sd.vect    2.5%     25%     50%     75%   97.5% Rhat n.eff
delta.c      1.90    1.74   -1.53    0.71    1.91    3.03    5.30 1.00   570
dev[1]    2995.58    2.02 2993.68 2994.20 2994.94 2996.28 3000.71 1.01  2000
dev[2]    3064.08    1.86 3062.30 3062.79 3063.55 3064.69 3068.71 1.01  1100
ls[1]        3.09    0.04    3.02    3.07    3.09    3.12    3.17 1.00  2000
ls[2]        3.15    0.04    3.08    3.13    3.15    3.18    3.22 1.01  1500
mu[1]       22.68    1.19   20.34   21.92   22.68   23.48   24.99 1.00   870
mu[2]       24.58    1.25   22.15   23.75   24.57   25.42   27.02 1.00  1200
ss[1]      487.75   37.82  418.52  461.10  485.74  511.92  569.74 1.00  2000
ss[2]      549.30   40.44  476.95  520.62  547.32  577.18  632.35 1.01  1600
total.dev 6059.66    2.75 6056.41 6057.67 6058.93 6060.96 6066.85 1.00  2000
deviance  6059.66    2.75 6056.41 6057.67 6058.93 6060.96 6066.85 1.00  2000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.8 and DIC = 6063.5
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the &lt;code&gt;Rhat&lt;/code&gt; statistics are below 1.1 and the &lt;code&gt;n.eff&lt;/code&gt; values are much closer, in general, to the “nominal” sample size &lt;code&gt;n.sims=2000&lt;/code&gt;. Even better, we can pass “reasonable” initial values and ensure even better and quicker convergence. For instance, we could use the files &lt;a href=&#34;normal-inits1.txt&#34;&gt;&lt;code&gt;normal-initis1.txt&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;normal-inits2.txt&#34;&gt;&lt;code&gt;normal-inits2.txt&lt;/code&gt;&lt;/a&gt;, which can be loaded into the workspace and stored in a &lt;code&gt;list&lt;/code&gt; using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inits=list(source(&amp;quot;normal-inits1.txt&amp;quot;)$value,source(&amp;quot;normal-inits2.txt&amp;quot;)$value)
inits&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]]$mu
## [1] 0.316036 1.282370
## 
## [[1]]$ls
## [1] 0.437099 0.493518
## 
## 
## [[2]]
## [[2]]$mu
## [1] 1.73574 1.31659
## 
## [[2]]$ls
## [1] -1.96202 -1.06980&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can run the original model by specifying these&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m=jags(data=cost.data,inits=inits,model.file=model.file,parameters.to.save=params,
       n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and see that convergence is &lt;em&gt;not&lt;/em&gt; an issue, even with simply &lt;code&gt;n.iter=&lt;/code&gt; 2000 total iterations and &lt;code&gt;n.burnin=&lt;/code&gt; 1000 discarded as burn-in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m,digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;normal-mod.txt&amp;quot;, fit using jags,
 2 chains, each with 2000 iterations (first 1000 discarded)
 n.sims = 2000 iterations saved
          mu.vect sd.vect    2.5%     25%     50%     75%   97.5% Rhat n.eff
delta.c      1.92    1.73   -1.39    0.75    1.94    3.07    5.33 1.00  2000
dev[1]    2995.61    2.03 2993.67 2994.16 2994.98 2996.31 3001.05 1.00  2000
dev[2]    3064.19    1.84 3062.30 3062.85 3063.58 3065.00 3068.91 1.00  1400
ls[1]        3.09    0.04    3.02    3.07    3.09    3.12    3.17 1.01   210
ls[2]        3.15    0.04    3.08    3.13    3.15    3.18    3.23 1.00   810
mu[1]       22.65    1.20   20.22   21.85   22.63   23.43   25.01 1.00  1100
mu[2]       24.57    1.26   22.12   23.73   24.60   25.43   27.06 1.00  2000
ss[1]      486.70   37.71  418.45  461.68  484.59  510.23  568.02 1.01   210
ss[2]      551.06   42.28  473.12  520.73  549.09  578.76  641.82 1.00   820
total.dev 6059.79    2.67 6056.44 6057.71 6059.21 6061.27 6066.09 1.00  1400
deviance  6059.79    2.67 6056.44 6057.71 6059.21 6061.27 6066.09 1.00  1400

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 3.6 and DIC = 6063.3
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The &lt;a href=&#34;https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/jags_user_manual.pdf/download&#34;&gt;&lt;code&gt;JAGS&lt;/code&gt; manual&lt;/a&gt; states on page 16 that “&lt;em&gt;initial values are not supplied by the user, then each parameter chooses its own initial value based on the values of its parents. The initial value is chosen to be a ‘typical value’ from the prior distribution. The exact meaning of ‘typical value’ depends on the distribution of the stochastic node, but is usually the mean, median, or mode&lt;/em&gt;”&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 5. Evidence synthesis and decision models — SOLUTIONS</title>
      <link>/practical/05_ald/solutions/</link>
      <pubDate>Tue, 21 Jun 2022 12:15:00 +0000</pubDate>
      <guid>/practical/05_ald/solutions/</guid>
      <description>


&lt;div id=&#34;decision-modelling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Decision modelling&lt;/h2&gt;
&lt;p&gt;Once you have checked the model code in the file &lt;code&gt;EvSynth.txt&lt;/code&gt;, you can concentrate on the &lt;tt&gt;R&lt;/tt&gt; script. The first part
is fairly simple.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Influenza example --- source: Cooper et al (2004); Baio (2012)

# Sets up the working directory to the current one
working.dir &amp;lt;- getwd()
setwd(working.dir)

# Defines the data
# Number of interventions (t=0: control; t=1: prophylactic use of Neuramidase Inhibitors (NI) 
T &amp;lt;- 2                  

# Evidence synthesis on effectiveness of NIs prophylaxis vs placebo
r0 &amp;lt;- r1 &amp;lt;- n0 &amp;lt;- n1 &amp;lt;- numeric()   # defines observed cases &amp;amp; sample sizes
r0 &amp;lt;- c(34,40,9,19,6,34)
r1 &amp;lt;- c(11,7,3,3,3,4)
n0 &amp;lt;- c(554,423,144,268,251,462)
n1 &amp;lt;- c(553,414,144,268,252,493)
S &amp;lt;- length(r0)             # number of relevant studies

# Evidence synthesis on incidence of influenza in healthy adults (under t=0)
x &amp;lt;- m &amp;lt;- numeric()         # defines observed values for baseline risk
x &amp;lt;- c(0,6,5,6,25,18,14,3,27)
m &amp;lt;- c(23,241,159,137,519,298,137,24,132)
H &amp;lt;- length(x)

# Data on costs
unit.cost.drug &amp;lt;- 2.4       # unit (daily) cost of NI
length.treat &amp;lt;- 6*7     # 6 weeks course of treatment
c.gp &amp;lt;- 19          # cost of GP visit to administer prophylactic NI
vat &amp;lt;- 1.175            # VAT
c.ni &amp;lt;- unit.cost.drug*length.treat*vat 

# Informative prior on cost of influenza 
mu.inf &amp;lt;- 16.78         # mean cost of influenza episode
sigma.inf &amp;lt;- 2.34       # sd cost of influenza episode
tau.inf &amp;lt;- 1/sigma.inf^2    # precision cost of influenza episode&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The First couple of lines set up the working directory. In previous practicals, we have seen how you can do this by setting a path to the
folder you want to use. In this case, we first define a variable &lt;code&gt;working.dir&lt;/code&gt;, which we set equal to the current directory (accessed by
the &lt;tt&gt;R&lt;/tt&gt; command &lt;code&gt;getwd()&lt;/code&gt;). The second line is actually not necessary, strictly speaking, because we already are in the
current directory. But this shows, again, how we can use &lt;code&gt;getwd()&lt;/code&gt; and &lt;code&gt;setwd()&lt;/code&gt; to move across the folders in our computer. Notice that we are using in the script the assign “&lt;code&gt;-&amp;gt;&lt;/code&gt;” sign, while in previous practicals we have used the equal “&lt;code&gt;=&lt;/code&gt;” sign. For all intents and purposes,
&lt;tt&gt;R&lt;/tt&gt; considers them as meaning the same thing.&lt;/p&gt;
&lt;p&gt;Then we start to define the data. Some of the variables we need to define are simple scalars, e.g. &lt;code&gt;T &amp;lt;- 2&lt;/code&gt;, the number of interventions.
Others are vectors, in which case we need to first define them as &lt;code&gt;numeric()&lt;/code&gt;. Notice that you can cascade the &lt;code&gt;-&amp;gt;&lt;/code&gt; operator as in
&lt;code&gt;r0 &amp;lt;- r1 &amp;lt;- n0 &amp;lt;- n1 &amp;lt;- numeric()&lt;/code&gt;, which defines several objects as equal to each other and to an empty vector.&lt;/p&gt;
&lt;p&gt;The next bit of code defines a &lt;tt&gt;R&lt;/tt&gt; function that we will use to compute the value of the parameters to associate with a logNormal distribution so that the mean and standard deviation &lt;em&gt;on the natural scale&lt;/em&gt; are (approximately) equal to some input values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Informative prior on length of influenza episodes
## Compute the value of parameters (mulog,sigmalog) for a logNormal 
## distribution to have mean and sd (m,s)
lognPar &amp;lt;- function(m,s) {
  s2 &amp;lt;- s^2
  mulog &amp;lt;- log(m) - 0.5 * log(1+s2/m^2)
  s2log &amp;lt;- log(1+(s2/m^2))
  sigmalog &amp;lt;- sqrt(s2log)
  list(mulog = mulog, sigmalog = sigmalog)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;tt&gt;R&lt;/tt&gt; functions are defined by the keyword &lt;code&gt;function&lt;/code&gt; and may or may not have arguments. If you want to specify a function without argument, you can use the following code: &lt;code&gt;myfn &amp;lt;- function()&lt;/code&gt;. The commands included between the two curly brackets &lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt; are those that the function will execute.&lt;/p&gt;
&lt;p&gt;In this case, we are specifying that the function called &lt;code&gt;lognPar&lt;/code&gt; has two inputs (arguments). &lt;code&gt;m&lt;/code&gt; is the mean that you want your logNormal
distribution to have on the natural scale, while &lt;code&gt;s&lt;/code&gt; indicates its intended standard deviation. The function proceeds to first defining the
variance &lt;code&gt;s2&lt;/code&gt; by squaring the standard deviation; then it creates a new variable &lt;code&gt;mulog&lt;/code&gt; defined as the mean on the log scale (cfr. &lt;a href=&#34;https://egon.stats.ucl.ac.uk/static/stat0019/slides/07_ALD/&#34;&gt;Lecture 7&lt;/a&gt;) in terms of the mean and standard deviation on the natural scale; then
it creates &lt;code&gt;s2log&lt;/code&gt;, the variance on the log scale, as well as &lt;code&gt;sigmalog&lt;/code&gt;, the standard deviation on the log scale. Finally, the variables that we want the function to output are included in a (named) list. When this code is “sourced” (or executed), &lt;code&gt;lognPar&lt;/code&gt; becomes available to your &lt;tt&gt;R&lt;/tt&gt; workspace and for example you can use it using a command like the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lognPar(3,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$mulog
[1] 0.9147499

$sigmalog
[1] 0.6064031&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- rlnorm(10000,lognPar(3,2)$mulog,lognPar(3,2)$sigmalog)
c(mean(x),sd(x),quantile(x,0.025),quantile(x,0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                         2.5%     97.5% 
2.9924542 2.0223211 0.7648183 8.1689680 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which returns the values you should use to model a logNormal distribution so that its mean and standard deviation match your intended
values. You can check that all works the way you want by simulating a variable &lt;code&gt;x&lt;/code&gt; using the &lt;tt&gt;R&lt;/tt&gt; built-in function &lt;code&gt;rlnorm&lt;/code&gt; — in this case we can do 10000 simulations using &lt;code&gt;lognPar(3,2)$mulog&lt;/code&gt; as the mean and &lt;code&gt;lognPar(3,2)$sigmalog&lt;/code&gt; as the standard deviation. The summary statistics provided above shows that effectively this works perfectly.&lt;/p&gt;
&lt;p&gt;In fact, we can use the newly created function to complete the definition of the main data for our model, as shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m.l &amp;lt;- 8.2                              # original value in the paper: 8.2
s.l &amp;lt;- sqrt(2)                          # original value in the paper: sqrt(2)
mu.l &amp;lt;- lognPar(m.l,s.l)$mulog          # mean time to recovery (log scale)
sigma.l &amp;lt;- lognPar(m.l,s.l)$sigmalog    # sd time to recovery (log scale)
tau.l &amp;lt;- 1/sigma.l^2                    # precision time to recovery (log scale)

# Parameters of unstructured effects
mean.alpha &amp;lt;- 0
sd.alpha &amp;lt;- sqrt(10)
prec.alpha &amp;lt;- 1/sd.alpha^2
mean.mu.delta &amp;lt;- 0
sd.mu.delta &amp;lt;- sqrt(10)
prec.mu.delta &amp;lt;- 1/sd.mu.delta^2
mean.mu.gamma &amp;lt;- 0
sd.mu.gamma &amp;lt;- 1000
prec.mu.gamma &amp;lt;- 1/sd.mu.gamma^2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All these are fairly simple. The only thing that is perhaps worth noticing is that the &lt;tt&gt;BUGS&lt;/tt&gt; model will use some Normal and logNormal distributions and so we will need to use precisions. For this reason, we create for example the quantity &lt;code&gt;tau.l&lt;/code&gt;, which is 1 divided by a variance (i.e. a precision), which we can directly use.&lt;/p&gt;
&lt;p&gt;We can now call &lt;tt&gt;BUGS&lt;/tt&gt; and run the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepares to launch OpenBUGS
library(R2OpenBUGS)

# Creates the data list
data &amp;lt;- list(S=S,H=H,r0=r0,r1=r1,n0=n0,n1=n1,x=x,m=m,mu.inf=mu.inf,tau.inf=tau.inf,
             mu.l=mu.l,tau.l=tau.l,mean.alpha=mean.alpha,prec.alpha=prec.alpha,
             mean.mu.delta=mean.mu.delta,prec.mu.delta=prec.mu.delta,
             mean.mu.gamma=mean.mu.gamma,prec.mu.gamma=prec.mu.gamma)

# Points to the txt file where the OpenBUGS model is saved
filein &amp;lt;- &amp;quot;EvSynth.txt&amp;quot;

# Defines the parameters list
params &amp;lt;- c(&amp;quot;p1&amp;quot;,&amp;quot;p2&amp;quot;,&amp;quot;rho&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;c.inf&amp;quot;,&amp;quot;alpha&amp;quot;,&amp;quot;delta&amp;quot;,&amp;quot;gamma&amp;quot;)

# Creates a function to draw random initial values 
inits &amp;lt;- function(){
    list(alpha=rnorm(S,0,1),delta=rnorm(S,0,1),mu.delta=rnorm(1),
       sigma.delta=runif(1),gamma=rnorm(H,0,1),mu.gamma=rnorm(1),
       sigma.gamma=runif(1),c.inf=rnorm(1))
}

# Sets the number of iterations, burnin and thinning
n.iter &amp;lt;- 10000
n.burnin &amp;lt;- 9500
n.thin &amp;lt;- 20

# Finally calls OpenBUGS to do the MCMC run and saves results to the object &amp;quot;es&amp;quot;
es &amp;lt;- bugs(data=data,inits=inits,parameters.to.save=params,model.file=filein,
    n.chains=2, n.iter, n.burnin, n.thin, DIC=TRUE, working.directory=working.dir)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most of these commands should be fairly familiar by now. We first load &lt;code&gt;R2OpenBUGS&lt;/code&gt;, then include all the relevant data in a list and define
the path to the file where the model is saved (here we assume that the file is in the working directory) and then define the parameters to
monitor.&lt;/p&gt;
&lt;p&gt;When it comes to defining the inital values, we use this time a bespoke function that we create to simulate suitable values for the variables we want to initialise. For example, the &lt;tt&gt;BUGS&lt;/tt&gt; model defines&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;...
# Evidence synthesis for efffectiveness of NIs
    for (s in 1:S) { 
        r0[s] ~ dbin(pi0[s],n0[s])
      ...
        delta[s] ~ dnorm(mu.delta,tau.delta)
        alpha[s] ~ dnorm(mean.alpha,prec.alpha)
    } 
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which implies that the node &lt;code&gt;alpha&lt;/code&gt; is a vector with length &lt;code&gt;S&lt;/code&gt;. Thus, when we initialise it, we need to provide &lt;tt&gt;R&lt;/tt&gt; and &lt;tt&gt;BUGS&lt;/tt&gt; with &lt;code&gt;S&lt;/code&gt; values. We do this in our &lt;code&gt;inits&lt;/code&gt; function by defining &lt;code&gt;alpha=rnorm(S,0,1)&lt;/code&gt; — this creates &lt;code&gt;S&lt;/code&gt; random draws from a Uniform(0,1)
distribution.&lt;/p&gt;
&lt;p&gt;In fact, we are &lt;em&gt;not&lt;/em&gt; initialising all the unobserved nodes associated with a probability distribution: if you look at the &lt;tt&gt;BUGS&lt;/tt&gt; model code, you will notice that the nodes &lt;code&gt;phi&lt;/code&gt; and &lt;code&gt;l&lt;/code&gt; are also of this kind and so, technically, they need initialisation. If we do not do anything, &lt;tt&gt;BUGS&lt;/tt&gt; will take care of it itself. But we can simply add them by simply modifying the &lt;code&gt;inits&lt;/code&gt; function as following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creates a function to draw random initial values 
inits &amp;lt;- function(){
    list(alpha=rnorm(S,0,1),delta=rnorm(S,0,1),mu.delta=rnorm(1),
       sigma.delta=runif(1),gamma=rnorm(H,0,1),mu.gamma=rnorm(1),
       sigma.gamma=runif(1),c.inf=rnorm(1)
         # Now add the new variables
         , # Make sure you include a &amp;#39;comma&amp;#39; between variables! 
         l=runif(1),phi=rnorm(1)
         )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we need to separate the variables included in the list using commas. You can see what this function does by simply calling it,
e.g. &lt;code&gt;inits()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We instruct &lt;tt&gt;R&lt;/tt&gt; and &lt;tt&gt;BUGS&lt;/tt&gt; to run this model for 2 chains — notice we hard-code this in the call to the function &lt;code&gt;bugs&lt;/code&gt;. You can always do this, although it is &lt;em&gt;not&lt;/em&gt; best practice (and you are probably better off by creating suitable variables and then referring to them as inputs to functions). We select a burn-in of 9500 iterations and then do a further 10000 iterations, which we thin by 20. This means we run the model for a total of &lt;span class=&#34;math inline&#34;&gt;\(2\times (9500+10000)=39000\)&lt;/span&gt; iterations and then because we throw away the first &lt;span class=&#34;math inline&#34;&gt;\(2\times 9500\)&lt;/span&gt; and we only store 1 in 20 of the remaining, the final analysis is based on &lt;span class=&#34;math inline&#34;&gt;\(1000\)&lt;/span&gt; iterations.&lt;/p&gt;
&lt;p&gt;Once &lt;tt&gt;BUGS&lt;/tt&gt; has finished, we regain control of the &lt;tt&gt;R&lt;/tt&gt; session and we can print the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Displays the summary statistics
print(es,digits=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;EvSynth.txt&amp;quot;, 
Current: 2 chains, each with 10000 iterations (first 9500 discarded), n.thin = 20
Cumulative: n.sims = 1000 iterations saved
          mean   sd  2.5%   25%   50%   75%  97.5% Rhat n.eff
p1        0.02 0.01  0.00  0.01  0.02  0.03   0.06 1.01   290
p2        0.00 0.01  0.00  0.00  0.00  0.01   0.01 1.01   240
rho       0.22 0.18  0.11  0.17  0.21  0.24   0.39 1.00  1000
l         8.15 1.41  5.74  7.22  8.01  8.97  11.18 1.00   550
c.inf    16.90 2.33 12.63 15.18 16.90 18.45  21.66 1.00   520
alpha[1] -2.68 0.17 -3.06 -2.79 -2.67 -2.57  -2.39 1.00  1000
alpha[2] -2.29 0.16 -2.63 -2.40 -2.29 -2.19  -1.98 1.00   590
alpha[3] -2.65 0.32 -3.29 -2.86 -2.65 -2.44  -2.07 1.00   560
alpha[4] -2.63 0.23 -3.09 -2.79 -2.63 -2.47  -2.21 1.00  1000
alpha[5] -3.57 0.38 -4.34 -3.82 -3.55 -3.29  -2.91 1.01   290
alpha[6] -2.62 0.18 -2.96 -2.73 -2.62 -2.50  -2.28 1.00   430
delta[1] -1.41 0.30 -1.95 -1.63 -1.43 -1.22  -0.78 1.00  1000
delta[2] -1.69 0.33 -2.40 -1.89 -1.66 -1.47  -1.13 1.00   710
delta[3] -1.50 0.43 -2.36 -1.75 -1.52 -1.25  -0.54 1.01   540
delta[4] -1.71 0.41 -2.68 -1.93 -1.66 -1.44  -1.03 1.02   140
delta[5] -1.40 0.48 -2.22 -1.70 -1.45 -1.15  -0.37 1.00  1000
delta[6] -1.89 0.43 -2.94 -2.13 -1.81 -1.58  -1.27 1.01   260
gamma[1] -3.12 0.87 -4.89 -3.70 -3.05 -2.52  -1.63 1.01   290
gamma[2] -4.59 0.62 -5.95 -4.94 -4.53 -4.17  -3.53 1.00   610
gamma[3] -4.98 0.87 -7.11 -5.46 -4.87 -4.37  -3.63 1.01   260
gamma[4] -3.23 0.43 -4.14 -3.51 -3.20 -2.95  -2.44 1.00  1000
gamma[5] -5.24 0.57 -6.60 -5.56 -5.19 -4.85  -4.28 1.00  1000
gamma[6] -4.00 0.43 -4.93 -4.26 -3.97 -3.71  -3.19 1.01   240
gamma[7] -4.77 0.82 -6.55 -5.25 -4.70 -4.19  -3.42 1.00  1000
gamma[8] -1.50 0.55 -2.66 -1.84 -1.46 -1.14  -0.51 1.01   330
gamma[9] -4.68 0.81 -6.43 -5.15 -4.58 -4.13  -3.40 1.00   330
deviance 93.98 5.87 84.56 89.65 93.39 97.90 107.00 1.00  1000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 16.84 and DIC = 110.80
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All seems reasonable — the value for &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; is below the 1.1 threshold for all the monitored nodes and the effective sample size is reasonably large (and close to the nominal value &lt;code&gt;n.sims&lt;/code&gt;=1000). You can play around with reducing the level of thinning (to increase the sample size) or increasing the total number of iterations to see how the results are affected, but in general terms, the model seems to have
reached convergence.&lt;/p&gt;
&lt;p&gt;One graphical way of confirming this is to create “traceplots” of the simulations, which you can do using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convergence check through traceplots (example for node p1)
plot(es$sims.list$p1[1:500],t=&amp;quot;l&amp;quot;,col=&amp;quot;blue&amp;quot;,ylab=&amp;quot;p1&amp;quot;)
points(es$sims.list$p1[501:1000],t=&amp;quot;l&amp;quot;,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/05_ald/solutions_files/figure-html/bugs5-1.png&#34; width=&#34;460.8&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we first plot the first half of the simulations (in this case for the node &lt;code&gt;p1&lt;/code&gt;). &lt;tt&gt;BUGS&lt;/tt&gt; stores the simulations for all the different chains by stacking one chain after the other and thus we do this by accessing the first 500 values of the simulations for &lt;code&gt;p1&lt;/code&gt;, which are stored inside the &lt;tt&gt;BUGS&lt;/tt&gt; object as &lt;code&gt;es$sims.list$p1&lt;/code&gt;. The &lt;tt&gt;R&lt;/tt&gt; notation &lt;code&gt;[1:500]&lt;/code&gt; instructs &lt;tt&gt;R&lt;/tt&gt; to access the positions 1 to 500 of a vector. The options used in the &lt;code&gt;plot&lt;/code&gt; function specify that we want to plot lines (&lt;code&gt;t=l&lt;/code&gt;) in blue (&lt;code&gt;col=blue&lt;/code&gt;) and use a &lt;span class=&#34;math inline&#34;&gt;\(y-\)&lt;/span&gt;axis label “p1”. Then we add to the existing plot using the &lt;tt&gt;R&lt;/tt&gt; built-in function &lt;code&gt;points&lt;/code&gt;. This has a
syntax very similar to &lt;code&gt;plot&lt;/code&gt; but does not overwrite an existing graph. In this case, we superimpose the elements from position 501 to position 1000 of the vector &lt;code&gt;es$sims.list$p1&lt;/code&gt; (the simulations for the second chain). We use the options &lt;code&gt;t=l&lt;/code&gt; and &lt;code&gt;col=red&lt;/code&gt; to instruct
&lt;tt&gt;R&lt;/tt&gt; to plot red lines.&lt;/p&gt;
&lt;p&gt;As is possible to see, the traceplot “looks good” — the two chains are well mixed and on top of each other, confirming convergence (which ties up with the analysis of &lt;code&gt;Rhat&lt;/code&gt; and &lt;code&gt;n.eff&lt;/code&gt; for this particular node). You can try and replicate this analysis for other nodes.&lt;/p&gt;
&lt;p&gt;Finally, we need to perform the full economic analysis. We could programme all the commands we need ourselves, but we can use &lt;code&gt;BCEA&lt;/code&gt; to
do most of the work for us.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Attaches the es object to the R workspace (to use the posteriors for the economic analysis)
attach.bugs(es)

# Runs economic analysis 
# cost of treatment
c &amp;lt;- e &amp;lt;- matrix(NA,n.sims,T)
c[,1] &amp;lt;- (1-p1)*(c.gp) + p1*(c.gp+c.inf)
c[,2] &amp;lt;- (1-p2)*(c.gp+c.ni) + p2*(c.gp+c.ni+c.inf)
e[,1] &amp;lt;- -l*p1
e[,2] &amp;lt;- -l*p2

library(BCEA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;BCEA&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:graphics&amp;#39;:

    contour&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;treats &amp;lt;- c(&amp;quot;status quo&amp;quot;,&amp;quot;prophylaxis with NIs&amp;quot;)
m &amp;lt;- bcea(e,c,ref=2,treats,Kmax=10000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that in this case we “attach” the &lt;tt&gt;BUGS&lt;/tt&gt; object &lt;code&gt;es&lt;/code&gt; to the &lt;tt&gt;R&lt;/tt&gt; workspace. This will allow us to access all the relevant quantites stored inside of it directly by calling their name, i.e. we will not need to use the clunky notation &lt;code&gt;es$sims.list$p1&lt;/code&gt; to access the simulated values for the parameter &lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt;, but we will just need to call &lt;code&gt;p1&lt;/code&gt;. This is handy, but, again, we need to be careful because attaching an object will overwrite any other object with the same name that already exists in the &lt;tt&gt;R&lt;/tt&gt; workspace.&lt;/p&gt;
&lt;p&gt;Once we have done this, we define the suitable economic summaries. We can think of this step as moving from the “Statistical model” to the
“Economic model” box (as in &lt;a href=&#34;https://egon.stats.ucl.ac.uk/static/stat0019/slides/04_Intro_HE/#7&#34;&gt;here&lt;/a&gt;) — cfr. also the slides for &lt;a href=&#34;https://egon.stats.ucl.ac.uk/static/stat0019/slides/07_ALD/&#34;&gt;Lecture 7&lt;/a&gt;. This step is fairly simple – we first create two matrices &lt;code&gt;e&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;, which we will then fill with the simulations for the effectiveness and cost variables for the &lt;span class=&#34;math inline&#34;&gt;\(T=2\)&lt;/span&gt; treatments considered. Notice that we define &lt;code&gt;e&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; as matrices filled with &lt;code&gt;NA&lt;/code&gt; (&lt;tt&gt;R&lt;/tt&gt; notation for “null” or missing values), with &lt;code&gt;n.sims&lt;/code&gt;=1000 rows and &lt;code&gt;T&lt;/code&gt;=2 columns. The notation &lt;code&gt;c[,1]&lt;/code&gt; should be clear by now. With this, we mean to talk all the rows and only the first column of the matrix &lt;code&gt;c&lt;/code&gt;. We fill this with suitable values obtained by combining the probabilities of infection and the relevant
costs.&lt;/p&gt;
&lt;p&gt;At this point, we are ready to load &lt;code&gt;BCEA&lt;/code&gt; and then call the function &lt;code&gt;bcea&lt;/code&gt;, which performs the basic economic analysis for us. Before we do
so, we define for convenience a vector of names, to associate with the interventions we are considering. In this case, the index 1 is associated with the status quo (so the first column of &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;e&lt;/code&gt; contains the simulations for this treatment), while the second is associated with the active intervention.&lt;/p&gt;
&lt;p&gt;Finally, we create an object &lt;code&gt;m&lt;/code&gt; in which we store the output of the call to &lt;code&gt;bcea&lt;/code&gt;. There are some mandatory and some optional inputs to
this function — check &lt;code&gt;help(bcea)&lt;/code&gt; as well as &lt;em&gt;BMHE&lt;/em&gt; and &lt;em&gt;Bayesian Cost-Effectiveness Analysis with the R package BCEA&lt;/em&gt; to see more
details. At the very minimum, &lt;code&gt;bcea&lt;/code&gt; expects that you pass as arguments the matrices including the simulations for effects and costs, in this
order. Thus, unless you specify a name for the arguments, &lt;tt&gt;R&lt;/tt&gt; will assume you are following the default. For example, the function &lt;code&gt;bcea&lt;/code&gt; uses the following arguments in exactly this order.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;e&lt;/code&gt; = a numeric matrix with simulations for the effectiveness
variable, for all the treatments considered (must have more than 1
column);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;c&lt;/code&gt; = a numeric matrix with simulations for the cost variable, for
all the treatments considered (must have more than 1 column);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;ref&lt;/code&gt; = a number to identify the treatment to be considered as the
“reference”, i.e. the one we are comparing against the other(s).
This is an optional argument and unless specified differently, the
default is &lt;code&gt;1&lt;/code&gt;, in which case &lt;tt&gt;BCEA&lt;/tt&gt; will
assume that the intervention of interest is in the first column of
&lt;code&gt;e&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. In this case, however, we specify &lt;code&gt;ref=2&lt;/code&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;interventions&lt;/code&gt; = a vector of strings of length equal to the number
of columns in &lt;code&gt;e&lt;/code&gt;, giving names to the interventions. This is also
an optional argument and unless otherwise specified,
&lt;tt&gt;BCEA&lt;/tt&gt; will create labels in the form
&lt;code&gt;Intervention1&lt;/code&gt;, &lt;code&gt;Intervention2&lt;/code&gt;, …;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Kmax&lt;/code&gt; = a number specifying the maximum value for the willingness
to pay to be considered. The default value is &lt;code&gt;k=50000&lt;/code&gt;. The
willingness to pay is then approximated on a discrete grid in the
interval &lt;code&gt;[0,Kmax]&lt;/code&gt;. The grid is equal to the argument &lt;code&gt;wtp&lt;/code&gt; — see
below — if that parameter is provided, or simply composed of 501
elements if &lt;code&gt;wtp=NULL&lt;/code&gt; (the default);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;wtp&lt;/code&gt; = an optional vector containing specific values for the
willingness to pay grid. If not specified then
&lt;tt&gt;BCEA&lt;/tt&gt; will construct a grid of 501 values
from 0 to &lt;code&gt;Kmax&lt;/code&gt; (see point above). This option is useful when
performing intensive computations (e.g. for the EVPPI);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;plot&lt;/code&gt; = a logical value (i.e. &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;), indicating
whether the function should produce the summary plot or not. The
default is set to &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 9. Markov models — SOLUTIONS</title>
      <link>/practical/09_mm/solutions/</link>
      <pubDate>Wed, 22 Jun 2022 15:15:00 +0000</pubDate>
      <guid>/practical/09_mm/solutions/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;div id=&#34;individual-level-data-on-event-history-and-markov-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Individual level data on event history and Markov models&lt;/h2&gt;
&lt;p&gt;In this case, we consider individual level data, e.g. derived from a randomised study, in which patients have been:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Randomised to either standard of care (&lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt;) or an innovative &lt;a href=&#34;https://en.wikipedia.org/wiki/Cancer_immunotherapy&#34;&gt;immuno-oncologic&lt;/a&gt; drug;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Followed up for a period of time during which their “event history” has been recorded. In particular, we know whether or not the individuals have “&lt;em&gt;progressed&lt;/em&gt;” to a more serious stage of the cancer (and in that case, the time since enrollement at which this has been confirmed), as well as whether or not the individual has “&lt;em&gt;died&lt;/em&gt;” (again with the exact time of death being recorded).&lt;/li&gt;
&lt;/ol&gt;
As shown in class, the data look like this.
&lt;table class=&#34; lightable-classic table&#34; style=&#34;font-family: &amp;quot;Arial Narrow&amp;quot;, &amp;quot;Source Sans Pro&amp;quot;, sans-serif; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Patient
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Treatment
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Progression?
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Death?
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Progression time
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Death time
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
31.99
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
32.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
30.55
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
30.60
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.46
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.27
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.57
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;border-bottom: 2px solid;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-bottom: 2px solid;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-bottom: 2px solid;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-bottom: 2px solid;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-bottom: 2px solid;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-bottom: 2px solid;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\ldots\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The time is recorded in months; from the modelling point of view, this is not a problem; however, for the sake of running the Markov model for a “lifetime horizon”, it may be more efficient to convert the times in, say, years — this means that the values are rescaled (by 12, in this case) and so the overall number of cycles becomes smaller (so, instead of running the Markov model for 120 months, which implies a relatively large computational burden), we can do it for 10 years.&lt;/p&gt;
&lt;p&gt;As shown in class, this dataset has the advantage of using the nature of the data — for each individual we know whether and when they experience the two events of interest (progression and death). This is not possible when we use digitised data on PFS and OS separately. However to run the analysis, we need to convert the data to a suitable format (often referred to as “multistate”). For example, we can use the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; and re-arrange the data in a very efficient way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# &amp;#39;tidyverse&amp;#39; code to re-arrange the data in &amp;quot;multistate&amp;quot; format
msmdata=
  # Transition Pre to Post
  data %&amp;gt;% mutate(                                                        # use the original data and start making changes to them
  id=patid,                                                               # patient ID
  from=1,                                                                 # starting state (1=&amp;quot;Pre-progression&amp;quot;)
  to=2,                                                                   # arriving state (2=&amp;quot;Progressed&amp;quot;)
  trans=1,                                                                # transition ID (1=&amp;quot;Pre-progression -&amp;gt; Progressed&amp;quot;)
  Tstart=0,                                                               # entry time
  Tstop=prog_t,                                                           # exit time (time at which the event happens)
  time=Tstop-Tstart,                                                      # observed time
  status=case_when(                                                       # event indicator
    prog==1~1,                                                            #   if progressed then 1
    TRUE~0                                                                #   otherwise 0 (censored for progression)
  ),
  treat=treat                                                             # treatment arm
) %&amp;gt;% select(id,from,to,trans,Tstart,Tstop,time,status,treat) %&amp;gt;%         # selects only the relevant columns (for simplicity)
  bind_rows(                                                              # stack these new rows below those selected above
  # Transition Pre to Death
    data %&amp;gt;% mutate(                                                      # use the original data and start making changes to them
      id=patid,                                                           # patient ID
      from=1,                                                             # starting state (1=&amp;quot;Pre-progression&amp;quot;)
      to=3,                                                               # arriving state (3=&amp;quot;Death&amp;quot;)
      trans=2,                                                            # transition ID (2=&amp;quot;Pre-progression -&amp;gt; Death&amp;quot;)
      Tstart=0,                                                           # entry time
      Tstop=death_t,                                                      # exit time (time at which the event happens)
      time=Tstop-Tstart,                                                  # observed time
      status=case_when(                                                   # event indicator
        (death==1 &amp;amp; prog_t==death_t)~1,                                   #   if death then 1
        TRUE~0                                                            #   otherwise 0 (censored for death)
      ),
      treat=treat                                                         # treatment arm
    ) %&amp;gt;% select(id,from,to,trans,Tstart,Tstop,time,status,treat)         # selects only the relevant columns (for simplicity)
  ) %&amp;gt;% 
  bind_rows(                                                              # stack these new rows below those selected above
  # Transition Post to Death
    data %&amp;gt;% filter(prog==1) %&amp;gt;% mutate(                                  # use the original data, but **filter only those who have progressed**
      id=patid,                                                           # patient ID
      from=2,                                                             # starting state (2=&amp;quot;Progressed&amp;quot;)
      to=3,                                                               # arriving state (3=&amp;quot;Death&amp;quot;)
      trans=3,                                                            # transition ID (3=&amp;quot;Progressed -&amp;gt; Death&amp;quot;)
      Tstart=prog_t,                                                      # entry time. NB: this time is the time of progression!
      Tstop=death_t,                                                      # exit time (time at which the event happens). NB: this time it&amp;#39;s death!
      time=Tstop-Tstart,                                                  # observed time
      status=case_when(                                                   # event indicator
        death==1~1,                                                       #   if death then 1
        TRUE~0                                                            #   otherwise 0 (censored for death **after progression**)
      ),
      treat=treat                                                         # treatment arm
    ) %&amp;gt;% select(id,from,to,trans,Tstart,Tstop,time,status,treat)         # selects only the relevant columns (for simplicity)
  ) %&amp;gt;% arrange(id,trans)

# Visualise the data
msmdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,868 × 9
##       id  from    to trans Tstart Tstop     time status treat
##    &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1     1     1     2     1    0   32.0  32.0          1     1
##  2     1     1     3     2    0   32    32            0     1
##  3     1     2     3     3   32.0 32     0.00920      0     1
##  4     2     1     2     1    0   30.6  30.6          1     1
##  5     2     1     3     2    0   30.6  30.6          0     1
##  6     2     2     3     3   30.6 30.6   0.0476       0     1
##  7     3     1     2     1    0   27.9  27.9          1     1
##  8     3     1     3     2    0   28    28            0     1
##  9     3     2     3     3   27.9 28     0.0944       0     1
## 10     4     1     2     1    0    2.88  2.88         1     0
## # … with 1,858 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now run three separate survival analyses for each of the three transitions, for instance using &lt;code&gt;survHE&lt;/code&gt; (but you don’t have to do this — in case you need it, notice that &lt;code&gt;survHE&lt;/code&gt; is installed in the &lt;a href=&#34;../../tips/#virtual-machine&#34;&gt;Binder remote server&lt;/a&gt;). This step is &lt;strong&gt;much&lt;/strong&gt; more complicated than shown here. For example, we would need to test several parametric models (as discussed in &lt;a href=&#34;../../slides/06_Survival&#34;&gt;Lecture 6&lt;/a&gt;). Then we would need to validate the different models — on the basis of their fit to the observed data, but more importantly in relation to the &lt;strong&gt;extrapolation&lt;/strong&gt; over times that have not been observed.&lt;/p&gt;
&lt;p&gt;In this case, we can simplify things and assume that, for all three models (PFS, OS and OS after progression, which we indicate respectively as &lt;code&gt;m_12&lt;/code&gt;, &lt;code&gt;m_13&lt;/code&gt; and &lt;code&gt;m_23&lt;/code&gt; — the reason for this terminology will be obvious later), we select a Gompertz distribution as the best one. In &lt;code&gt;survHE&lt;/code&gt;, the models would be fitted using the following commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loads survHE
library(survHE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: flexsurv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;survHE&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked _by_ &amp;#39;.GlobalEnv&amp;#39;:
## 
##     data, msmdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Runs survival models on the specific subsets to obtain estimate of the various transition probabilities
m_12=fit.models(Surv(time,status)~as.factor(treat),              # model &amp;#39;formula&amp;#39;: defines the time and censoring indicator and the covariates
                data=msmdata %&amp;gt;% filter(trans==1),               # subsets the msmdata by filtering transition number 1 (pre-progression-&amp;gt;progressed)
                distr=&amp;quot;gom&amp;quot;,                                     # selects the Gompertz model
                method=&amp;quot;hmc&amp;quot;,                                    # instructs R to use HMC/Bayesian modelling
                priors=list(gom=list(a_alpha=1.5,b_alpha=1.5)))  # specifies the informative prior

m_13=fit.models(Surv(time,status)~as.factor(treat),              # model &amp;#39;formula&amp;#39;: defines the time and censoring indicator and the covariates
                data=msmdata %&amp;gt;% filter(trans==2),               # subsets the msmdata by filtering transition number 2 (pre-progression-&amp;gt;death)
                distr=&amp;quot;gom&amp;quot;,                                     # selects the Gompertz model
                method=&amp;quot;hmc&amp;quot;,                                    # instructs R to use HMC/Bayesian modelling
                priors=list(gom=list(a_alpha=1.5,b_alpha=1.5)))  # specifies the informative prior

m_23=fit.models(Surv(time,status)~as.factor(treat),              # model &amp;#39;formula&amp;#39;: defines the time and censoring indicator and the covariates
                data=msmdata %&amp;gt;% filter(trans==3),               # subsets the msmdata by filtering transition number 3 (progressed-&amp;gt;death)
                distr=&amp;quot;gom&amp;quot;,                                     # selects the Gompertz model
                method=&amp;quot;hmc&amp;quot;,                                    # instructs R to use HMC/Bayesian modelling
                priors=list(gom=list(a_alpha=1.5,b_alpha=1.5)))  # specifies the informative prior```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we fit these models, we obtain the estimates for the model parameters (which in the case of the Gompertz are the shape &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and the rate &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;). These can be used to reconstruct the full survival curves for an arbitrary time interval — for example in the case of the Gompertz model, for any given time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, the survival curve is
&lt;span class=&#34;math display&#34;&gt;\[S(t) = 1-\exp\left(-\frac{\mu}{\alpha} \exp(\alpha t)-1\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;survHE&lt;/code&gt; the function &lt;code&gt;make.surv&lt;/code&gt; can be used to construct &lt;code&gt;nsim&lt;/code&gt; simulations of the survival curves for any specific interval of time &lt;code&gt;t=...&lt;/code&gt;. Once these are obtained, we can use them to compute the approximated transition probabilities using the formula
&lt;span class=&#34;math display&#34;&gt;\[\lambda_{s&amp;#39;sj}\approx 1-\frac{S_{t+k}}{S_t}\]&lt;/span&gt;
for a given pair of times &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t+k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The script provided contains a few functions that code up this process. We can load up the functions contained in the script &lt;code&gt;survHE_utils.R&lt;/code&gt;; these essentially use &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; to manipulate the output of the models using the following steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute the survival curves using the &lt;code&gt;make.surv&lt;/code&gt; function in &lt;code&gt;survHE&lt;/code&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Use the approximation formula to translate these into the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{12}\)&lt;/span&gt; (transition from Pre-progression to Progressed), &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{13}\)&lt;/span&gt; (transition from Pre-progressed to Death) and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{23}\)&lt;/span&gt; (transition from Progressed to Death).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Run the specialised function &lt;code&gt;three_state_mm&lt;/code&gt; that:&lt;br /&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;first completes the transition matrix by retrieving the remaining transition probabilities (&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{11}=1-\lambda_{12}-\lambda_{13}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{22}=1-\lambda_{23}\)&lt;/span&gt;;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;then move people around according to the Markov matrix algebra.&lt;br /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Run the specialised function &lt;code&gt;markov_trace&lt;/code&gt; to manipulate the resulting state occupancy tibble and then use &lt;code&gt;ggplot2&lt;/code&gt; to plot the Markov trace.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sources the specialised functions
source(&amp;quot;survHE_utils.R&amp;quot;)

# Then run the Markov model using the specialised function &amp;#39;three_state_mm&amp;#39;
mm=three_state_mm(
  m_12,m_13,m_23,         # these are the three objects containing the parameters estimates
  t=seq(0,130),           # specifies that the Markov model needs to be run for discrete times from 0 to 130 (months)
  nsim=1,                 # only uses 1 simulation from the distribution of the model parameters (the mean)
  start=c(1000,0,0)       # initial population: 1000 in pre-progression, 0 in progressed and 0 in death
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualise the resulting object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $m
## # A tibble: 260 × 11
##    treat     t `Pre-progressed` Progressed Death sim_idx lambda_11 lambda_12
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;            &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1     1     1            1000        0     0          1     0.993   0.00506
##  2     1     2             993.       5.06  1.67       1     0.993   0.00527
##  3     1     3             986.      10.2   3.45       1     0.993   0.00550
##  4     1     4             979.      15.4   5.33       1     0.993   0.00574
##  5     1     5             972.      20.7   7.32       1     0.992   0.00599
##  6     1     6             964.      26.1   9.43       1     0.992   0.00625
##  7     1     7             957.      31.6  11.7        1     0.992   0.00652
##  8     1     8             949.      37.2  14.0        1     0.991   0.00680
##  9     1     9             941.      42.8  16.5        1     0.991   0.00710
## 10     1    10             932.      48.6  19.1        1     0.991   0.00740
## # … with 250 more rows, and 3 more variables: lambda_13 &amp;lt;dbl&amp;gt;, lambda_22 &amp;lt;dbl&amp;gt;,
## #   lambda_23 &amp;lt;dbl&amp;gt;
## 
## $running_time
## Time difference of 0.121031 secs
## 
## $base_case
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This includes three elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The tibble &lt;code&gt;m&lt;/code&gt;; this includes the state occupancy as well as the value of the relevant transition probabilities for each time point in the “virtual follow up”.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The running time; this is a function of the number of simulations used — in this case, we’re only using &lt;code&gt;nsim&lt;/code&gt;=1 and so the computation is very fast. Note that in general, running this &lt;code&gt;R&lt;/code&gt; is more efficient than any implementation in spreadsheets.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The user can instruct &lt;code&gt;R&lt;/code&gt; to also compute the Markov model for the “base-case” scenario, which is essentially the same as the case with &lt;code&gt;nsim&lt;/code&gt;=1. So in this case, the element &lt;code&gt;base_case&lt;/code&gt; is not computed and it is set to &lt;code&gt;NULL&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, we can visualise the results using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;markov_trace(mm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/09_mm/solutions_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;note-on-using-survhe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Note on using survHE&lt;/h2&gt;
&lt;p&gt;The code above uses the current &lt;code&gt;CRAN&lt;/code&gt; version of &lt;code&gt;survHE&lt;/code&gt;, which you can install by using the &lt;code&gt;R&lt;/code&gt; command &lt;code&gt;install.packages(&#39;survHE&#39;)&lt;/code&gt; or from the GitHub repository &lt;code&gt;remotes::install_github(&#39;giabaio/survHE&#39;)&lt;/code&gt; – assuming you have installed &lt;code&gt;remotes&lt;/code&gt; (check &lt;a href=&#34;../../../tips/computer-specification&#34;&gt;here&lt;/a&gt; for more details on &lt;code&gt;remotes&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;survHE&lt;/code&gt; is a bit of a complicated package – it’s not like most of its functions are difficult, it’s just that it is designed to “wrap up” complex packages doing the survival modelling. In particular, the two Bayesian versions are performed using very structured and heavy &lt;code&gt;R&lt;/code&gt; packages (&lt;code&gt;rstan&lt;/code&gt; and &lt;code&gt;INLA&lt;/code&gt;), which means that its installation can be long. If you’re on the &lt;a href=&#34;../../../tips/computer-specification#virtual-machine&#34;&gt;Binder VM&lt;/a&gt;, the installation of the whole thing may break it. In that case, you can resort to doing a “cheat” and installing the frequentist module only. You can do this by using the &lt;code&gt;R&lt;/code&gt; command &lt;code&gt;remotes::install_github(&#34;giabaio/survHE&#34;,ref=&#34;devel&#34;)&lt;/code&gt;. Note that in this case we use the extra option &lt;code&gt;ref=&#34;devel&#34;&lt;/code&gt;, which instructs &lt;code&gt;R&lt;/code&gt; to install the version contained in the GitHub branch named &lt;code&gt;devel&lt;/code&gt;, which contains the code to run the frequentist version of the models, only.&lt;/p&gt;
&lt;p&gt;In this case, you will need to slightly modify the code above, to remove the reference to the &lt;code&gt;&#34;hmc&#34;&lt;/code&gt; method. For instance, you need to modify the call to &lt;code&gt;fit.models&lt;/code&gt; to the following&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_12=fit.models(Surv(time,status)~as.factor(treat),              # model &amp;#39;formula&amp;#39;: defines the time and censoring indicator and the covariates
                data=msmdata %&amp;gt;% filter(trans==1),               # subsets the msmdata by filtering transition number 1 (pre-progression-&amp;gt;progressed)
                distr=&amp;quot;gom&amp;quot;                                      # selects the Gompertz model
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(notice the removal to the &lt;code&gt;prior&lt;/code&gt; argument too: this is a frequentist model, so there’s no space for priors…).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 6. Network meta-analysis — SOLUTIONS</title>
      <link>/practical/06_nma/solutions/</link>
      <pubDate>Tue, 21 Jun 2022 15:30:00 +0000</pubDate>
      <guid>/practical/06_nma/solutions/</guid>
      <description>


&lt;div id=&#34;fixed-effects-nma&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed effects NMA&lt;/h2&gt;
&lt;p&gt;The data from the smoking cessation studies discussed in the lecture are
included in the file &lt;a href=&#34;smoke.Rdata&#34;&gt;&lt;code&gt;smoke.Rdata&lt;/code&gt;&lt;/a&gt; and can be loaded into the
&lt;tt&gt;R&lt;/tt&gt; workspace using the &lt;code&gt;load&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loads the data (assuming they are in the current folder)
load(&amp;quot;smoke.Rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use other built-in &lt;tt&gt;R&lt;/tt&gt; commands to
inspect the object we have just loaded into our workspace to figure out
what is stored in it, for example as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# List the objects present in the workspace
ls()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;current_repo&amp;quot; &amp;quot;smoke.list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# What type of object is &amp;#39;smoke.list&amp;#39;?
class(smoke.list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# What&amp;#39;s in the data list?
names(smoke.list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;r&amp;quot;  &amp;quot;n&amp;quot;  &amp;quot;t&amp;quot;  &amp;quot;na&amp;quot; &amp;quot;NS&amp;quot; &amp;quot;NT&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command &lt;code&gt;ls()&lt;/code&gt; simply lists all the objects currently present in the
workspace. In this case, we only have an object &lt;code&gt;smoke.list&lt;/code&gt;, which has
been created when using the &lt;code&gt;load&lt;/code&gt; command above. We can check its
“class” (in this case, unsurprisingly, a list) and show the names of the
elements contained in it, using the command &lt;code&gt;names&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can ask &lt;tt&gt;R&lt;/tt&gt; to tell us more about these
variables; for instance, we can inspect each variable’s “class” (e.g.its nature) using the following helpful
&lt;tt&gt;R&lt;/tt&gt; command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(smoke.list,class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $r
## [1] &amp;quot;matrix&amp;quot; &amp;quot;array&amp;quot; 
## 
## $n
## [1] &amp;quot;matrix&amp;quot; &amp;quot;array&amp;quot; 
## 
## $t
## [1] &amp;quot;matrix&amp;quot; &amp;quot;array&amp;quot; 
## 
## $na
## [1] &amp;quot;integer&amp;quot;
## 
## $NS
## [1] &amp;quot;integer&amp;quot;
## 
## $NT
## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;tt&gt;R&lt;/tt&gt; function &lt;code&gt;lapply&lt;/code&gt; can be used to apply
a function to the elements of a list (such as &lt;code&gt;smoke.list&lt;/code&gt;). In this
case, we want &lt;tt&gt;R&lt;/tt&gt; to tell us what class each of
the elements of &lt;code&gt;smoke.list&lt;/code&gt; belongs to, which is what the command
returns — for instance, the object &lt;code&gt;r&lt;/code&gt; inside the object &lt;code&gt;smoke.list&lt;/code&gt; is
a matrix, while the object &lt;code&gt;NS&lt;/code&gt; is an integer. We can also visualise
each, e.g. by using the following commands&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Shows the first few elements of the object r included inside the object smoke.list
head(smoke.list$r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]   79   77   NA   NA
## [2,]   18   21   NA   NA
## [3,]    8   19   NA   NA
## [4,]   75   NA  363   NA
## [5,]    2   NA    9   NA
## [6,]   58   NA  237   NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Shows the first few elements of the object n included inside the object smoke.list
head(smoke.list$n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]  702  694   NA   NA
## [2,]  671  535   NA   NA
## [3,]  116  149   NA   NA
## [4,]  731   NA  714   NA
## [5,]  106   NA  205   NA
## [6,]  549   NA 1561   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More in details, the elements of &lt;code&gt;smoke.list&lt;/code&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;NS&lt;/code&gt;: the total number of studies included in our analysis (24);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;NT&lt;/code&gt;: the total number of interventions considered (4);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;na&lt;/code&gt;: a vector containing the number of arms included in each of the
studies;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;r&lt;/code&gt;: a matrix with &lt;code&gt;NS&lt;/code&gt; rows and &lt;code&gt;NT&lt;/code&gt; columns, containing the number
of subjects that in each study and under each treatment arms have
been observed to quit smoking;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;n&lt;/code&gt;: a matrix with &lt;code&gt;NS&lt;/code&gt; rows and &lt;code&gt;NT&lt;/code&gt; columns, containing the total
number of subjects observed in each study and under each treatment
arms;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;t&lt;/code&gt;: a matrix with &lt;code&gt;NS&lt;/code&gt; rows and 3 columns, identifying the label
associated with the treatments included in each of the studies.
Notice that there are 3 columns because all studies have at most 3
treatment involved (i.e. all studies compare either 2 or 3
treatments — cfr. the lecture slides). The treatments are labelled
as 1 = No intervention; 2 = Self-help; 3 = Individual counselling; 4
= Group counselling.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that some of the data will be associated with &lt;code&gt;NA&lt;/code&gt; (i.e. “Not
Available” or “missing”). In this case, these are not really &lt;a href=&#34;https://egon.stats.ucl.ac.uk/static/stat0019/slides/08_Missing/&#34;&gt;“missing
data”&lt;/a&gt;, but rather indicate that that particular column is not relevant.
For example, in study 1 there are only two arms (you can confirm this by
asking &lt;tt&gt;R&lt;/tt&gt; to tell what &lt;code&gt;na[1]&lt;/code&gt; is). The matrix
&lt;code&gt;r&lt;/code&gt; has values 79 and 77 in the first two columns and then &lt;code&gt;NA&lt;/code&gt; in the
third and fourth column — this is because study 1 only had data on arm 1
and arm 2. Incidentally, you can check what these arms where by looking
at the value of the first row in the matrix &lt;code&gt;t&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smoke.list$t[1,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## t1 t2 t3 
##  1  2 NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which tells you that the first arm was treatment 1, the second arm was
treatment 2 and the third arm was nothing.&lt;/p&gt;
&lt;p&gt;You can do a similar check on row (i.e. study) 4, using the following
commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many arms were used in study 4?
smoke.list$na[4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# What treatment arms were they?
smoke.list$t[4,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## t1 t2 t3 
##  1  3 NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data on the number of subjects quitting smoke for study 4
smoke.list$r[4,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  75  NA 363  NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data on the total sample size in study 4
smoke.list$n[4,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 731  NA 714  NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, study 4 considered 2 arms (comparing interventions 1 and 3, i.e. No intervention vs Individual counselling) and the number of quitters out of the total number of subjects studied were 75/731 and 363/714, respectively&lt;/p&gt;
&lt;p&gt;Now we are ready to prepare to run the model. First, we consider the
“fixed-effect” specification, whose model code is included in the file
&lt;a href=&#34;smokefix_model.txt&#34;&gt;&lt;code&gt;smokefix_model.txt&lt;/code&gt;&lt;/a&gt;. One of the complications of this model code is in
the use of the &lt;a href=&#34;https://egon.stats.ucl.ac.uk/static/stat0019/slides/09_NMA/#nested-index&#34;&gt;“nested index”&lt;/a&gt; notation. For example, the code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(s in 1:NS) { # loop over studies
      for (a in 1:na[s])  { # loop over arms
         r[s,t[s,a]] ~ dbin(p[s,t[s,a]], n[s,t[s,a]])
         ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;can be interpreted in this way. Let us consider &lt;code&gt;s&lt;/code&gt; = 1, i.e. the first
study in our dataset. This consists of &lt;code&gt;na[1]&lt;/code&gt; = 2 arms, which means we
will have two observed data points in terms of number of subjects who
quit smoking. This also means that the index &lt;code&gt;a&lt;/code&gt; will run from 1 to
&lt;code&gt;na[s]&lt;/code&gt; = &lt;code&gt;na[1]&lt;/code&gt; = 2. Moreover, &lt;code&gt;t[s,a]&lt;/code&gt; is in this case &lt;code&gt;t[1,1]&lt;/code&gt; = 1
and &lt;code&gt;t[s,a]&lt;/code&gt; = &lt;code&gt;t[1,2]&lt;/code&gt; = 2, which in turns means that the above code
effectively means that we are using the following modelling assumptions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r[1,1] ~ dbin(p[1,1], n[1,1])
r[1,2] ~ dbin(p[1,2], n[1,2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The use of the nested index notation is a clever shortcut for the full
specification of all the cases (for all the studies and for all arms
specified in each study) and it is particularly helpful for
“non-rectangular” data (i.e. when not all the rows have data on the same
number of columns).&lt;/p&gt;
&lt;p&gt;From a more substantial point of view, we are modelling the logit of the
study- and arm-specific probability of quitting smoking using a linear
term made by an overall study-specific mean (&lt;code&gt;mu[s]&lt;/code&gt;) and an incremental
term (&lt;code&gt;delta[s,t[s,a]]&lt;/code&gt;), which accounts for the “treatment effect”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit(p[s,t[s,a]]) &amp;lt;- mu[s] + delta[s,t[s,a]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we are using the nested index notation in exactly the same way as
above. In addition, what characterises this model as a “fixed-effect”
specification is the distributional assumption on the incremental
effects &lt;code&gt;delta&lt;/code&gt;. These are modelled as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## log odds ratio compared to treatment in arm 1 of study s
delta[s,t[s,1]] &amp;lt;- 0
for (a in 2:na[s])  {
   delta[s,t[s,a]] &amp;lt;- d[t[s,a]] - d[t[s,1]]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Firstly, we set the “baseline” intervention for each study &lt;code&gt;s&lt;/code&gt;. In
particular, we arbitrarily assume that the first intervention
(associated with the index identified by &lt;code&gt;t[s,1]&lt;/code&gt;) is the reference one
for study &lt;code&gt;s&lt;/code&gt; and as such, we (again, arbitrarily) assign it an “extra”
effect of 0. Obviously, this means that for the reference treatment, the
study- and arm-specific probability of quitting smoking is simply
&lt;code&gt;mu[s]&lt;/code&gt;, because in that case we would be adding to the linear predictor
a value for the corresponding &lt;code&gt;delta&lt;/code&gt; equal to 0. Any other intervention
(from 2 to &lt;code&gt;na[s]&lt;/code&gt;) is modelled through the difference between its
specific value &lt;code&gt;d[t[s,a]]&lt;/code&gt; and the value associated with the
study-specific reference intervention, &lt;code&gt;d[t[s,1]]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let us make a specific example: consider for example study &lt;code&gt;s&lt;/code&gt; = 21. The
details are as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many arms?
smoke.list$na[21]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Which arms?
smoke.list$t[21,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## t1 t2 t3 
##  2  3  4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that this study compares interventions 2, 3 and 4 (Self-help,
Individual and Group counselling) and that, arbitrarily, we assume that
the one in the first column of &lt;code&gt;t[21,]&lt;/code&gt; is the reference — in this case,
that is Self-help. In line with the code above, we then set&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delta[21,2] &amp;lt;- 0
delta[21,3] &amp;lt;- d[21,3] - d[21,2]
delta[21,4] &amp;lt;- d[21,4] - d[21,2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;because &lt;code&gt;t[21,1]&lt;/code&gt; = 2, &lt;code&gt;t[21,2]&lt;/code&gt; = 3 and &lt;code&gt;t[21,3]&lt;/code&gt; = 4 (cfr. with the
code above describing the general definition of the variables
&lt;code&gt;delta[s,t[s,a]]&lt;/code&gt;). Again, the nested index notation allows us to use a
single double loop to model all the possible cases.&lt;/p&gt;
&lt;p&gt;The log ORs &lt;code&gt;d&lt;/code&gt; are then defined as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d[1] &amp;lt;- 0  # log odds ratio compared to treatment 1 (e.g. placebo)
for (i in 2:NT) {
    d[i] ~ dnorm(0, 0.0001)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This again sets one reference category, which in this case is associated
with intervention 1 (No intervention), by setting the corresponding log
OR to 0. Then we assign a vague prior to all the other log ORs (the
interventions labelled from 2 to &lt;code&gt;NT&lt;/code&gt; = 4), using a Normal with mean
equal to 0 and a very small precision.&lt;/p&gt;
&lt;p&gt;The next bit of the model code constructs the ORs for all potential
treatment comparisons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## odds ratios for all treatment comparisons
for (c in 1:(NT-1)) {
  for (k in (c+1):NT)  {
    or[c,k] &amp;lt;- exp(d[c] - d[k])
    or[k,c] &amp;lt;- 1/or[c,k]
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, there is some clever coding to use loops and compactly write down
&lt;em&gt;all&lt;/em&gt; the possible pairwise comparisons. Notice that the variables &lt;code&gt;d&lt;/code&gt;
define the log ORs and thus in order to obtain the OR on the natural
scale, we need to exponentiate the difference between any two values.
The line &lt;code&gt;or[k,c] &amp;lt;- 1/or[c,k]&lt;/code&gt; uses the fact that ORs for the
comparison between two generic interventions &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; are simply the
reciprocal of the ORs for the comparison between &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the model code has some additional definitions for other useful
variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Log odds of quitting successfully under no intervention (from published data)
alpha ~ dnorm(-2.6, 6.925208) # = SD 0.38
## Absolute probability of quitting successfully under each intervention
for (i in 1:NT) {
    logit(pq[i]) &amp;lt;- alpha + d[i]
}

## Life years gained by quitting
L ~ dnorm(15, 0.0625) # SD=4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Firstly, we define a model for the absolute probability of quitting
smoking under each intervention. We do on the logit scale, by defining
the baseline value &lt;span class=&#34;math inline&#34;&gt;\(\alpha \sim \mbox{Normal}(\mu_\alpha,\sigma_\alpha)\)&lt;/span&gt;, to which we add
the incremental effect of each treatment. Notice that because we set
&lt;code&gt;d[1]&lt;/code&gt; = 0, then &lt;code&gt;alpha&lt;/code&gt; is equal to &lt;code&gt;logit(pq[1])&lt;/code&gt;, which is the
absolute “success rate” for No intervention. This is incremented by the
value of the log OR for each active treatment (against the reference,
i.e. No intervention). Notice also that we use an informative prior
distribution to model the parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. We have information
suggesting that a random smoker who is not undergoing any active
treatment has an average chance of quitting smoking of about 7%, ranging
between around 2% to 13.8%. We can map this information into a suitable
prior on the logit scale by setting
&lt;span class=&#34;math display&#34;&gt;\[\mu_\alpha = \mbox{logit}(0.07) = \log\left(\frac{0.07}{1-0.07}\right) = \log\left(\frac{0.07}{0.93}\right) \approx -2.6.\]&lt;/span&gt;
We can also use the fact that
&lt;span class=&#34;math display&#34;&gt;\[\sigma_\alpha \approx \frac{\mbox{logit}(0.138)-\mbox{logit}(0.07)}{1.96} \approx 0.38\]&lt;/span&gt;
— this is reasonable if we assume some sort of symmetry in the interval
estimate, whereby the upper extreme is 1.96 standard deviations away
from the central point, which implies that
&lt;span class=&#34;math display&#34;&gt;\[\mbox{logit}(0.07)+ 1.96\sigma_\alpha \approx \mbox{logit}(0.138).\]&lt;/span&gt;
Of course, we need to include in the
&lt;tt&gt;OpenBUGS&lt;/tt&gt; model the precision,
i.e. &lt;span class=&#34;math inline&#34;&gt;\(1/(0.38)^2=6.925208\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The model for the life years gained by quitting smoking is constructed
in a similar way: our best estimate is a gain of between 7 and 22 extra
years, with an average of 15, which we turn into a Normal distribution
with mean 15 and standard deviation of 4 (i.e. precision of
&lt;span class=&#34;math inline&#34;&gt;\(1/16=0.0625\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The model is then run from &lt;tt&gt;R&lt;/tt&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(R2OpenBUGS)

### Initial values
inits &amp;lt;- list(list(mu=rep(0,24), d=c(NA,0,0,0)),
              list(mu=rep(-1,24), d=c(NA,1,1,1)))

### Pilot run with no burn-in, to illustrate convergence using traceplots
res0 &amp;lt;- bugs(model=&amp;quot;smokefix_model.txt&amp;quot;, data=smoke.list, inits=inits,
            parameters=c(&amp;quot;d&amp;quot;),
            n.chains=2, n.burnin=0, n.iter=10000
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we first define the initial values creating a suitable list made
by two “sub-lists” — this implies we are prepared to run the model using
two parallel chains. We initialise the variables &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; — notice
that because the first element of the vector &lt;code&gt;d&lt;/code&gt; is in fact fixed at 0,
we cannot initialise it. We overcome this issue by creating a vector of
initial values, the first of which is set to &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition, this time we run the model with no burn-in, to explore
convergence in more details than we’ve done so far. The code below uses
the results of the &lt;tt&gt;OpenBUGS&lt;/tt&gt; model as stored
in the &lt;tt&gt;R&lt;/tt&gt; object &lt;code&gt;res0&lt;/code&gt; to produce traceplots
for the only variable monitored (the vector &lt;code&gt;d&lt;/code&gt;). Notice in particular
that we can use the object &lt;code&gt;res$sims.array&lt;/code&gt;, which (as the name
suggests) is an array of dimension &lt;code&gt;Number of iterations stored&lt;/code&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; &lt;code&gt;Number of chains run&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;
&lt;code&gt;Number of parameters monitored&lt;/code&gt;. In this case, the number of parameters
is equal to 4, because there are 3 “active” elements in &lt;code&gt;d&lt;/code&gt; (since
&lt;code&gt;d[1]&lt;/code&gt; is set to 0 and thus is not technically a random variable, in
&lt;tt&gt;OpenBUGS&lt;/tt&gt;), plus the model deviance, which
&lt;tt&gt;OpenBUGS&lt;/tt&gt; computes by default. You can check
this by simply printing the summary statistics for your model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(R2OpenBUGS)
print(res0,digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;smokefix_model.txt&amp;quot;, 
Current: 2 chains, each with 10000 iterations (first 0 discarded)
Cumulative: n.sims = 20000 iterations saved
            mean     sd    2.5%     25%     50%     75%   97.5%  Rhat n.eff
d[2]       0.225  0.126  -0.020   0.140   0.227   0.309   0.473 1.001 20000
d[3]       0.765  0.063   0.650   0.727   0.766   0.805   0.879 1.001 20000
d[4]       0.839  0.178   0.499   0.719   0.839   0.959   1.183 1.001 20000
deviance 494.927 16.902 482.300 489.500 494.100 499.300 510.700 1.031 20000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 27.220 and DIC = 522.100
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The actual traceplot is produce using the built-in functions &lt;code&gt;plot&lt;/code&gt; and
&lt;code&gt;points&lt;/code&gt; as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Splits the graphical output into a 2-by-2 panel (side-by-side graphs)
par(mfrow=c(2,2))
# First graph
plot(res0$sims.array[,1,1],t=&amp;quot;l&amp;quot;,xlab=&amp;quot;Iterations&amp;quot;,ylab=&amp;quot;d[2]&amp;quot;,col=&amp;quot;blue&amp;quot;)
points(res0$sims.array[,2,1],t=&amp;quot;l&amp;quot;,col=&amp;quot;red&amp;quot;)
# Second graph
plot(res0$sims.array[,1,2],t=&amp;quot;l&amp;quot;,xlab=&amp;quot;Iterations&amp;quot;,ylab=&amp;quot;d[3]&amp;quot;,col=&amp;quot;blue&amp;quot;)
points(res0$sims.array[,2,2],t=&amp;quot;l&amp;quot;,col=&amp;quot;red&amp;quot;)
# Third graph
plot(res0$sims.array[,1,3],t=&amp;quot;l&amp;quot;,xlab=&amp;quot;Iterations&amp;quot;,ylab=&amp;quot;d[4]&amp;quot;,col=&amp;quot;blue&amp;quot;)
points(res0$sims.array[,2,3],t=&amp;quot;l&amp;quot;,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/06_nma/solutions_files/figure-html/traceplots1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is possible to see, for all the three important parameters,
convergence does not seem an issue and in fact the two chains seem to
mix up almost immediately, despite being seen to start from rather
different points (cfr. the red and blue lines). Notice that this
strategy is not an absolute requirement! We can monitor all the relevant
parameters and run the model for a large number of iterations in the
first place. But, especially when there are many parameters, this course
of action may be beneficial, because we are not stuck waiting for
&lt;tt&gt;OpenBUGS&lt;/tt&gt; to finish the simulations for a long
time, before we can even assess how the model is working in terms of
convergence.&lt;/p&gt;
&lt;p&gt;At this point, we can monitor all the model parameters (including &lt;code&gt;L&lt;/code&gt;
and &lt;code&gt;pq&lt;/code&gt;) and re-run the model to produce the relevant estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- bugs(model=&amp;quot;smokefix_model.txt&amp;quot;, data=smoke.list, inits=inits,
            parameters=c(&amp;quot;d&amp;quot;,&amp;quot;L&amp;quot;,&amp;quot;pq&amp;quot;),
            n.chains=2, n.burnin=1000, n.iter=5000)

# Show summary statistics
print(res,digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;smokefix_model.txt&amp;quot;, 
Current: 2 chains, each with 5000 iterations (first 1000 discarded)
Cumulative: n.sims = 8000 iterations saved
            mean    sd    2.5%     25%     50%     75%   97.5%  Rhat n.eff
d[2]       0.224 0.126  -0.023   0.139   0.225   0.309   0.470 1.001  8000
d[3]       0.764 0.058   0.649   0.725   0.765   0.804   0.877 1.004   420
d[4]       0.840 0.176   0.505   0.719   0.838   0.958   1.189 1.001  8000
L         14.895 4.009   6.947  12.250  14.940  17.550  22.750 1.001  7300
pq[1]      0.073 0.026   0.034   0.054   0.069   0.087   0.136 1.001  2700
pq[2]      0.090 0.034   0.040   0.065   0.085   0.108   0.171 1.002  2300
pq[3]      0.143 0.048   0.069   0.108   0.137   0.171   0.254 1.001  7500
pq[4]      0.154 0.055   0.070   0.114   0.146   0.186   0.284 1.001  4900
deviance 494.653 7.175 482.400 489.500 494.100 499.200 510.102 1.001  2700

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 26.940 and DIC = 521.600
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, convergence is clearly reached (cfr. &lt;code&gt;Rhat&lt;/code&gt; and &lt;code&gt;n.eff&lt;/code&gt;). We
could proceed with further analyses as well as with building the
cost-effectiveness model, but we defer this to after we’ve run the
random effects model (cfr. lecture slides for evidence of heterogeneity
in individual studies; we can replicate the analysis monitoring the
nodes &lt;code&gt;delta&lt;/code&gt;, which are the study- and treatment-specific log ORs).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects-nma&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random effects NMA&lt;/h2&gt;
&lt;p&gt;The model code is fairly similar to the one discussed above for the
fixed effects NMA. The only difference, really, is in how the study- and
treatment-specific log ORs &lt;code&gt;delta&lt;/code&gt; are modelled. In this case, we
consider a simple specification, characterised by the following code
lines&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delta[s,t[s,a]] ~ dnorm(md[s,t[s,a]],taud[s,t[s,a]])

# random effects means
md[s,t[s,a]] &amp;lt;- d[t[s,a]] - d[t[s,1]]

## random effects 1/variance constrained to be the same for every comparison
taud[s,t[s,a]] &amp;lt;- tau

# model for the standard deviation of the random effects
sd ~ dunif(0, 10)
# rescaling to the precision
tau &amp;lt;- 1/pow(sd, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;— notice that we can include more complexity, for instance by modelling
the precision as dependent on the studies or the treatments, as well as
by considering a more structured model accounting for correlation within
trials with 3 arms or more (but we do not do this here!).&lt;/p&gt;
&lt;p&gt;The model is run again using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res2 &amp;lt;- bugs(model=&amp;quot;smokere_model.txt&amp;quot;, data=smoke.list, inits=inits,
            parameters=c(&amp;quot;d&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;pq&amp;quot;, &amp;quot;L&amp;quot;),
            n.chains=2, n.burnin=1000, n.iter=20000
)
print(res2,digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Bugs model at &amp;quot;smokere_model.txt&amp;quot;, 
Current: 2 chains, each with 20000 iterations (first 1000 discarded)
Cumulative: n.sims = 38000 iterations saved
            mean     sd    2.5%     25%     50%     75%   97.5%  Rhat n.eff
d[2]       0.519  0.386  -0.230   0.267   0.514   0.766   1.305 1.001  6300
d[3]       0.810  0.232   0.374   0.656   0.805   0.956   1.289 1.001  5900
d[4]       1.169  0.455   0.304   0.867   1.156   1.460   2.098 1.001 38000
sd         0.820  0.183   0.533   0.690   0.795   0.923   1.248 1.001 38000
pq[1]      0.073  0.026   0.034   0.054   0.069   0.088   0.136 1.001 38000
pq[2]      0.122  0.059   0.042   0.080   0.111   0.152   0.269 1.001  6500
pq[3]      0.152  0.057   0.065   0.110   0.143   0.184   0.286 1.001  8400
pq[4]      0.208  0.096   0.071   0.138   0.191   0.262   0.437 1.001 38000
L         14.989  3.993   7.109  12.310  15.000  17.670  22.810 1.001 38000
deviance 281.823 10.073 263.900 274.700 281.200 288.200 303.500 1.001 38000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = Dbar-Dhat)
pD = 44.870 and DIC = 326.700
DIC is an estimate of expected predictive error (lower deviance is better).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the model seems to do well in terms of convergence, although
autocorrelation is possibly more of a concern (check the values of
&lt;code&gt;n.eff&lt;/code&gt;, which are slightly smaller than the nominal sample size of
38000). This is not uncommon with hierarchical/random effect models.&lt;/p&gt;
&lt;p&gt;In terms of comparison with the results of the fixed effects version,
there is generally an increase in the value of the mean for the log ORs
&lt;code&gt;d&lt;/code&gt;, coupled with larger uncertainty. On the other hand, the absolute
probabilities of quitting &lt;code&gt;pq&lt;/code&gt; are rather stable. The estimate for &lt;code&gt;L&lt;/code&gt;
does not change — but this is not surprising, as this node is modelled
independently on the other variables in the code and based on an
informative prior, which is not updated by any data. So, simply changing
parts of the model that are effectively disconnected by the one in which
we model &lt;code&gt;L&lt;/code&gt; is not changing our estimates for this node.&lt;/p&gt;
&lt;p&gt;We can also complete the model to include the cost-effectiveness
component. We do this by firstly defining a vector of unit costs
associated with each intervention. Here we assume that No intervention
does not involve any cost for the public payer, while Self-help,
Individual and Group counselling do have some costs. We then define the
measures of effectiveness as the overall life years gained (&lt;code&gt;L&lt;/code&gt;)
weighted by the absolute probability of quitting smoking (&lt;code&gt;pq&lt;/code&gt;) for each
intervention. We build the variables &lt;code&gt;e&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; in the loop over the 4
interventions. Notably, in this case, we do not model the costs
(although a variant of this model that does account for this is
presented in the the &lt;a href=&#34;https://gianluca.statistica.it/book/bcea/&#34;&gt;BCEA Book&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### Cost-effectiveness analysis
library(BCEA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;BCEA&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:graphics&amp;#39;:
## 
##     contour&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unit.cost &amp;lt;- c(0,200,6000,600)
ints &amp;lt;- c(&amp;quot;No contact&amp;quot;,&amp;quot;Self help&amp;quot;,&amp;quot;Individual counselling&amp;quot;,&amp;quot;Group counselling&amp;quot;)
e &amp;lt;- c &amp;lt;- matrix(NA,res2$n.sims,4)
# MCMC sample from distribution of life-years gained by quitting
L &amp;lt;- res2$sims.list$L 
# ...and from distributions of probability of quitting for each of 4 interventions
pq &amp;lt;- res2$sims.list$pq 

for (t in 1:4) {
    e[,t] &amp;lt;- L*pq[,t]
    c[,t] &amp;lt;- unit.cost[t]
}
colnames(e) &amp;lt;- colnames(c) &amp;lt;- ints&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can run &lt;code&gt;BCEA&lt;/code&gt; to post-process the model output and produce
the relevant summaries, e.g. summaries or the cost-effectiveness plane.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- bcea(e,c,interventions=ints,Kmax=1000,ref=4)
summary(m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NB: k (wtp) is defined in the interval [0 - 1000]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Cost-effectiveness analysis summary 
## 
## Reference intervention:  Group counselling
## Comparator intervention(s): No contact
##                           : Self help
##                           : Individual counselling
## 
## Optimal decision: choose No contact for k &amp;lt; 274
##                          Self help for 274 &amp;lt;= k &amp;lt; 310
##                          Group counselling for k &amp;gt;= 310
## 
## 
## Analysis for willingness to pay parameter k = 1000
## 
##                        Expected utility
## No contact                       1096.7
## Self help                        1629.9
## Individual counselling          -3727.5
## Group counselling                2523.5
## 
##                                                 EIB    CEAC     ICER
## Group counselling vs No contact             1426.85 0.89945   296.03
## Group counselling vs Self help               893.65 0.78947   309.20
## Group counselling vs Individual counselling 6251.05 1.00000 -6345.08
## 
## Optimal intervention (max expected utility) for k = 1000: Group counselling
##            
## EVPI 97.442&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that, because this model involves multiple comparisons, the
default output for the &lt;code&gt;plot&lt;/code&gt; function in &lt;code&gt;BCEA&lt;/code&gt; is not entirely
satisfactory. There are ways in which we can modify this default
behaviour to improve the look of the pictures (see the help for &lt;code&gt;BCEA&lt;/code&gt;
as well as the &lt;a href=&#34;https://gianluca.statistica.it/book/bcea/&#34;&gt;BCEA Book&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(m)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/06_nma/solutions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 7. PSA to structural uncertainty — SOLUTIONS</title>
      <link>/practical/07_structural/solutions/</link>
      <pubDate>Wed, 22 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/practical/07_structural/solutions/</guid>
      <description>


&lt;p&gt;The first thing we do is loading and analysing the “base-case” model, which is stored in the object &lt;code&gt;statins_base.Rdata&lt;/code&gt; and the “robust” version of the model, stored in &lt;code&gt;statins_HC.Rdata&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(BCEA)
library(R2OpenBUGS)
load(&amp;quot;statins_base.Rdata&amp;quot;)
load(&amp;quot;statins_HC.Rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;print&lt;/code&gt; to visualise the output for the two models, for example as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(statins_base)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output to this call is a long list of summary statistics — it is also possible to visualise an excerpt by using code such as the following&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(statins_base$summary[,c(&amp;quot;mean&amp;quot;,&amp;quot;sd&amp;quot;,&amp;quot;2.5%&amp;quot;,&amp;quot;97.5%&amp;quot;,&amp;quot;Rhat&amp;quot;,&amp;quot;n.eff&amp;quot;)],n=15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                 mean       sd      2.5%     97.5%     Rhat n.eff
cost.hosp[1] 238.7010 137.0428  91.68029  482.0066 1.001533   980
cost.hosp[2] 315.6340 168.7579 124.20521  668.4667 1.002554  1000
cost.hosp[3] 523.0695 451.0165 144.02705 1357.9918 1.002820  1000
cost.hosp[4] 424.9861 232.1917 170.08732  877.9484 1.001743   980
cost.hosp[5] 305.1978 172.3249 120.79156  656.3935 1.002747   550
cost.hosp[6] 301.1282 163.4569 121.85935  618.0017 1.001342  1000
cost.stat[1] 480.8821 289.1621 137.50194 1232.7586 1.004167   360
cost.stat[2] 350.0194 201.6103 103.42871  871.5393 1.000252  1000
cost.stat[3] 166.6851 125.6502  34.24849  498.3156 1.000219  1000
cost.stat[4] 305.4061 261.7113  47.69648 1008.5598 1.004566  1000
cost.stat[5] 346.9371 209.6277 103.21293  880.0139 1.006177   400
cost.stat[6] 165.0717 129.7352  35.28509  526.9310 1.000074  1000
cost.tot[1]  719.5831 324.8498 309.13357 1526.9058 1.005610   270
cost.tot[2]  665.6534 265.4862 331.33786 1357.5038 1.001748  1000
cost.tot[3]  689.7546 467.3877 242.54283 1629.7467 1.002190  1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which produces the first &lt;code&gt;n=15&lt;/code&gt; rows (i.e. parameters) for the whole summary table. In particular, we only select the columns headed as &lt;code&gt;&#34;mean&#34;&lt;/code&gt;, &lt;code&gt;&#34;sd&#34;&lt;/code&gt;, etc. (we do so to exclude additional quantiles that are automatically stored in the object &lt;code&gt;statins_base$summary&lt;/code&gt;). We should make sure that the models have all converged and that autocorrelation is not an issue (by e.g. analysing the &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_{eff}\)&lt;/span&gt; statistics).&lt;/p&gt;
&lt;p&gt;We can already check the DIC associated with each of the two models, to get some ideas of which one will be given most weight in the structural PSA. We can do so by using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Displays the DIC for the two models
c(statins_base$DIC,statins_HC$DIC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2233.875 2225.988&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As is easy to see, the “robust” model is associated with a relatively lower DIC (by over 10 points).&lt;/p&gt;
&lt;p&gt;We can now move on and use the results from the two Bayesian models as inputs to &lt;code&gt;BCEA&lt;/code&gt;. The objects &lt;code&gt;statins_base$sims.list&lt;/code&gt; and &lt;code&gt;statins_HC$sims.list&lt;/code&gt; contain the simulated values for all the model parameters monitored in &lt;code&gt;list&lt;/code&gt; format. We can follow the script and use the code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Defines the intervention labels
interventions &amp;lt;- c(&amp;quot;Atorvastatin&amp;quot;,&amp;quot;Fluvastatin&amp;quot;,&amp;quot;Lovastatin&amp;quot;,&amp;quot;Pravastatin&amp;quot;,
                   &amp;quot;Rosuvastatin&amp;quot;,&amp;quot;Simvastatin&amp;quot;)
# BCEA object with the economic analysis of the &amp;quot;base case&amp;quot; model
m1 &amp;lt;- bcea(statins_base$sims.list$effect,statins_base$sims.list$cost.tot,
           ref=1,interventions=interventions)
# BCEA object with the economic analysis of the Half-Cauchy model
m2 &amp;lt;- bcea(statins_HC$sims.list$effect,statins_HC$sims.list$cost.tot,
           ref=1,interventions=interventions)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to first define a vector of intervention labels and then apply the function &lt;code&gt;bcea&lt;/code&gt; to the suitable variables of effects and costs in the two models.&lt;/p&gt;
&lt;p&gt;The two objects &lt;code&gt;m1&lt;/code&gt; and &lt;code&gt;m2&lt;/code&gt; can be post-processed as any &lt;code&gt;BCEA&lt;/code&gt; objects (e.g. using &lt;code&gt;plot&lt;/code&gt; or &lt;code&gt;print&lt;/code&gt; methods). But in addition to this, we can also combine them to perform the PSA to structural assumptions. To do so, we need to manipulate the original objects in a suitable way. Firstly, we need to create a &lt;code&gt;list&lt;/code&gt; of models, which we can simply do using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Combines the BUGS models
models &amp;lt;- list(statins_base,statins_HC)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the newly created object contains the information from the two &lt;code&gt;BUGS&lt;/code&gt; models. We can also create suitable lists in which we store the relevant variables of effectiveness and costs from the two models, for example using code such as the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creates the variables of effectiveness and costs
effects &amp;lt;- list(statins_base$sims.list$effect, statins_HC$sims.list$effect)
costs &amp;lt;- list(statins_base$sims.list$cost.tot, statins_HC$sims.list$cost.tot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can feed these inputs to the &lt;code&gt;BCEA&lt;/code&gt; function &lt;code&gt;struct.psa&lt;/code&gt; as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finally uses BCEA to perform the structural PSA to consider the base and HC models
m3 &amp;lt;- struct.psa(models,effects,costs,ref=1,interventions=interventions)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;struct.psa&lt;/code&gt; takes as basic arguments three lists, containing the &lt;code&gt;BUGS&lt;/code&gt; models, the list of effects and the list of costs simulations and combines them to compute the model weights (based on the DICs).&lt;/p&gt;
&lt;p&gt;The object &lt;code&gt;m3&lt;/code&gt; is a &lt;code&gt;list&lt;/code&gt;, which contains 3 elements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(m3,function(x) list(class(x),names(x))) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$he
$he[[1]]
[1] &amp;quot;bcea&amp;quot; &amp;quot;list&amp;quot;

$he[[2]]
 [1] &amp;quot;n_sim&amp;quot;         &amp;quot;n_comparators&amp;quot; &amp;quot;n_comparisons&amp;quot; &amp;quot;delta_e&amp;quot;      
 [5] &amp;quot;delta_c&amp;quot;       &amp;quot;ICER&amp;quot;          &amp;quot;Kmax&amp;quot;          &amp;quot;k&amp;quot;            
 [9] &amp;quot;ceac&amp;quot;          &amp;quot;ib&amp;quot;            &amp;quot;eib&amp;quot;           &amp;quot;kstar&amp;quot;        
[13] &amp;quot;best&amp;quot;          &amp;quot;U&amp;quot;             &amp;quot;vi&amp;quot;            &amp;quot;Ustar&amp;quot;        
[17] &amp;quot;ol&amp;quot;            &amp;quot;evi&amp;quot;           &amp;quot;ref&amp;quot;           &amp;quot;comp&amp;quot;         
[21] &amp;quot;step&amp;quot;          &amp;quot;interventions&amp;quot; &amp;quot;e&amp;quot;             &amp;quot;c&amp;quot;            


$w
$w[[1]]
[1] &amp;quot;numeric&amp;quot;

$w[[2]]
NULL


$DIC
$DIC[[1]]
[1] &amp;quot;numeric&amp;quot;

$DIC[[2]]
NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;lapply&lt;/code&gt; applies iteratively the function &lt;code&gt;class&lt;/code&gt; and &lt;code&gt;names&lt;/code&gt; to each element of the object &lt;code&gt;m3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As is possible to see, the first one, named &lt;code&gt;he&lt;/code&gt;, is an object in the class &lt;code&gt;BCEA&lt;/code&gt; and so it contains the usual elements that such objects do (e.g. &lt;code&gt;n.sim&lt;/code&gt;, &lt;code&gt;n.comparators&lt;/code&gt;, etc). The elements &lt;code&gt;w&lt;/code&gt; and &lt;code&gt;DIC&lt;/code&gt; are numeric vectors and include the weights and the value of the DIC associated with each individual models. We can visualise for example the weights by using the command,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m3$w)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.01901127 0.98098873&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which indicates that the second model (the “robust” Half-Cauchy) is given a weight of over 98%. This is consistent with the fact that its DIC is lower than the one for the base case model, which in turn indicates better fit.&lt;/p&gt;
&lt;p&gt;We can also use all the methods implemented for &lt;code&gt;BCEA&lt;/code&gt; objects to analyse and visualise the output of the model averaging. For example, we can summarise the cost-effectiveness analysis by typing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(m3$he)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Cost-effectiveness analysis summary 

Reference intervention:  Atorvastatin
Comparator intervention(s): Fluvastatin
                          : Lovastatin
                          : Pravastatin
                          : Rosuvastatin
                          : Simvastatin

Optimal decision: choose Simvastatin for k &amp;lt;  and  for k &amp;gt;= 


Analysis for willingness to pay parameter k = 25000

             Expected utility
Atorvastatin            22823
Fluvastatin             22435
Lovastatin              21381
Pravastatin             21731
Rosuvastatin            22526
Simvastatin             22738

                                  EIB  CEAC     ICER
Atorvastatin vs Fluvastatin   388.695 0.768  3919.60
Atorvastatin vs Lovastatin   1442.154 0.843  1432.44
Atorvastatin vs Pravastatin  1092.152 0.958   336.72
Atorvastatin vs Rosuvastatin  297.304 0.738  5227.07
Atorvastatin vs Simvastatin    85.002 0.594 19129.49

Optimal intervention (max expected utility) for k = 25000: Atorvastatin
           
EVPI 193.22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we could plot the cost-effectiveness plane with the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ceplane.plot(m3$he)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/07_structural/solutions_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
or generate multiple treatments comparison cost-effectiveness acceptability curves with the following commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m3.multi&amp;lt;-multi.ce(m3$he)
ceac.plot(m3.multi)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/07_structural/solutions_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that because &lt;em&gt;in this particular case&lt;/em&gt; one of the models is effectively given an almost 100% weight, the model average will resemble it almost identically.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cohort discrete Markov model</title>
      <link>/practical/09_mm/cohort-model/</link>
      <pubDate>Wed, 22 Jun 2022 15:15:00 +0000</pubDate>
      <guid>/practical/09_mm/cohort-model/</guid>
      <description> 



&lt;div id=&#34;instructions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Instructions&lt;/h2&gt;
&lt;p&gt;The following exercise helps you run the Markov model and the underlying Bayesian model to estimate the transition probabilities in &lt;span&gt;&lt;span&gt;&lt;code&gt;R&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;and &lt;span&gt;&lt;span&gt;&lt;code&gt;BUGS&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;. You should also look specifically at BMHE and actually run through the calculations to make sure you understand how the process works. In particular, look at the part relative to discounting and how you do that, which is very prevelent in real applied work.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Open the file &lt;a href=&#34;MarkovModel1.txt&#34;&gt;&lt;code&gt;MarkovModel1.txt&lt;/code&gt;&lt;/a&gt; and inspect the &lt;code&gt;BUGS&lt;/code&gt; model code.
Make sure you understand the assumptions encoded in the model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Follow the script &lt;a href=&#34;MarkovModel1.R&#34;&gt;&lt;code&gt;MarkovModel1.R&lt;/code&gt;&lt;/a&gt;, which will guide you through the
necessary steps to create the data, run the MCMC model and then
perform the relevant economic analysis from &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;solutions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solutions&lt;/h2&gt;
&lt;p&gt;The &lt;tt&gt;R&lt;/tt&gt; script guides you through the process of running the Markov model analysis for the asthma problem seen in the lecture.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/5state.png&#34; width=&#34;500&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;The graphical representation of the Markov model for the ‘asthma’ problem discussed in section 5.4 of BMHE&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The script first sets up the number of states &lt;span class=&#34;math inline&#34;&gt;\(S=5\)&lt;/span&gt; and the number of time points &lt;span class=&#34;math inline&#34;&gt;\(J=12\)&lt;/span&gt; weeks in the virtual follow up. Also, we load the data, given in the form of matrices with the observed transitions across the states, during the actual follow up in the trial. The code is relatively straightforward — we use the &lt;tt&gt;R&lt;/tt&gt; command &lt;code&gt;matrix&lt;/code&gt; to define the data. Notice that the numbers included in the matrix are read by row (as the command &lt;code&gt;byrow=TRUE&lt;/code&gt; suggests).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;S = 5
r.0 = (matrix(c(
66,32,0,0,2, 
42,752,0,5,20, 
0,4,0,1,0, 
0,0,0,0,0, 
0,0,0,0,156),c(S,S),byrow=TRUE))

r.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     [,1] [,2] [,3] [,4] [,5]
[1,]   66   32    0    0    2
[2,]   42  752    0    5   20
[3,]    0    4    0    1    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0  156&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can check that these tie up with the matrix presented in the lecture slides — notice that &lt;code&gt;r.0&lt;/code&gt; indicates the data for the control arm.&lt;/p&gt;
&lt;p&gt;Perhaps more interestingly, the script also defines the prior distribution for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\lambda\)&lt;/span&gt; in terms of a Dirichlet distribution.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;Click to view more details on the Dirichlet distribution&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;The Dirichlet distribution is a multivariate generalisation of the Beta. Specifically, where the Beta$(\alpha,\beta)$ is a good model for a single parameter ranging between 0 and 1, the Dirichlet can be used to model $S$ parameters $\lambda_1,\ldots,\lambda_S$ that are all constrained in the range $[0;1]$ and such that $\sum_{s=1}^S \lambda_s = 1$. This fits nicely the property that transition probabilites off a given state $s$ are **exhaustive and mutually exclusive**, meaning that all are between 0 and 1 and that they must sum to 1.&lt;/p&gt;
&lt;p&gt;Like the Beta distribution is related to the Binomial sampling distribution, so the Dirichlet is to the multivariate generalisation of the Binomial, i.e. the &lt;em&gt;Multinomial&lt;/em&gt; distribution. So, if our situation involves $S$ possible outcomes each of which occurs in $y_1,\ldots,y_S$ counts out of the total sample size $n$, we can model the sampling distribution as $y_1,\ldots,y_S \sim {\sf Multinomial}(\boldsymbol\lambda)$, where $\boldsymbol\lambda=(\lambda_1,\ldots,\lambda_S)$ is the vector of probabilities associated with each possible outcome.&lt;/p&gt;
&lt;p&gt;Modelling $\boldsymbol\lambda \sim {\sf Dirichlet}(\boldsymbol{a})$ for a vector of parameters $\boldsymbol{a}=(a_1,\ldots,a_S)$ is the equivalent of the &lt;em&gt;conjugated&lt;/em&gt; Beta-Binomial model &amp;mdash; with the added benefit that the posterior distribution is also a Dirichlet:
$$\boldsymbol\lambda\mid \boldsymbol{y}\sim {\sf Dirichlet}(a_1+y_1, \ldots, a_S+y_S).$$&lt;/p&gt;
&lt;p&gt;In a Dirichlet distribution, the parameters $\boldsymbol{a}$ have a relatively intuitive interpretation, as they are proportional to the expected probability associated with the various outcomes. So if $a_S$ is very large in comparison to the other parameters $a_1,\ldots,a_{s-1},a_{s+1},\ldots,a_S$, then we are encoding the fact that outcome $s$ is much more likely to occur than the others. The scale of the parameters indicates the precision. So for example, a Dirichlet$(1,1,1)$ encodes the assumption that the three possible categories are all equally likely (because the underlying parameters $a_1,a_2,a_3$ are all the same). But in the same way, so would a Dirichlet$(100,100,100)$, or for that matter a Dirichlet$(0.01,0.01,0.01)$. In all cases the three parameters have the same value. The third distribution implies the lowest level of precision, while the second one implies large precision: intuitively, the larger the value, the more prior knowledge we consider.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/dirichlets.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The graph above shows four &lt;a href=&#34;https://en.wikipedia.org/wiki/Simplex&#34;&gt;&lt;em&gt;symplexes&lt;/em&gt;&lt;/a&gt;: each side of the triangles represent one of the Dirichlet parameters (in this case we assume an underlying variable with three possible categories). When $a_1=a_2=a_3=1$, the mass of points (representing simulations from the underlying Dirichlet distribution) is spread all over the area in the triangle. This is the equivalent of a vague prior where the probability mass is spread all over the range of the variable. Conversely, when $a_1=a_2=a_3=50$, the mass is concentrated in a small, central part of the triangle &amp;mdash; intuitively, this means that the three dimensions carry the same weight (because the parameters have the same value); because that value is large, then the variance of the points is relatively low. When one of the dimensions has a much larger value than the others, the points tend to be pulled towards that area, as happens when we consider a Dirichlet$(2,5,10)$.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;In this case, the “scale” parameter of the Dirichlet distribution is assumed to be &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0=\alpha_1=10\)&lt;/span&gt;. This assumption implies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We effectively assume the same prior distribution in both arms of the trial. &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt; governs the behaviour of the control arm, while &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt; is the parameter defining how the probabilities &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1,\ldots,\lambda_S\)&lt;/span&gt; act in the treatment arm.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;We define &lt;code&gt;alpha.0 = alpha.1 = rep(scale, S)&lt;/code&gt;; this implies that&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scale=10
alpha.0 = alpha.1 = rep(scale,S)    
alpha.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10 10 10 10 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;i.e. that &lt;em&gt;in the prior&lt;/em&gt;, each of the &lt;span class=&#34;math inline&#34;&gt;\(S=5\)&lt;/span&gt; categories (STW, UTW, Pex, Hex, TF) is assumed to have the same probability &lt;code&gt;scale&lt;/code&gt;/ &lt;span class=&#34;math inline&#34;&gt;\(\sum_{s=1}^S\)&lt;/span&gt; &lt;code&gt;scale&lt;/code&gt;. Intuitively, we specify our prior by imagining a “thought experiment” in which the sample size is &lt;span class=&#34;math inline&#34;&gt;\(\sum_{s=1}^S\)&lt;/span&gt; &lt;code&gt;scale&lt;/code&gt;=50, in this case. This sample size is assumed in our prior to be distributed equally across the &lt;span class=&#34;math inline&#34;&gt;\(S=5\)&lt;/span&gt; states, so that we think that there are &lt;code&gt;scale&lt;/code&gt;=10 individuals in each of the states. This is a relatively strong prior. For example, a Dirichlet(0.1,0,1,0.1,0.1,0.1) would indicate the same thought experiment with a sample size of just &lt;span class=&#34;math inline&#34;&gt;\(5\times 0.1=0.5\)&lt;/span&gt; individuals, of which &lt;span class=&#34;math inline&#34;&gt;\(0.1\)&lt;/span&gt; is allocated to each of the states.&lt;/p&gt;
&lt;p&gt;Of course, there’s nothing special about this construction — in fact simply convenience. We may have thought about this problem much more carefully and determined that in fact we would expect, in the prior, more individuals to be in the STW state, say twice as many as in UTW, three times as many as in Pex, four times as many as in Hex and 10 times as many as in TF. This could translate, for example, into a Dirichlet(10,5,3.3,2.5,1) prior. If we felt very confident about this, we may even express our prior into a Dirichlet(1000,500,330,250,100) prior — i.e. with the same proportionality but much larger numbers, to imply bigger precision.&lt;/p&gt;
&lt;p&gt;The script also proceeds to define the initial values. This is also interesting. We use the mathematical results whereby we can simulate from a Dirichlet distribution by first constructing independent Gamma variables and then rescaling the simulated values by their sum. In &lt;tt&gt;R&lt;/tt&gt;, we create the &lt;code&gt;inits&lt;/code&gt; function, in which we first simulate a matrix of Gamma(scale, 1) values using the command &lt;code&gt;rgamma(4*S,scale,1)&lt;/code&gt;; then we place the resulting vector of &lt;span class=&#34;math inline&#34;&gt;\(4\times S\)&lt;/span&gt; values into a matrix of dimension &lt;span class=&#34;math inline&#34;&gt;\(4\times S\)&lt;/span&gt;, e.g. the object &lt;code&gt;temp.0&lt;/code&gt;. Notice that because TF is assumed to be an &lt;em&gt;absorbing&lt;/em&gt; state, by definition &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\lambda_{S}=(0,0,0,0,1)\)&lt;/span&gt;. Thus, we do not need to initialise this row of the matrix of parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\lambda\)&lt;/span&gt; and therefore, we only need a matrix of size &lt;span class=&#34;math inline&#34;&gt;\(4\times S\)&lt;/span&gt;. Next, we create the variable &lt;code&gt;sum.temp.0&lt;/code&gt; in which we record the row totals of &lt;code&gt;temp.0&lt;/code&gt; and then construct the matrix &lt;code&gt;mat.0&lt;/code&gt; by rescaling &lt;code&gt;temp.0&lt;/code&gt; by these totals. Finally, we use the command &lt;code&gt;list&lt;/code&gt; to store the named variables we want to output in the function &lt;code&gt;inits&lt;/code&gt;, i.e. &lt;code&gt;lambda.0&lt;/code&gt; and &lt;code&gt;lambda.1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inits &amp;lt;- function(){
    temp.0 &amp;lt;- matrix(rgamma(4*S,scale,1),4,S)
    sum.temp.0 &amp;lt;- apply(temp.0,1,sum)
    mat.0 &amp;lt;- temp.0/sum.temp.0
    temp.1 &amp;lt;- matrix(rgamma(4*S,scale,1),4,S)
    sum.temp.1 &amp;lt;- apply(temp.1,1,sum)
    mat.1 &amp;lt;- temp.1/sum.temp.1 
    list(lambda.0=rbind(mat.0,rep(NA,S)),lambda.1=rbind(mat.1,rep(NA,S)))
}

inits()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$lambda.0
          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 0.2620049 0.1656849 0.1735277 0.1906087 0.2081738
[2,] 0.1721008 0.1694657 0.3168546 0.1709276 0.1706513
[3,] 0.2621914 0.2312971 0.2281732 0.1588414 0.1194969
[4,] 0.1646739 0.1817589 0.2073144 0.1667142 0.2795386
[5,]        NA        NA        NA        NA        NA

$lambda.1
          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 0.1258738 0.1952657 0.3466712 0.1115298 0.2206596
[2,] 0.1677436 0.1495265 0.2917020 0.2010646 0.1899634
[3,] 0.2207405 0.1171875 0.2113657 0.2074268 0.2432796
[4,] 0.1412204 0.1312841 0.2364687 0.1709408 0.3200860
[5,]        NA        NA        NA        NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the &lt;code&gt;BUGS&lt;/code&gt; model has run, we have estimates for the transition probabilities &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\lambda\)&lt;/span&gt;. Given these simulations, we can construct the Markov model “virtual” follow up, using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now run the Markov model from R
start &amp;lt;- c(1,0,0,0,0)   # NB analysis for 1 single patient!
                        # (can create a &amp;quot;virtual&amp;quot; population of N individuals
                        #  and allocate them across the states at time j=0)

# Markov transitions
m.0 &amp;lt;- m.1 &amp;lt;- array(NA,c(n.sims,S,(J+1)))
for (s in 1:S){
    m.0[,s,1] &amp;lt;- start[s]
    m.1[,s,1] &amp;lt;- start[s]
}

lam0=lam1=array(NA,c(n.sims,S,S))
for (i in 1:n.sims) {
  lam0[i,,]=rbind(lambda.0[i,,],c(0,0,0,0,1))
  lam1[i,,]=rbind(lambda.1[i,,],c(0,0,0,0,1))
    for (j in 2:(J+1)){
        for (s in 1:S){
           # Use the new matrices lam0 and lam1 to do the matrix multiplication
            m.0[i,s,j] &amp;lt;- sum(m.0[i,,j-1]*lam0[i,,s])
            m.1[i,s,j] &amp;lt;- sum(m.1[i,,j-1]*lam1[i,,s])
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For simplicity, we run the MM for a single patient and we initialise the “cohort” so that the patient is in state &lt;span class=&#34;math inline&#34;&gt;\(s=1\)&lt;/span&gt; (STW) at the beginning of the follow up (&lt;span class=&#34;math inline&#34;&gt;\(j=0\)&lt;/span&gt;). There’s no special reason for this; we could have a larger size of the cohort (e.g. the sum of the vector &lt;code&gt;start&lt;/code&gt;); or we may have a different initial distribution across the states.&lt;/p&gt;
&lt;p&gt;Then we construct the arrays &lt;code&gt;m.0&lt;/code&gt; and &lt;code&gt;m.1&lt;/code&gt; in which we will store the total transitions in each state and for each time. We use the recursive relationship &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{m}_{j+1}=\boldsymbol{m}_j \boldsymbol\Lambda_j\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Lambda_j\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(S\times S\)&lt;/span&gt; matrix storing the transition probabilities for all the states, at time &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Notice that because the objects &lt;code&gt;lambda.0&lt;/code&gt; and &lt;code&gt;lambda.1&lt;/code&gt; are obtained as output from &lt;tt&gt;bugs&lt;/tt&gt;, they are arrays where the first dimension is the number of simulations produced.&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;BUGS&lt;/code&gt; stores the simulations for the “random” part of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Lambda_j\)&lt;/span&gt;. In fact, because the last row (containing the transition probabilities off the absorbing state TF) is deterministically defined, &lt;code&gt;BUGS&lt;/code&gt; doesn’t consider it a parameter. Thus, before we can apply the matrix multiplication, we need to construct a matrix, which we call &lt;code&gt;lam0&lt;/code&gt; and &lt;code&gt;lam1&lt;/code&gt; respectively for the control and active treatment arm, in the code above, in which we stack up the &lt;span class=&#34;math inline&#34;&gt;\(i-\)&lt;/span&gt; MCMC simulated values for the first &lt;span class=&#34;math inline&#34;&gt;\((S-1)\)&lt;/span&gt; rows of the transition probability matrix (estimating the transitions off the states STW, UTW, Pex and Hex) and a row vector of values &lt;span class=&#34;math inline&#34;&gt;\((0 0 0 0 1)\)&lt;/span&gt;, which indicates that for the state TF, the only possible movement is to remain in it.&lt;/p&gt;
&lt;p&gt;The rest of the script post-process the output of the Bayesian model to create a “Markov trace”, i.e. a barplot displaying the number/proportion of patients transitioning in each state at each time point. This is a useful graph, as it allows us to try and make sense of the underlying population dynamics that is implied by the Markov model we have constructed. In real-life applications, we would want to produce this graph and validate it with the help of the wider research team (including clinicians and experts of the subject matter).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/barplot-1.png&#34; style=&#34;width:45.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/barplot-2.png&#34; style=&#34;width:45.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above barplots, the darker bar indicates the proportion of individuals in STW and increasingly lighter colours indicate, respectively, the proportion of people in UTW, Pex, Hex and TF. As is possible to see, the active treatment (SFC) seems to be associated with a population dynamics in which more people tend to remain in the most favourable outcome (STW).&lt;/p&gt;
&lt;p&gt;Finally, we define and apply discounting to the resulting costs and effects and then use &lt;code&gt;BCEA&lt;/code&gt; to produce the economic analysis. The code is fairly straighforward. Firstly, we define the discount rate, set at 3.5% for both benefits and costs. Notice that this is the “standard” &lt;em&gt;annual&lt;/em&gt; discount rate suggested by NICE. In this particular example, the virtual follow up is set at &lt;span class=&#34;math inline&#34;&gt;\(J=12\)&lt;/span&gt; weeks, so technically we don’t really need to discount the output (as 12 weeks is barely 4 months, let alone more than one year!). Then we fill in the elements of the vectors &lt;code&gt;disc.b&lt;/code&gt; and &lt;code&gt;disc.c&lt;/code&gt; with the discounted series of values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## General formulation to apply discount
delta.b &amp;lt;- 0.035    # discount rate for benefits (3.5%)
delta.c &amp;lt;- 0.035    # discount rate for costs (3.5%)
# Defines the discount factors
disc.b &amp;lt;- numeric(); disc.c &amp;lt;- numeric()
disc.b[1] &amp;lt;- 1; disc.c[1] &amp;lt;- 1
for (j in 2:J) {
    disc.b[j] &amp;lt;- (1+delta.b)^(j-1)
    disc.c[j] &amp;lt;- (1+delta.c)^(j-1)
}

disc.b&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 1.000000 1.035000 1.071225 1.108718 1.147523 1.187686 1.229255 1.272279
 [9] 1.316809 1.362897 1.410599 1.459970&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Essentially, the discounted multiplier for time &lt;span class=&#34;math inline&#34;&gt;\(j=0\)&lt;/span&gt; is simply 1 (no discounting); at time &lt;span class=&#34;math inline&#34;&gt;\(j=1\)&lt;/span&gt; it is 1.035; and at the end of follow up is 1.45997.&lt;/p&gt;
&lt;p&gt;Then, we actually apply these discounting multipliers to the estimated effects and costs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disc.cost0 &amp;lt;- disc.eff0 &amp;lt;- disc.cost1 &amp;lt;- disc.eff1 &amp;lt;- matrix(NA,mm1$n.sims,J)
for (j in 1:J) {
    disc.cost0[,j] &amp;lt;- cost0[,j]/disc.c[j]
    disc.cost1[,j] &amp;lt;- cost1[,j]/disc.c[j]
    disc.eff0[,j] &amp;lt;- m.0[,1,j]/disc.b[j]
    disc.eff1[,j] &amp;lt;- m.1[,1,j]/disc.b[j]
}

# Shows difference between raw and discounted costs (for 1st simulation)
cost0[1,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 100.23659 104.27436  89.77322  76.01900  66.42544  60.41060  56.81619
 [8]  54.71874  53.50987  52.81776  52.42299  52.19835&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disc.cost0[1,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 100.23659 100.74817  83.80426  68.56478  57.88594  50.86410  46.22001
 [8]  43.00843  40.63601  38.75402  37.16364  35.75304&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As is possible to see, the raw costs are actually higher than the discounted counterparts (apart from the first element, for which the discounting multiplier is 1 and so, effectively, no discounting occurs).&lt;/p&gt;
&lt;p&gt;At this point, we define the economic outcome as the sum of the discounted series of values for benefits and costs, over the &lt;span class=&#34;math inline&#34;&gt;\(J=12\)&lt;/span&gt; time points, using the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sums the values across all time points and creates matrix of costs
c &amp;lt;- matrix(NA,n.sims,2)
c[,1] &amp;lt;- apply(cost0,1,sum)
c[,2] &amp;lt;- apply(cost1,1,sum)

# Effectiveness
e &amp;lt;- matrix(NA,n.sims,2)
e[,1] &amp;lt;- apply(m.0[,1,],1,sum)
e[,2] &amp;lt;- apply(m.1[,1,],1,sum)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to complete the analysis, we run &lt;code&gt;BCEA&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cost-effectiveness analysis
library(BCEA)
ints &amp;lt;- c(&amp;quot;FP&amp;quot;,&amp;quot;SFC&amp;quot;)
m &amp;lt;- bcea(e,c,ref=2,interventions=ints,Kmax=300)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can be used to summarise the economic model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NB: k (wtp) is defined in the interval [0 - 300]

Cost-effectiveness analysis summary 

Reference intervention:  SFC
Comparator intervention: FP

SFC dominates for all k in [0 - 300] 


Analysis for willingness to pay parameter k = 300

    Expected utility
FP           -250.17
SFC           523.57

             EIB   CEAC   ICER
SFC vs FP 773.74 0.9995 -94.31

Optimal intervention (max expected utility) for k = 300: SFC
             
EVPI 0.018456&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 8. Survival analysis — SOLUTIONS</title>
      <link>/practical/08_survival/solutions/</link>
      <pubDate>Wed, 22 Jun 2022 11:00:00 +0000</pubDate>
      <guid>/practical/08_survival/solutions/</guid>
      <description>


&lt;div id=&#34;survival-analysis-using-survhe&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Survival analysis using &lt;code&gt;survHE&lt;/code&gt;&lt;/h2&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Preliminaries&lt;/h3&gt;
&lt;p&gt;This practical assumes that you have installed &lt;code&gt;survHE&lt;/code&gt;, a
&lt;tt&gt;R&lt;/tt&gt; package specifically designed to perform
survival analysis in health economic evaluation and with advanced
facilities for Bayesian modelling.&lt;/p&gt;
&lt;p&gt;You can install &lt;code&gt;survHE&lt;/code&gt; you can either use the “official” &lt;a href=&#34;https://cran.r-project.org/web/packages/survHE/index.html&#34;&gt;CRAN version&lt;/a&gt;, or the
most-updated, &lt;a href=&#34;https://github.com/giabaio/survHE&#34;&gt;“development”&lt;/a&gt;. This can take a little time, as there
are several “dependencies” (i.e. packages that are required for &lt;code&gt;survHE&lt;/code&gt;
to work properly).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Install survHE from CRAN
install.packages(&amp;quot;survHE&amp;quot;)

# Or the development version fro GitHub
devtools::install.github(&amp;quot;giabaio/survHE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;survHE&lt;/code&gt; is installed, you simply need to load it into your
&lt;tt&gt;R&lt;/tt&gt; session, as usual and then we can also load
some data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loads survHE in the session
library(survHE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: flexsurv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: survival&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;survival_data.Rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This loads a dataset called &lt;code&gt;dat&lt;/code&gt;, which contains some survival data. In
particular, the dataset includes the patients ID; the time of
progression to a more severe stage of cancer; an indicator for the event
of interest (mortality); an indicator for the treatment arm (coded as 0
= control and 1 = active treatment); an indicator for the patients’ sex
(0 = male; 1 = female); the patients’ age (in years); and the Index of
Multiple Deprivation (IMD) score (this is a census-based, area-level
measure of socio-economic circumstances. It is coded as categorical
variable taking values in the interval &lt;span class=&#34;math inline&#34;&gt;\([1;5]\)&lt;/span&gt;, where 1 indicates the
least deprived and 5 indicates the most deprived areas). We can inspect
it as usual, using built-in &lt;tt&gt;R&lt;/tt&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loads survHE in the session
head(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  ID_patient time event arm sex age imd
1          1 0.03     0   0   1  32   2
2          2 0.03     0   0   1  43   2
3          3 0.92     0   0   1  25   4
4          4 1.48     0   0   0  36   3
5          5 1.64     0   0   1  38   5
6          6 1.64     0   0   0  35   1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(dat$arm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
189 178 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(dat$arm,dat$event)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
      0   1
  0  90  99
  1 105  73&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 189 individuals in arm &lt;code&gt;0&lt;/code&gt; (controls) and 178 in the arm &lt;code&gt;1&lt;/code&gt;
(some active drug). The data include a patient ID, the time at which the
event has been observed (e.g. progression to a worse disease state) and
an indicator for censoring. Individuals who are not fully observed are
associated with &lt;code&gt;censored=1&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-in-a-frequentist-setting&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Model fitting in a frequentist setting&lt;/h3&gt;
&lt;p&gt;We are instructed to fit both the Exponential and the Weibull model to
the data, assuming a linear predictor of the form
&lt;span class=&#34;math display&#34;&gt;\[g(\mu_i) = \log(\mu_i) = \beta_0 + \beta_1\texttt{arm}_i.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to analyse the data, we first need to define the model we want
to use and the distributions we want to use. We can simply set this out
using the following &lt;tt&gt;R&lt;/tt&gt; commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Defines the model formula and the distributions
formula=Surv(time,event)~as.factor(arm)
mods=c(&amp;quot;exp&amp;quot;,&amp;quot;weibull&amp;quot;)

# Then runs survHE to estimate the two models
m1=fit.models(formula=formula,data=dat,distr=mods)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;formula&lt;/code&gt; specifies the model in terms of regression for the
generalised linear predictor, which in this case only depends on the
treatment arm (notice that, because &lt;code&gt;arm&lt;/code&gt; is a categorical variable, we
include it in our analysis as a &lt;tt&gt;R&lt;/tt&gt; “&lt;code&gt;factor&lt;/code&gt;”;
the first value &lt;code&gt;arm=0&lt;/code&gt; will be used as reference category). Notice also
that we need to use the specific notation &lt;code&gt;Surv(time=time, event=event)&lt;/code&gt;
to tell &lt;tt&gt;R&lt;/tt&gt; and &lt;code&gt;survHE&lt;/code&gt; that our data are in
survival analysis form. Then we set up a vector &lt;code&gt;mods&lt;/code&gt; in which we
include some string text identifying the Exponential and Weibull models
(more details are available in the &lt;a href=&#34;https://www.jstatsoft.org/article/view/v095i14&#34;&gt;&lt;code&gt;survHE&lt;/code&gt; documentation&lt;/a&gt;).
Finally, we are ready to run the function &lt;code&gt;fit.models&lt;/code&gt;, which is used by
&lt;code&gt;survHE&lt;/code&gt; to perform the analysis and estimate the model parameters.&lt;/p&gt;
&lt;p&gt;The results of the models are stored in an object &lt;code&gt;m1&lt;/code&gt;, which contains
several elements.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Explores the model output
names(m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;models&amp;quot;        &amp;quot;model.fitting&amp;quot; &amp;quot;method&amp;quot;        &amp;quot;misc&amp;quot;         &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(m1,names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$models
[1] &amp;quot;Exponential&amp;quot;   &amp;quot;Weibull (AFT)&amp;quot;

$model.fitting
[1] &amp;quot;aic&amp;quot; &amp;quot;bic&amp;quot; &amp;quot;dic&amp;quot;

$method
NULL

$misc
[1] &amp;quot;time2run&amp;quot;   &amp;quot;formula&amp;quot;    &amp;quot;data&amp;quot;       &amp;quot;model_name&amp;quot; &amp;quot;km&amp;quot;        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;tt&gt;R&lt;/tt&gt; command &lt;code&gt;lapply&lt;/code&gt; can be used to “apply”
the function &lt;code&gt;names&lt;/code&gt; to all the elements of the list &lt;code&gt;m1&lt;/code&gt;, to provide
details of each of its elements. So, for example, the object &lt;code&gt;m$models&lt;/code&gt;
contains two objects (&lt;code&gt;Exponential&lt;/code&gt; and &lt;code&gt;Weibull (AFT)&lt;/code&gt;), in which the
estimates are stored.&lt;/p&gt;
&lt;p&gt;The output for the modelling can be visualised using the &lt;code&gt;print&lt;/code&gt; method,
as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Defines the model formula and the distributions
print(m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model fit for the Exponential model, obtained using Flexsurvreg 
(Maximum Likelihood Estimate). Running time: 0.023 seconds

                      mean         se       L95%      U95%
rate             0.0824203 0.00828355  0.0676839  0.100365
as.factor(arm)1 -0.4656075 0.15427131 -0.7679738 -0.163241

Model fitting summaries
Akaike Information Criterion (AIC)....: 1274.576
Bayesian Information Criterion (BIC)..: 1282.387&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(m1,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model fit for the Weibull AF model, obtained using Flexsurvreg 
(Maximum Likelihood Estimate). Running time: 0.011 seconds

                     mean        se     L95%      U95%
shape            1.816383 0.1098390 1.613371  2.044941
scale           10.220953 0.5705218 9.161747 11.402616
as.factor(arm)1  0.342019 0.0855445 0.174355  0.509683

Model fitting summaries
Akaike Information Criterion (AIC)....: 1203.130
Bayesian Information Criterion (BIC)..: 1214.846&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This takes an optional argument, which allows to specify which model
should be printed, in case more than one distribution has been selected
(e.g. in this case). Notice that, by default, &lt;code&gt;survHE&lt;/code&gt; uses maximum
likelihood as the “&lt;code&gt;method&lt;/code&gt;” to perform the estimation (as reported by
the output of the &lt;code&gt;print&lt;/code&gt; function).&lt;/p&gt;
&lt;p&gt;The model output can also be plotted in terms of the resulting survival
curves, on top of the Kaplan-Meier estimate. This can be done using the
&lt;code&gt;plot&lt;/code&gt; command, using the option &lt;code&gt;add.km=TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Defines the model formula and the distributions
plot(m1,add.km=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/08_survival/solutions_files/figure-html/model2-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The resulting graph shows the survival curves within the observed
time-frame (0.03—20.92), for all the models fitted in &lt;code&gt;m&lt;/code&gt;. As expected
from the theory, the Exponential model does not do a good job at
following the observed shape of the data, as it is not flexible enough.
The Weibull model is much closer to the empirical estimate provided by
the Kaplan-Meier curve. This is confirmed by the analysis of the
Information Criterion statistics (AIC and BIC): for the Exponential
model they are both greater than the equivalent values obtained for the
Weibull model, indicating that the latter fit the observed data better.&lt;/p&gt;
&lt;p&gt;In general terms, the survival curves (which are just 1 — the cumulative
probability curves) can be used to read off the relevant probability at
a given time. For example, if we consider the following graph, it would
be fairly easy to read off that at time &lt;span class=&#34;math inline&#34;&gt;\(t=15\)&lt;/span&gt;, the survival probability
is roughly about 0.25 (in fact, to be precise, it can be computed with
some algebra to be 0.3797412). Similar (approximate) computations can be
made on a grid of values (as represented in the graph) for different
times and or probability values.
&lt;img src=&#34;/practical/08_survival/solutions_files/figure-html/model2b-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
It is often useful to compute, at least in an approximate ways,
(survival) probabilities using this method.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-modelling&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Bayesian modelling&lt;/h3&gt;
&lt;p&gt;As mentioned in the lecture, standard MCMC algorithms may struggle with
survival data, especially when they are characterised by a large number
of censored observations. Thus, &lt;code&gt;survHE&lt;/code&gt; implements Bayesian analysis
using two alternative Bayesian computation methods. The first one is
based on Integrated Nested Laplace Approximation (INLA), while the
second uses a variant of MCMC called Hamiltonian Monte Carlo (HMC).&lt;/p&gt;
&lt;p&gt;Without going too much into the details (some of which are described in
the &lt;code&gt;survHE&lt;/code&gt; manual, INLA is very fast (almost as fast as the MLE procedure) and produces
precise results, but is only available (at present) for a limited set of
distributions. On the other hand, HMC is a little slower, but is perhaps
a little more flexible and allows for more distributional assumptions.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;survHE&lt;/code&gt;, it is very simple to specify what “method” of inference
should be used, by simply setting the option &lt;code&gt;method&lt;/code&gt; to either &lt;code&gt;mle&lt;/code&gt;
(the default), or &lt;code&gt;inla&lt;/code&gt;, or &lt;code&gt;hmc&lt;/code&gt;. So, for example, we could replicate
the analysis above using INLA by simply using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Runs survHE to estimate the two models using INLA
m2=fit.models(formula=formula,data=dat,distr=mods,method=&amp;quot;inla&amp;quot;)

# Shows the output for the Exponential model
print(m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model fit for the Exponential model, obtained using INLA (Bayesian inference via 
Integrated Nested Laplace Approximation). Running time: 0.62141 seconds

                      mean         se       L95%      U95%
rate             0.0828715 0.00836297  0.0669431  0.100186
as.factor(arm)1 -0.4665097 0.14721656 -0.7474321 -0.177349

Model fitting summaries
Akaike Information Criterion (AIC)....: 1276.583
Bayesian Information Criterion (BIC)..: 1288.299
Deviance Information Criterion (DIC)..: 1277.371&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# And then for the Weibull model
print(m2,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model fit for the Weibull AF model, obtained using INLA (Bayesian inference via 
Integrated Nested Laplace Approximation). Running time: 1.284 seconds

                     mean        se     L95%      U95%
shape            1.764107 0.1114979 1.552269  1.973406
scale           10.281869 0.6121163 9.186980 11.575483
as.factor(arm)1  0.344241 0.0893014 0.182237  0.532154

Model fitting summaries
Akaike Information Criterion (AIC)....: 1205.359
Bayesian Information Criterion (BIC)..: 1220.981
Deviance Information Criterion (DIC)..: 1206.669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As is possible to see, many of the results are very similar to the MLE
analysis above. This is because, by default, both the INLA and HMC
implementation use relatively weak prior distributions for both the
location &lt;span class=&#34;math inline&#34;&gt;\(\mu_i=g^{-1}(\boldsymbol\beta)\)&lt;/span&gt; and the ancillary parameters
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\alpha\)&lt;/span&gt; (see lecture slides). These priors can be modified, but this
requires some changes to the call to the &lt;code&gt;fit.models&lt;/code&gt; function (see the
manual for more details). Because INLA specifies a Bayesian model, there
is an additional Information Criterion available, the DIC, which is also
printed in the summary tables. Once again, the Weibull model is
preferable as it is associated with lower values of the AIC, BIC and
DIC.&lt;/p&gt;
&lt;p&gt;In a very similar way, we can specify the models using HMC as the
inferential engine, by using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Runs survHE to estimate the two models using HMC
m3=fit.models(formula=formula,data=dat,distr=mods,method=&amp;quot;hmc&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can still use the &lt;code&gt;print&lt;/code&gt; method to visualise the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Shows the output for the Exponential model
print(m3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model fit for the Exponential model, obtained using Stan (Bayesian inference via 
Hamiltonian Monte Carlo). Running time: 1.1656 seconds

                      mean         se       L95%       U95%
rate             0.0821905 0.00792447  0.0676933  0.0988463
as.factor(arm)1 -0.4626672 0.15001521 -0.7682862 -0.1793789

Model fitting summaries
Akaike Information Criterion (AIC)....: 1276.579
Bayesian Information Criterion (BIC)..: 1288.295
Deviance Information Criterion (DIC)..: 1274.295&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# And then for the Weibull model
print(m3,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model fit for the Weibull AF model, obtained using Stan (Bayesian inference via 
Hamiltonian Monte Carlo). Running time: 2.6113 seconds

                     mean        se     L95%      U95%
shape            1.804606 0.1106481 1.594570  2.018583
scale           10.273537 0.5923679 9.243481 11.492342
as.factor(arm)1  0.346925 0.0892387 0.177872  0.518907

Model fitting summaries
Akaike Information Criterion (AIC)....: 1205.143
Bayesian Information Criterion (BIC)..: 1220.765
Deviance Information Criterion (DIC)..: 1203.313&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, the results are fairly similar, numerically, due to the fact
that the priors are relatively weak and there are enough data to
consistently inform the posterior distributions for the parameters.
Again, the &lt;code&gt;survHE&lt;/code&gt; manual explains in more details how the priors can
be modified in order to include genuine information. Because HMC is an
MCMC algorithm, we can check the convergence diagnostics, much as we had
done for the &lt;tt&gt;BUGS&lt;/tt&gt; output in the previous
practicals. In particular, we could check the traceplots and histograms
for the posterior distributions of the parameters using built-in
functions in the &lt;code&gt;rstan&lt;/code&gt; package, which &lt;code&gt;survHE&lt;/code&gt; uses to perform the HMC
analysis, as in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Traceplots for the parameters of the Exponential model (the first element of m3$models)
rstan::traceplot(m3$models[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/08_survival/solutions_files/figure-html/model6-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Histograms for the parameters of the Weibull model (the second element of m3$models)
rstan::stan_hist(m3$models[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/08_survival/solutions_files/figure-html/model6-2.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A more familiar version of the summary statistics table for the HMC
output can be obtained by adding another optional argument to the call
to &lt;code&gt;print&lt;/code&gt;, as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Shows the output for the Exponential model
print(m3,2,original=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Inference for Stan model: WeibullAF.
2 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=2000.

               mean  se_mean       sd        2.5%         25%         50%
beta[1]    2.327924 0.001666 0.057311    2.223919    2.286785    2.325146
beta[2]    0.346925 0.002579 0.089239    0.177872    0.286620    0.343483
alpha      1.804606 0.002958 0.110648    1.594570    1.726319    1.803580
scale     10.273537 0.017270 0.592368    9.243481    9.843244   10.228174
lp__    -600.346661 0.039879 1.251980 -603.624141 -600.881002 -600.046654
                75%       97.5% n_eff     Rhat
beta[1]    2.366375    2.441681  1184 0.999171
beta[2]    0.407953    0.518907  1197 0.999553
alpha      1.883740    2.018583  1399 0.999804
scale     10.658682   11.492342  1177 0.999187
lp__    -599.475113 -598.892282   986 1.002001

Samples were drawn using NUTS(diag_e) at Mon Jun  6 14:39:34 2022.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily see that the &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; statistic is below the arbitrary
threshold of 1.1 for all the nodes and that the effective sample size
&lt;code&gt;n_eff&lt;/code&gt; is also rather close to the nominal sample size of 2000,
indicating that convergence is reached and autocorrelation is not an
issue.&lt;/p&gt;
&lt;p&gt;We can plot the results of all the model, selectively, by specifying a
more complex call to the &lt;code&gt;plot&lt;/code&gt; function, for example as in the
following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(MLE=m1,INLA=m2,HMC=m3,
     # Selects which models from the three fitted objects
     mods=c(1,2,3,4,5,6),
     # Specifies colours to plot the curves
     colour=c(&amp;quot;blue&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;red&amp;quot;,&amp;quot;yellow&amp;quot;,&amp;quot;magenta&amp;quot;,&amp;quot;orange&amp;quot;),
     # Defines the time horizon over which to make the plot
     t=seq(0,50)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/08_survival/solutions_files/figure-html/model8-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, the option &lt;code&gt;MLE=m1,INLA=m2,HMC=m3,mod=c(1,2,3,4,5,6)&lt;/code&gt; instructs
&lt;tt&gt;R&lt;/tt&gt; to first stack together the three objects
&lt;code&gt;m1&lt;/code&gt;, &lt;code&gt;m2&lt;/code&gt; and &lt;code&gt;m3&lt;/code&gt; (and give them the names &lt;code&gt;MLE&lt;/code&gt;, &lt;code&gt;INLA&lt;/code&gt; and &lt;code&gt;HMC&lt;/code&gt;) and the to select the models 1 to 6 (in this case,
all of them, because in each method we have fitted two distributions).
Then we specify colours and labels. As is possible to see, there is
virtually no difference in the estimates for the Exponential model,
while there are some minor ones for the Weibull. We can also set an
option &lt;code&gt;t=seq(0,50)&lt;/code&gt;, which instructs &lt;tt&gt;R&lt;/tt&gt; to
extrapolate the survival curves beyond the observed data and up to time
= 50.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;probabilistic-sensitivity-analysis&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;Probabilistic sensitivity analysis&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;survHE&lt;/code&gt; is designed to perform automatically PSA on the survival curves, based
on the underlying uncertainty in the model parameters. Irrespective of
the inferential engine (MLE or Bayesian), the function &lt;code&gt;make.surv&lt;/code&gt; uses
a simulation approach (based either on boostrap in the case of MLE, or
simulations from the posterior distributions in the case of the Bayesian
models) to then reconstruct the entire probability distribution of the
survival curves, in a specified time range.&lt;/p&gt;
&lt;p&gt;For example, the following code constructs an object &lt;code&gt;psa1&lt;/code&gt; in which
&lt;code&gt;nsim=1000&lt;/code&gt; simulations for the survival curves of &lt;code&gt;mod=2&lt;/code&gt; (the Weibull
specification) in &lt;code&gt;m1&lt;/code&gt; (the MLE analysis) are stored.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Performs PSA on the survival curves for the Weibull model (under MLE)
psa1=make.surv(m1,mod=2,t=seq(0.01,50),nsim=1000)
psa.plot(psa1,offset=2.5,col=c(&amp;quot;blue&amp;quot;,&amp;quot;red&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/08_survival/solutions_files/figure-html/model9-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The specialised function &lt;code&gt;psa.plot&lt;/code&gt; can be used to visualise the
resulting survival curves and the underlying uncertainty. &lt;code&gt;psa.plot&lt;/code&gt; can
be customised, e.g. by specifying the colour with which the curves need
to be plotted, or the distance between the terms of the label, which
appears in the top part of the graph. These describe the combination of
covariates associated with each curve — in this case, the blue curve is
associated with a value of the intercept of 1 and a value of the
treatment arm of 0 (i.e. the control arm), while the red curve is
associated with a value of 1 for the treatment arm (i.e. the active
treatment).&lt;/p&gt;
&lt;p&gt;In fact, the most recent (and current) version of &lt;code&gt;survHE&lt;/code&gt; can use the simpler function &lt;code&gt;plot&lt;/code&gt; to perform the extrapolation and PSA (see &lt;a href=&#34;https://egon.stats.ucl.ac.uk/static/stat0019/slides/06_Survival/?panelset2=plotting-%282%292#26&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Without getting into the technical details, the process can be
replicated for the Bayesian models — the main difference here is in the
fact that in this case (and particularly under HMC), the resulting
simulations will be a better approximation of the underlying joint
probability distribution of all the model parameters. As mentioned in
the classes, in cases where there is substantial correlation among the
parameters of the survival model (&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\alpha,\boldsymbol\beta\)&lt;/span&gt;), then this is
likely to give results that may differ from the rougher approximation
based on bootstrap.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 10. Missing data - SOLUTIONS</title>
      <link>/practical/10_missing/solutions/</link>
      <pubDate>Thu, 23 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/practical/10_missing/solutions/</guid>
      <description>


&lt;div id=&#34;bivariate-normal-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Normal model&lt;/h2&gt;
&lt;p&gt;The data are stored in a list format because the variables are non-balanced, in terms of sample size. In other words, the data for arm &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt; contain fewer points than for &lt;span class=&#34;math inline&#34;&gt;\(t=2\)&lt;/span&gt;. It would be possible to format this dataset using a &lt;code&gt;data.frame&lt;/code&gt;, but this would mean having a single column for each variable (stacking together the two treatment arms) and adding a treatment indicator.&lt;/p&gt;
&lt;p&gt;In any case, we can visualise the relevant information using usual &lt;code&gt;R&lt;/code&gt; commands, e.g. &lt;code&gt;names&lt;/code&gt; to check the naming of the variables and the &lt;code&gt;$&lt;/code&gt; operator to access them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Reads in the data list
data=readRDS(&amp;quot;missing_data.rds&amp;quot;)

# Checks the name of the variables
names(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;c&amp;quot; &amp;quot;e&amp;quot; &amp;quot;n&amp;quot; &amp;quot;u&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# And shows some values
data$c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
 [1]     NA    0.1     NA     NA     NA    0.1     NA     NA     NA  516.0
[11]     NA     NA     NA     NA  404.0     NA     NA  116.0  145.0  436.0
[21]  782.0     NA  145.0     NA     NA     NA    2.0     NA     NA    0.1
[31]     NA     NA     NA     NA  193.0    0.1     NA     NA   64.0     NA
[41]     NA     NA     NA     NA     NA 1039.0     NA    2.0     NA     NA
[51]     NA     NA    0.1     NA  246.0     NA     NA     NA  141.0  400.0
[61]  116.0    0.1     NA  360.0  281.0     NA    0.1     NA     NA     NA
[71]  107.0     NA     NA  123.0     NA

[[2]]
 [1]    NA    NA    NA 183.0    NA    NA    NA    NA   0.1 107.0    NA    NA
[13]    NA    NA 516.0    NA    NA    NA    NA    NA    NA    NA    NA    NA
[25]    NA   0.1    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
[37]   0.1    NA    NA 194.0  60.0 227.0    NA    NA    NA 266.0    NA    NA
[49]   0.1 169.0 346.0    NA    NA   0.1    NA    NA    NA 366.0    NA    NA
[61]    NA 380.5    NA    NA    NA    NA    NA    NA 268.0 123.0    NA    NA
[73]    NA 389.5    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As is possible to see, the element &lt;code&gt;data$c&lt;/code&gt; is itself a list. The data are clearly affected by many missing values (recorded in &lt;code&gt;R&lt;/code&gt; as &lt;code&gt;NA&lt;/code&gt;). We can also inspect the distribution of the variables using histograms, e.g. using the following commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Produces a histogram of the cost distribution in arm t=1
hist(data$c[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/setup2-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In general terms, it is important to make some preliminary analysis to evaluate the impact of missingness. For instance, if you had a large dataset with only a handful of missing values, then perhaps you could simply do a “Complete Case Analysis”, which probably would not have a massive impact on your overall results. But if missingness is very prevalent, then this would imply the need for more sophisticated analyses, as well as the fact that the results would be by necessity to be taken with a rather large pinch of salt… We can summarise the proportion of missing data using the following command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(1:2,function(x) table(is.na(data$e[[x]]))/sum(table(is.na(data$e[[x]]))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]

FALSE  TRUE 
 0.36  0.64 

[[2]]

    FALSE      TRUE 
0.2261905 0.7738095 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we can use the &lt;code&gt;R&lt;/code&gt; command &lt;code&gt;table&lt;/code&gt; to tabulate the data (on the effectiveness variable, named &lt;code&gt;e&lt;/code&gt;) depending on whether they are &lt;code&gt;NA&lt;/code&gt; (= missing) or not. We use the &lt;code&gt;lapply&lt;/code&gt; function to perform the operation on both the elements of the list &lt;code&gt;data$e&lt;/code&gt;. The code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(is.na(data$e[[x]]))/sum(table(is.na(data$e[[x]])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;can be used to compute the proportion (instead of the absolute number) of missingness in each arm. As is possible to see, in this particular case, missingness is a big problem, as in the two arms there are respectively 64.00% and 77.38% of missing values.&lt;/p&gt;
&lt;p&gt;We can then proceed to the modelling part. Firstly, we fit a bivariate Normal model for costs and effects; while this is not ideal because it clearly fails to accommodate the marked skewness in the data (as evidenced by the histogram above), it is a good starting point and at least allows us to account for potential correlation between costs and effects (and their missing mechanisms).&lt;/p&gt;
&lt;p&gt;The model code is not too dissimilar to those used in Practical 4 (on individual level data), but it does require some modifications, to account for the missing data. We firstly assume a marginal Normal distribution for the effectiveness
&lt;span class=&#34;math display&#34;&gt;\[ e_i \sim \mbox{Normal}\left( \phi_{eit},\psi_{et} \right), \]&lt;/span&gt;
where the individual mean response (QALYs) is modelled as a linear regression as a function of the baseline utility
&lt;span class=&#34;math display&#34;&gt;\[ \phi_{eit} = \alpha_0 + \alpha_1 (u_{it}-\mu_{ut}). \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;BUGS&lt;/code&gt; code, we need to be careful in defining the Normal distribution, which requires the &lt;em&gt;precision&lt;/em&gt; instead of the variance. For this reason, we code for example&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eff1[i] ~ dnorm(phi.e1[i],tau.e[1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;tau.e[1]&lt;/code&gt; is the precision, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\tau_{e1}=\psi_{et}^{-1}\)&lt;/span&gt;. In addition, the linear predictor simply translates the regression for &lt;span class=&#34;math inline&#34;&gt;\(\phi_{eit}\)&lt;/span&gt; — we notice here that we center the covariate &lt;span class=&#34;math inline&#34;&gt;\(u_{it}\)&lt;/span&gt; for simplicity in the interpretation and ease of convergence (in this way, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt; is the overall mean QALY in the population).&lt;/p&gt;
&lt;p&gt;The next thing to understand is that, because also &lt;span class=&#34;math inline&#34;&gt;\(u_{it}\)&lt;/span&gt; is affected by missing values, we need to model it explicitly, even if it is used as covariate in the model. Thus, we need to include a suitable `BUGS} statement to define a probability distribution to represent it. We choose for simplicity the same form as the QALYs &lt;span class=&#34;math inline&#34;&gt;\(e_{it}\)&lt;/span&gt; and thus specify a Normal distribution depending on a mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ut}\)&lt;/span&gt; and a &lt;em&gt;precision&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ut}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we can model the conditional distribution of the costs, given the effectiveness variable. The assumption is a model
&lt;span class=&#34;math display&#34;&gt;\[ c_{it} \sim \mbox{Normal}(\phi_{cit},\psi_{ct}), \]&lt;/span&gt;
where the mean is specified as a linear predictor
&lt;span class=&#34;math display&#34;&gt;\[ \phi_{cit} = \beta_0 + \beta_1 (e_{it}-\mu_{et}). \]&lt;/span&gt;
Because we are again centering the covariate included in this model, we can simply characterise the population mean costs and benefits as &lt;span class=&#34;math inline&#34;&gt;\(\mu_{ct} = \beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{et}=\alpha_0\)&lt;/span&gt;, which we can then use in the economic analysis.&lt;/p&gt;
&lt;p&gt;The `BUGS} code is replicated for the two treatment arms and it then specifies the prior distributions. The model parameters are &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}=(\boldsymbol\alpha,\boldsymbol\beta,\boldsymbol\sigma_{e},\boldsymbol\sigma_{c},\boldsymbol\mu_u,\boldsymbol\sigma_u)\)&lt;/span&gt;, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\alpha=(\alpha_0,\alpha_1)\stackrel{iid}{\sim}\mbox{Normal}(0,v)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; a large variance (e.g. a precision of 0.00001);&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta=(\beta_0,\beta_1)\stackrel{iid}{\sim}\mbox{Normal}(0,v)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; a large variance (e.g. a precision of 0.00001);&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\sigma_e=(\sigma_{e1},\sigma_{e2})\)&lt;/span&gt; are the standard deviations for the effectiveness measures in the two treatment arms. We specify vague priors on the log-standard deviation scale, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\log \sigma_{et}\sim \mbox{Uniform}(-5,10)\)&lt;/span&gt;. This of course induces a prior on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{et}\)&lt;/span&gt; and then on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{et}^2\)&lt;/span&gt; and then on &lt;span class=&#34;math inline&#34;&gt;\(\tau_{et}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\sigma_c=(\sigma_{c1},\sigma_{c2})\)&lt;/span&gt; are the standard deviations for the cost measures in the two treatment arms. We specify vague priors on the log-standard deviation scale, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\log \sigma_{ct}\sim \mbox{Uniform}(-5,10)\)&lt;/span&gt;. This of course induces a prior on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ct}\)&lt;/span&gt; and then on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{et}^2\)&lt;/span&gt; and then on &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ct}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\mu_u=(\mu_{u1},\mu_{u2})\stackrel{iid}{\sim}\mbox{Normal}(0,v)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; a large variance (e.g. a precision of 0.00001);&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\sigma_u=(\sigma_{u1},\sigma_{u2})\)&lt;/span&gt; are the standard deviations for the baseline utility in the two treatment arms. We specify vague priors on the log-standard deviation scale, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\log \sigma_{ut}\sim \mbox{Uniform}(-5,10)\)&lt;/span&gt;. This of course induces a prior on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ut}\)&lt;/span&gt; and then on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ut}^2\)&lt;/span&gt; and then on &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ut}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;BUGS&lt;/code&gt; code maps these assumptions directly and also adds some lines to derive the analytic form of the &lt;em&gt;conditional&lt;/em&gt; variance and precision for the costs. These are useful, but are not necessarily fundamental parameters.&lt;/p&gt;
&lt;p&gt;We can now run the model using &lt;code&gt;OpenBUGS&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loads the package
library(R2OpenBUGS)

# Defines: 
# 1. model file
filein=&amp;quot;Normal_Normal.txt&amp;quot;

# 2. data list
datalist=list(N1=data$n[[1]],eff1=data$e[[1]],cost1=data$c[[1]],u1=data$u[[1]],
          N2=data$n[[2]],eff2=data$e[[2]],cost2=data$c[[2]],u2=data$u[[2]])

# 3. parameters to monitor
params&amp;lt;-c(&amp;quot;mu.e&amp;quot;,&amp;quot;sd.e&amp;quot;,&amp;quot;alpha0&amp;quot;,&amp;quot;alpha1&amp;quot;,&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;,&amp;quot;Delta_e&amp;quot;,
          &amp;quot;Delta_c&amp;quot;,&amp;quot;mu.c&amp;quot;,&amp;quot;sd.c&amp;quot;,&amp;quot;eff1&amp;quot;,&amp;quot;eff2&amp;quot;,&amp;quot;cost1&amp;quot;,&amp;quot;cost2&amp;quot;)

# 4. number of iterations
n.iter&amp;lt;-10000

# 5. sets up initial values for crucial parameters
inits=function(){
   list(alpha0=rnorm(2),alpha1=rnorm(2),beta0=rnorm(2),beta1=rnorm(2),mu.u=rnorm(2))
}

# 6. runs the model
NN=bugs(data=datalist,inits=inits,parameters.to.save=params,
         model.file=filein,n.chains=2,n.iter=n.iter,n.thin=1,DIC=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All is fairly straightforward; we first point &lt;code&gt;R&lt;/code&gt; to the &lt;code&gt;.txt&lt;/code&gt; file including the model code, then we define a datalist in the format that matches the name of the variables in the model code, then define the vector of parameters to monintor and select a suitable number of iterations for the MCMC procedure.&lt;/p&gt;
&lt;p&gt;In this case, it is important to generate suitable initial values. For instance, if you did not create the function &lt;code&gt;inits} and let&lt;/code&gt;BUGS} initialise the chains, the programme would return an error and cannot start the procedure. We set up the initial values for &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\alpha,\boldsymbol\beta,\mu_u\)&lt;/span&gt; from Normal(0,1) distributions, which ensures that the algorithm can sample reasonable starting points and run.&lt;/p&gt;
&lt;p&gt;We can check that the model has reached convergence and that the estimates are reasonable. For instance, we could use the &lt;code&gt;print&lt;/code&gt; method and apply it to the object &lt;code&gt;NN&lt;/code&gt;, where we have stored the &lt;code&gt;BUGS&lt;/code&gt; output. However, we have monitored many parameters and so this may be complicated to see. In cases such as this, it is probably more effective to use the element `$summary” to visualise the data, for instance using the following code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(NN$summary[,c(&amp;quot;mean&amp;quot;,&amp;quot;sd&amp;quot;,&amp;quot;2.5%&amp;quot;,&amp;quot;97.5%&amp;quot;,&amp;quot;Rhat&amp;quot;,&amp;quot;n.eff&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                mean         sd      2.5%    97.5%     Rhat n.eff
mu.e[1]   0.87119481 0.02232442 0.8268950 0.914805 1.000952 10000
mu.e[2]   0.91179054 0.02240187 0.8679975 0.956400 1.001053  9700
sd.e[1]   0.07969351 0.01191242 0.0605200 0.107200 1.000956 10000
sd.e[2]   0.08954983 0.01636157 0.0641200 0.127800 1.000912 10000
alpha0[1] 0.87119481 0.02232442 0.8268950 0.914805 1.000952 10000
alpha0[2] 0.91179054 0.02240187 0.8679975 0.956400 1.001053  9700&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which would simply show the first few rows of the overall summary table for a selected set of columns (those displaying the mean, sd, 2.5- and 97.5% quantiles, as well as the convergence statistics). We could also specify the parameters for which we want to see the summary statistics, for instance using the following code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN$summary[grep(&amp;quot;alpha&amp;quot;,rownames(NN$summary)),
           c(&amp;quot;mean&amp;quot;,&amp;quot;sd&amp;quot;,&amp;quot;2.5%&amp;quot;,&amp;quot;97.5%&amp;quot;,&amp;quot;Rhat&amp;quot;,&amp;quot;n.eff&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;               mean         sd      2.5%    97.5%     Rhat n.eff
alpha0[1] 0.8711948 0.02232442 0.8268950 0.914805 1.000952 10000
alpha0[2] 0.9117905 0.02240187 0.8679975 0.956400 1.001053  9700
alpha1[1] 0.7912292 0.15647962 0.4809875 1.091025 1.001219  4700
alpha1[2] 0.2751874 0.08561333 0.1061975 0.443105 1.000901 10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to show the selected summary statistics for all the nodes whose name contains the keyword &lt;code&gt;alpha&lt;/code&gt; (this is done using the &lt;code&gt;grep&lt;/code&gt; function — see &lt;code&gt;help(grep)&lt;/code&gt; in your &lt;code&gt;R&lt;/code&gt; terminal, for more details).&lt;/p&gt;
&lt;p&gt;Another way to simply check convergence when the number of model parameters is very large is to plot the output for both &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_{eff}\)&lt;/span&gt; for all parameters. We can do this simply using the following command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(NN$summary[,&amp;quot;Rhat&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/show.res3-1.png&#34; width=&#34;60%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
to check that the value of &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; is below the arbitrary threshold of 1.1 for all nodes. If this is the case, then the model has reached convergence; if not, we can investigate further and check which node has not converged yet. Of course the graph can be annotated and made prettier, but this crude version can still be very helpful.&lt;/p&gt;
&lt;p&gt;Another similar graphical representation can be made to check the number of effective samples obtained using the MCMC procedure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(NN$summary[,&amp;quot;n.eff&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/show.res4-1.png&#34; width=&#34;60%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As is possible to see here, most of the nodes have a value that is close to the nominal sample size, indicating virtually no issues with autocorrelation.&lt;/p&gt;
&lt;p&gt;Once we are satisfied about the output of the Bayesian model, we can feed it to &lt;code&gt;BCEA&lt;/code&gt; to perform the economic analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extracts the simulated values for the population mean effectiveness and costs
e=cbind(NN$sims.list$mu.e[,1],NN$sims.list$mu.e[,2])
c=cbind(NN$sims.list$mu.c[,1],NN$sims.list$mu.c[,2])

# Post-processes the results using BCEA
library(BCEA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;BCEA&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:graphics&amp;#39;:

    contour&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CEA_NN=bcea(e=e,c=c,ref = 2)
# Shows the C/E plane
ceplane.plot(CEA_NN)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/show.res5-1.png&#34; width=&#34;60%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that we first create suitable matrices with the simulations for the two main variables &lt;span class=&#34;math inline&#34;&gt;\((\mu_{et},\mu_{ct})\)&lt;/span&gt;; we extract these simulations from the object &lt;code&gt;NN&lt;/code&gt; — in particular, they are stored in the two lists &lt;code&gt;$sims.list$mu.e&lt;/code&gt; and &lt;code&gt;$sims.list$mu.c&lt;/code&gt;, respectively. Each of these two objects has dimension (10000, 2), i.e. 10000 rows (simulations) and 2 columns (treatment arms). We use the &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;cbind&lt;/code&gt; to “bind” together these values, so that the resulting objects &lt;code&gt;e&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; are matrices, as required by &lt;code&gt;BCEA&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Because we have monitored the variables &lt;code&gt;eff1,eff2,cost1,cost2&lt;/code&gt; in the object &lt;code&gt;NN&lt;/code&gt;, we can also check what the model is doing in terms of imputing the missing values. As suggested in the script, the easiest way doing so is by plotting the estimated posterior distribution, which we do using the following commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(NN$sims.list$eff1[,1],xlab=&amp;quot;QALYs for the first missing individual in t=1&amp;quot;,main=&amp;quot;&amp;quot;)
abline(v=1,lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/show.res6-1.png&#34; width=&#34;60%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(NN$sims.list$eff1[,1]&amp;gt;1)/NN$n.sims&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.0036&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The graph shows the posterior for &lt;code&gt;eff1&lt;/code&gt;, the measure of effectiveness in treatment arm &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt;. As is possible to see, about 0.360% of the simulated values above the theoretical upper limit of 1 — which is of course not reasonable. Another way of visualising this feature of the model output is by using the specialised plotting function &lt;code&gt;coefplot&lt;/code&gt; (which is available from the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;bmhe&lt;/code&gt; — which you would need to install), to produce a graph such as the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bmhe::coefplot(NN,parameter=&amp;quot;eff1&amp;quot;) + geom_vline(xintercept=1,linetype=2,size=1.3,col=&amp;quot;red&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/show.res7-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each of the 48 individuals affected by missing QALYs in &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt;, this shows a description of the posterior distribution (imputed values). The dots represent the means, while the lines extend over the 95% credible interval. As is possible to see, there are quite a few individuals whose imputed values distributions expands above 1, although the means are all below this threshold.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-gamma-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta-Gamma model&lt;/h2&gt;
&lt;p&gt;The Beta-Gamma model is theoretically more appropriate, because it correctly recognises the natural range of the two variables — the interval [0-1] for the QALYs and the open positive real line &lt;span class=&#34;math inline&#34;&gt;\((0,\infty)\)&lt;/span&gt; for the costs. Thus, it seems a more suitable modelling structure for this problem.&lt;/p&gt;
&lt;p&gt;In general terms, the model code is not much more complicated than the one for the bivariate Normal — the main complication is that the regression models for the individual average QALYs and costs cannot be constructed as linear predictor. Instead, we need to rescale the average QALYs (defined in [0-1]) onto &lt;span class=&#34;math inline&#34;&gt;\((-\infty,\infty)\)&lt;/span&gt;, which we do using a logistic regression
&lt;span class=&#34;math display&#34;&gt;\[ \mbox{logit}(\phi_{eit}) = \alpha_0 + \alpha_1 (u_{it}-\mu_{ut}). \]&lt;/span&gt;
This implies that the overall average effectiveness measure &lt;span class=&#34;math inline&#34;&gt;\(\mu_{et}\)&lt;/span&gt; is just a rescaled version of the intercept &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt; (given that we are centering the covariate &lt;span class=&#34;math inline&#34;&gt;\(u_{it}\)&lt;/span&gt;). In other words, we need to take the inverse logit transformation and define
&lt;span class=&#34;math display&#34;&gt;\[ \mu_{et} = \frac{\exp(\alpha_0)}{1+\exp(\alpha_0)}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similarly, we need to rescale the conditional regression for the average costs &lt;span class=&#34;math inline&#34;&gt;\(\phi_{cit}\)&lt;/span&gt; as a function of the centered effectiveness, which we can do using a log-linear regression
&lt;span class=&#34;math display&#34;&gt;\[ \mbox{log}(\phi_{cit}) = \beta_0 + \beta_1 (e_{it}-\mu_{et})  \]&lt;/span&gt;
and again the overall average cost is obtained by rescaling the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[ \mu_{ct} = \exp(\beta_0). \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In fact, the main complication in the &lt;code&gt;BUGS&lt;/code&gt; script is in the parameterisation of the Beta and Gamma distributions. &lt;code&gt;BUGS&lt;/code&gt; requires that the Beta distribution is defined in terms of two scale parameters and thus we code &lt;span class=&#34;math inline&#34;&gt;\(e_{it} \sim \mbox{Beta}(a_{ti},b_{ti})\)&lt;/span&gt; for each intervention arm and individuals. However, we need to actually model the individual mean &lt;span class=&#34;math inline&#34;&gt;\(\phi_{eit}\)&lt;/span&gt; and so we need to link this with &lt;span class=&#34;math inline&#34;&gt;\((a_{ti},b_{ti})\)&lt;/span&gt;. We can use the properties of the Beta distribution and derive that
&lt;span class=&#34;math display&#34;&gt;\[ a_{it} = \phi_{eit}\left[\frac{\phi_{eit}(1-\phi_{eit})}{\psi_{et}} - 1\right] \qquad \mbox{ and } \qquad b_{it} = (1-\phi_{eit})\left[\frac{\phi_{eit}(1-\phi_{eit})}{\psi_{et}} - 1\right],\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\psi_{et}\)&lt;/span&gt; is the variance for the Beta distribution (which is coded as &lt;code&gt;ss.e&lt;/code&gt; in the model file). By modelling the mean &lt;span class=&#34;math inline&#34;&gt;\(\phi_{eit}\)&lt;/span&gt; (through the priors induced on the parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\alpha\)&lt;/span&gt;) and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{et}\)&lt;/span&gt;, we then imply a prior on &lt;span class=&#34;math inline&#34;&gt;\((a_{it},b_{it})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A further complication is given by the fact that, because of the mathematical properties of the Beta distribution, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{et}\)&lt;/span&gt; is constrained in its range by the value taken by the overall mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{et}\)&lt;/span&gt;; for this reason, we define the prior as Uniform in the range $(0,).&lt;/p&gt;
&lt;p&gt;As for the Gamma distribution, we need to follow a similar strategy: the “original scale” parameters (which are required by &lt;code&gt;BUGS&lt;/code&gt;) are the shape and rate say &lt;span class=&#34;math inline&#34;&gt;\((\zeta_{it},\lambda_{it})\)&lt;/span&gt;; however, these can be related to the mean and variance using the mathematical properties of the Gamma distribution as
&lt;span class=&#34;math display&#34;&gt;\[ \mbox{shape}_{it} = \zeta_{it} = \frac{\phi_{cit}^2}{\sigma_{ct}^2} \qquad \mbox{ and } \qquad \mbox{rate}_{it} = \lambda_{it} = \frac{\phi_{cit}}{\sigma_{ct}^2}. \]&lt;/span&gt;
Again, priors on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{ct}\)&lt;/span&gt; (which in the model code is indicated as &lt;code&gt;sd.c[t]&lt;/code&gt; and is given a truncated &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution, to provide a continuous distribution on the positive real line) induce suitable priors for the shape and rate.&lt;/p&gt;
&lt;p&gt;The interesting point of this model is that because of its relative complexity, &lt;code&gt;OpenBUGS&lt;/code&gt; struggles to determine suitable initial values from which to start the simulation. In fact, we have tested several configurations, providing initial values for all the relevant model parameters and still have failed to run the model. Conversely, &lt;code&gt;JAGS&lt;/code&gt; is able to start and successfully estimate this model and data. The main reason for this crucial difference is probabaly to do with the specific version of the sampler used by the two pieces of software.&lt;/p&gt;
&lt;p&gt;In other words, to be able to run the Beta-Gamma model, it is necessary to install &lt;code&gt;JAGS&lt;/code&gt; (which is available and easy to install from &lt;a href=&#34;https://sourceforge.net/projects/mcmc-jags/files/&#34;&gt;https://sourceforge.net/projects/mcmc-jags/files/&lt;/a&gt;. Then we need to install the &lt;code&gt;R&lt;/code&gt; package &lt;code&gt;R2jags&lt;/code&gt; (which is the equivalent to &lt;code&gt;R2OpenBUGS&lt;/code&gt;). Fortunately, the differences between &lt;code&gt;JAGS&lt;/code&gt; and &lt;code&gt;OpenBUGS&lt;/code&gt; are virtually none. Thus, we can simply run the model following the script and using the commands below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Installs and R2jags 
install.packages(&amp;quot;R2jags&amp;quot;)

# Then loads the package
library(R2jags)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(this is of course only necessary once). When the package is installed, we just need to load it into our workspace and then proceed with the commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1. specifies the model file
filein=&amp;quot;Beta_Gamma.txt&amp;quot;

# 2. data list
datalist=list(N1=data$n[[1]],eff1=data$e[[1]],cost1=data$c[[1]],u1=data$u[[1]],
              N2=data$n[[2]],eff2=data$e[[2]],cost2=data$c[[2]],u2=data$u[[2]])

# 3. parameters to monitor
params&amp;lt;-c(&amp;quot;mu.e&amp;quot;,&amp;quot;sd.e&amp;quot;,&amp;quot;alpha0&amp;quot;,&amp;quot;alpha1&amp;quot;,&amp;quot;beta0&amp;quot;,&amp;quot;beta1&amp;quot;,&amp;quot;
          Delta_e&amp;quot;,&amp;quot;Delta_c&amp;quot;,&amp;quot;mu.c&amp;quot;,&amp;quot;sd.c&amp;quot;,&amp;quot;eff1&amp;quot;,&amp;quot;eff2&amp;quot;,&amp;quot;cost1&amp;quot;,&amp;quot;cost2&amp;quot;)

# 4. number of iterations
n.iter&amp;lt;-10000

# 5. reset the inits function
inits=NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we’re ready to run the main &lt;code&gt;jags&lt;/code&gt; function to run the model:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;JAGS&lt;/code&gt; compiles and initialises the model successfully and it does not even require us to specify the initial values (we still can do so, if we wanted to have full control, of course).&lt;/p&gt;
&lt;p&gt;We can now replicate the analysis performed above to check and summarise convergence, as well as to feed the output of our Beta-Gamma Bayesian model to &lt;code&gt;BCEA&lt;/code&gt;, to perform the economic analysis. Notice that the &lt;code&gt;JAGS&lt;/code&gt; object &lt;code&gt;BG&lt;/code&gt; has a slightly different structure and contains an “extra-layer” in comparison to its &lt;code&gt;OpenBUGS&lt;/code&gt; counterpart &lt;code&gt;NN&lt;/code&gt;}. As a result, in order to access the simulations, we need to go deeper inside the object and look into the element &lt;code&gt;$BUGSoutput$sims.list&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;e&amp;lt;-cbind(BG$BUGSoutput$sims.list$mu.e[,1],BG$BUGSoutput$sims.list$mu.e[,2])
c&amp;lt;-cbind(BG$BUGSoutput$sims.list$mu.c[,1],BG$BUGSoutput$sims.list$mu.c[,2])
CEA_BG&amp;lt;-bcea(e=e,c=c,ref = 2)

# Plots the CEAC
ceac.plot(CEA_NN)
points(CEA_BG$k,CEA_BG$ceac,t=&amp;quot;l&amp;quot;,col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/show.res10-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, the results are different, depending on the model used for imputation of the missing data. The graph above shows the CEACs for the two model specifications and as is possible to see the Normal-Normal model somewhat overestimates the probability of cost-effectiveness. This is likely due to the higher mean QALYs associated with each individual, given the imputations exceed the theoretical range of 1. If we check the &lt;code&gt;coefplot&lt;/code&gt; for the Beta-Gamma model, we now see that all the distributions are within the range [0-1], as should be.
&lt;img src=&#34;/practical/10_missing/solutions_files/figure-html/show.res11-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the graph we display the distributions for the Beta-Gamma (black) and Normal-Normal (red) models. A clear indication of the “pull” towards higher values that the bivariate Normal model shows is given by individual number 6. The red line is shifted upwards in comparison to the black one — this is because the Normal-Normal model does not know that it should not go above one (because the Normal distribution is unrestricted) and so it pulls up the estimate for this individual. This translate in slightly higher values for the QALYs, which in turn artificially increase the cost-effectiveness profile of the active intervention.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 15. Computing EVSI using regression</title>
      <link>/syllabus/practical15/</link>
      <pubDate>Fri, 24 Jun 2022 14:15:00 +0000</pubDate>
      <guid>/syllabus/practical15/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 16: Regression-based EVSI</title>
      <link>/syllabus/lecture16/</link>
      <pubDate>Fri, 24 Jun 2022 13:45:00 +0000</pubDate>
      <guid>/syllabus/lecture16/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Regression-based methods for EVSI computation&lt;/li&gt;
&lt;li&gt;Using SAVI to compute EVSI&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 14. Computing the EVSI using Monte Carlo simulations</title>
      <link>/syllabus/practical14/</link>
      <pubDate>Fri, 24 Jun 2022 13:00:00 +0000</pubDate>
      <guid>/syllabus/practical14/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 15: Calculating expected value of sample information</title>
      <link>/syllabus/lecture15/</link>
      <pubDate>Fri, 24 Jun 2022 11:15:00 +0000</pubDate>
      <guid>/syllabus/lecture15/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Efficient Computation methods for EVSI&lt;/li&gt;
&lt;li&gt;Efficient Nested Monte Carlo&lt;/li&gt;
&lt;li&gt;Using the EVSI Web App&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 13. Generating data for EVSI</title>
      <link>/syllabus/practical13/</link>
      <pubDate>Fri, 24 Jun 2022 10:30:00 +0000</pubDate>
      <guid>/syllabus/practical13/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 14: Generating data for the analysis of the EVSI</title>
      <link>/syllabus/lecture14/</link>
      <pubDate>Fri, 24 Jun 2022 09:45:00 +0000</pubDate>
      <guid>/syllabus/lecture14/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Variation in individual outcomes&lt;/li&gt;
&lt;li&gt;Predictive distribution data simulation&lt;/li&gt;
&lt;li&gt;Data generation in Excel and R&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 13: Expected value of sample information</title>
      <link>/syllabus/lecture13/</link>
      <pubDate>Fri, 24 Jun 2022 09:00:00 +0000</pubDate>
      <guid>/syllabus/lecture13/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Theory behind the EVSI&lt;/li&gt;
&lt;li&gt;Expected net benefit of sampling&lt;/li&gt;
&lt;li&gt;Optimising trial design&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 12. Computing the EVPPI in BCEA and SAVI</title>
      <link>/syllabus/practical12/</link>
      <pubDate>Thu, 23 Jun 2022 15:00:00 +0000</pubDate>
      <guid>/syllabus/practical12/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 12: Expected value of partial information</title>
      <link>/syllabus/lecture12/</link>
      <pubDate>Thu, 23 Jun 2022 13:45:00 +0000</pubDate>
      <guid>/syllabus/lecture12/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Theory behind the EVPPI&lt;/li&gt;
&lt;li&gt;Computational methods: regression-based methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 11. Computing the EVPI using nested Monte Carlo simulations</title>
      <link>/syllabus/practical11/</link>
      <pubDate>Thu, 23 Jun 2022 13:00:00 +0000</pubDate>
      <guid>/syllabus/practical11/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 11: Introduction to Value of Information</title>
      <link>/syllabus/lecture11/</link>
      <pubDate>Thu, 23 Jun 2022 11:00:00 +0000</pubDate>
      <guid>/syllabus/lecture11/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Introduction to Research prioritisation&lt;/li&gt;
&lt;li&gt;Value of Information and relevant measures&lt;/li&gt;
&lt;li&gt;Applications/examples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 10. Missing data</title>
      <link>/syllabus/practical10/</link>
      <pubDate>Thu, 23 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/syllabus/practical10/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 10: Missing data in cost-effectiveness modelling</title>
      <link>/syllabus/lecture10/</link>
      <pubDate>Thu, 23 Jun 2022 09:00:00 +0000</pubDate>
      <guid>/syllabus/lecture10/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Background – what’s difficult about missing data?&lt;/li&gt;
&lt;li&gt;Missing data mechanisms&lt;/li&gt;
&lt;li&gt;Missing data in health economic evaluation&lt;/li&gt;
&lt;li&gt;Applications/examples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 9. Markov models</title>
      <link>/syllabus/practical9/</link>
      <pubDate>Wed, 22 Jun 2022 15:15:00 +0000</pubDate>
      <guid>/syllabus/practical9/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 9: Markov models</title>
      <link>/syllabus/lecture9/</link>
      <pubDate>Wed, 22 Jun 2022 14:15:00 +0000</pubDate>
      <guid>/syllabus/lecture9/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;State-transition models for clinical history&lt;/li&gt;
&lt;li&gt;Bayesian implementation&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Markov models &amp;amp; survival analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 8. Survival analysis</title>
      <link>/syllabus/practical8/</link>
      <pubDate>Wed, 22 Jun 2022 13:00:00 +0000</pubDate>
      <guid>/syllabus/practical8/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 8: Survival analysis in HTA</title>
      <link>/syllabus/lecture8/</link>
      <pubDate>Wed, 22 Jun 2022 11:00:00 +0000</pubDate>
      <guid>/syllabus/lecture8/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Survival data in health economic evaluations&lt;/li&gt;
&lt;li&gt;Issues with Bayesian modelling&lt;/li&gt;
&lt;li&gt;PSA in survival analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The practical uses the &lt;tt&gt;R&lt;/tt&gt; packages &lt;a href=&#34;https://gianluca.statistica.it/software/survhe/&#34;&gt;&lt;code&gt;survHE&lt;/code&gt;&lt;/a&gt;. If you are interested, you can look at the extended &lt;code&gt;survHE&lt;/code&gt; tutorial (&lt;a href=&#34;http://www.statistica.it/gianluca/software/survhe/tutorial/&#34;&gt;here&lt;/a&gt;) and the &lt;code&gt;survHE&lt;/code&gt; &lt;a href=&#34;../../publication/baio-2019/&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 7. PSA to structural uncertainty</title>
      <link>/syllabus/practical7/</link>
      <pubDate>Wed, 22 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/syllabus/practical7/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 7: Model error and structural analysis</title>
      <link>/syllabus/lecture7/</link>
      <pubDate>Wed, 22 Jun 2022 09:00:00 +0000</pubDate>
      <guid>/syllabus/lecture7/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The rationale for structural uncertainty&lt;/li&gt;
&lt;li&gt;Model error and PSA to model uncertainty&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 6. Network meta-analysis</title>
      <link>/syllabus/practical6/</link>
      <pubDate>Tue, 21 Jun 2022 15:30:00 +0000</pubDate>
      <guid>/syllabus/practical6/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 6: Evidence synthesis and network meta-analysis</title>
      <link>/syllabus/lecture6/</link>
      <pubDate>Tue, 21 Jun 2022 14:15:00 +0000</pubDate>
      <guid>/syllabus/lecture6/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Motivation&lt;/li&gt;
&lt;li&gt;Fixed effects NMA&lt;/li&gt;
&lt;li&gt;Random effects NMA&lt;/li&gt;
&lt;li&gt;Applications/examples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 5. Evidence synthesis and decision models</title>
      <link>/syllabus/practical5/</link>
      <pubDate>Tue, 21 Jun 2022 12:15:00 +0000</pubDate>
      <guid>/syllabus/practical5/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 5: Aggregated level data</title>
      <link>/syllabus/lecture5/</link>
      <pubDate>Tue, 21 Jun 2022 11:15:00 +0000</pubDate>
      <guid>/syllabus/lecture5/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Role of evidence synthesis in decision modelling: Absolute and relative effects&lt;/li&gt;
&lt;li&gt;Meta-analysis of aggregated summaries from RCTs&lt;/li&gt;
&lt;li&gt;Evidence synthesis in health economics: Influenza example&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 4. Cost-effectiveness analysis with individual-level data</title>
      <link>/syllabus/practical4/</link>
      <pubDate>Tue, 21 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/syllabus/practical4/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 4: Individual level data in health economics</title>
      <link>/syllabus/lecture4/</link>
      <pubDate>Tue, 21 Jun 2022 09:00:00 +0000</pubDate>
      <guid>/syllabus/lecture4/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Data &amp;amp; “standard” analysis&lt;/li&gt;
&lt;li&gt;Example: UK700&lt;/li&gt;
&lt;li&gt;Analysis of cost data&lt;/li&gt;
&lt;li&gt;Analysis of cost-effectiveness data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 3. Introduction to R and cost-effectiveness analysis using BCEA</title>
      <link>/syllabus/practical3/</link>
      <pubDate>Mon, 20 Jun 2022 16:30:00 +0000</pubDate>
      <guid>/syllabus/practical3/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 3: Introduction to health economic evaluation</title>
      <link>/syllabus/lecture3/</link>
      <pubDate>Mon, 20 Jun 2022 15:30:00 +0000</pubDate>
      <guid>/syllabus/lecture3/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Health economic evaluation: What is and why do we need health economics?&lt;/li&gt;
&lt;li&gt;A framework for health economic evaluation&lt;/li&gt;
&lt;li&gt;Standard vs Bayesian HTA&lt;/li&gt;
&lt;li&gt;Decision-making: Cost-effectiveness plane; ICER; EIB&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 2. MCMC in BUGS</title>
      <link>/syllabus/practical2/</link>
      <pubDate>Mon, 20 Jun 2022 14:00:00 +0000</pubDate>
      <guid>/syllabus/practical2/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 2: Learning from data using MCMC and BUGS</title>
      <link>/syllabus/lecture2/</link>
      <pubDate>Mon, 20 Jun 2022 12:00:00 +0000</pubDate>
      <guid>/syllabus/lecture2/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bayes theorem for learning about parameters from observed data.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Introduction to Markov chain Monte Carlo (MCMC)&lt;/li&gt;
&lt;li&gt;Introduction to BUGS for estimating posterior distributions of parameters given data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical 1. Monte Carlo in BUGS</title>
      <link>/syllabus/practical1/</link>
      <pubDate>Mon, 20 Jun 2022 11:15:00 +0000</pubDate>
      <guid>/syllabus/practical1/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 1: Introduction to Bayesian reasoning, computation and BUGS</title>
      <link>/syllabus/lecture1/</link>
      <pubDate>Mon, 20 Jun 2022 10:00:00 +0000</pubDate>
      <guid>/syllabus/lecture1/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The Bayesian paradigm — expressing uncertainty using probabilities&lt;/li&gt;
&lt;li&gt;Overview of probability distributions for different types of quantities&lt;/li&gt;
&lt;li&gt;Inductive inference: Bayesian reasoning, Basic ideas, Forming ‘priors’”&lt;/li&gt;
&lt;li&gt;Predicting data with uncertain parameters.&lt;/li&gt;
&lt;li&gt;Introduction to Monte Carlo sampling in BUGS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;../&#34;&gt;Back to syllabus&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Course feedback</title>
      <link>/feedback/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      <guid>/feedback/</guid>
      <description> 



&lt;p&gt;&lt;em&gt;Please give scores and comments on the lecture and practical sessions to help us improve the course for the future.&lt;/em&gt; 🙇&lt;/p&gt;
&lt;form name=&#34;feedback_bmhe&#34; method=&#34;POST&#34; data-netlify=&#34;true&#34;&gt;
&lt;label&gt;&lt;b&gt;Your Name&lt;/b&gt;:&lt;/label&gt; &lt;input type=&#34;text&#34; name=&#34;name&#34; size=&#34;40%&#34;&gt;
&lt;label&gt;&lt;b&gt;Your Email&lt;/b&gt;:&lt;/label&gt; &lt;input type=&#34;email&#34; name=&#34;email&#34; size=&#34;40%&#34;&gt;
&lt;label&gt;&lt;b&gt;Affiliation&lt;/b&gt;:&lt;/label&gt; &lt;input type=&#34;text&#34; name=&#34;affiliation&#34; size=&#34;40%&#34;&gt;
&lt;label&gt;&lt;b&gt;Title&lt;/b&gt;:&lt;/label&gt; &lt;input type=&#34;text&#34; name=&#34;title&#34; size=&#34;40%&#34;&gt;
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;label&gt;&lt;b&gt; 1. How do you assess the quality of the lectures from 1 (very poor) to 10 (excellent) &lt;/b&gt; &lt;/label&gt;
&lt;ul class=&#34;likert&#34;&gt;
&lt;li&gt;
Very poor
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;1&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;2&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;3&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;4&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;5&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;6&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;7&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;8&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;9&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_lecture&#34; value=&#34;10&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
Excellent
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;&lt;br&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments1&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt; 2. How do assess the quality of the practicals from 1 (very poor) to 10 (excellent) &lt;/b&gt; &lt;/label&gt;
&lt;ul class=&#34;likert&#34;&gt;
&lt;li&gt;
Very poor
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;1&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;2&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;3&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;4&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;5&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;6&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;7&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;8&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;9&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;quality_practical&#34; value=&#34;10&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
Excellent
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments2&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;label&gt;&lt;b&gt; 3. What lecture(s) did you think were the most useful &lt;/b&gt;&lt;/label&gt;&lt;br&gt;
1. Introduction to Bayesian reasoning, computation and BUGS &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;01_Intro&#34;&gt;&lt;br&gt;
2. Learning from data using MCMC and BUGS &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;02_MCMC&#34;&gt;&lt;br&gt;
3. Introduction to health economic evaluation &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;03_Intro_HE&#34;&gt;&lt;br&gt;
4. Individual level data in health economics &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;04_ILD&#34;&gt;&lt;br&gt;
5. Aggregated level data &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;05_ALD&#34;&gt;&lt;br&gt;
6. Evidence synthesis and network meta-analysis &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;06_NMA&#34;&gt;&lt;br&gt;
7. Model error and structural analysis &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;07_Model_uncertainty&#34;&gt;&lt;br&gt;
8. Survival analysis in HTA &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;08_Survival&#34;&gt;&lt;br&gt;
9. Markov models &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;09_MM&#34;&gt;&lt;br&gt;
10. Missing data in cost-effectiveness modelling &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;10_Missing&#34;&gt;&lt;br&gt;
11. Introduction to Value of Information &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;11_VoI&#34;&gt;&lt;br&gt;
12. Expected value of partial information &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;12_EVPPI&#34;&gt;&lt;br&gt;
13. Expected value of sample information &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;13_EVSI&#34;&gt;&lt;br&gt;
14. Generating data for the analysis of the EVSI &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;14_Data_EVSI&#34;&gt;&lt;br&gt;
15. Calculating expected value of sample information &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;15_EVSI_MC&#34;&gt;&lt;br&gt;
16. Regression-based EVSI &lt;input type=&#34;checkbox&#34; name=&#34;useful_lecture&#34; value=&#34;16_EVSI_regression&#34;&gt;&lt;/p&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments3&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt; 4. Was the material useful? &lt;/b&gt; &lt;/label&gt;
&lt;em&gt;Lecture slides&lt;/em&gt;&lt;br&gt;
&lt;ul class=&#34;likert&#34;&gt;
&lt;li&gt;
Not useful at all
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;1&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;2&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;3&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;4&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;5&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
Extremely useful
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;em&gt;R practicals&lt;/em&gt; &lt;br&gt;
&lt;ul class=&#34;likert&#34;&gt;
&lt;li&gt;
Not useful at all
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;1&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;2&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;3&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;4&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;material_useful&#34; value=&#34;5&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
Extremely useful
&lt;/li&gt;
&lt;/ul&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments4&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt; 5. Did you feel you were prepared for the level of complexity of the lecture? &lt;/b&gt;&lt;/label&gt;
&lt;ul class=&#34;likert&#34;&gt;
&lt;li&gt;
Not very prepared at all
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;prepared_for_level&#34; value=&#34;1&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;prepared_for_level&#34; value=&#34;2&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;prepared_for_level&#34; value=&#34;3&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;prepared_for_level&#34; value=&#34;4&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;prepared_for_level&#34; value=&#34;5&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
Very prepared
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;&lt;br&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments5&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt; 6. Do you think you will use any of the concepts/methods/tools taught at the course in your applied work? &lt;/b&gt;&lt;/label&gt;&lt;br&gt;
Definitely - I will modify my workflow to include some or all of these &lt;input type=&#34;radio&#34; name=&#34;will_use_methods&#34; value=&#34;definitely&#34;&gt;&lt;br&gt;
May be - I will try and use these and convince my boss/colleagues of their usefulness &lt;input type=&#34;radio&#34; name=&#34;will_use_methods&#34; value=&#34;maybe&#34;&gt;&lt;br&gt;
Probably not - they are too complicated &lt;input type=&#34;radio&#34; name=&#34;will_use_methods&#34; value=&#34;probably not&#34;&gt;&lt;br&gt;
Definitely not - I don’t think they were useful at all &lt;input type=&#34;radio&#34; name=&#34;will_use_methods&#34; value=&#34;definitely not&#34;&gt;&lt;br&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments6&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt; 7. Do you think the period of the year in which the summer school was held was good? &lt;/b&gt;&lt;/label&gt;
&lt;ul class=&#34;likert&#34;&gt;
&lt;li&gt;
Not so much
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;period_good&#34; value=&#34;1&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;period_good&#34; value=&#34;2&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;period_good&#34; value=&#34;3&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;period_good&#34; value=&#34;4&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;period_good&#34; value=&#34;5&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
Excellent
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;&lt;br&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments7&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt; 8. Do you think the location of the course was good? &lt;/b&gt;&lt;/label&gt;
&lt;ul class=&#34;likert&#34;&gt;
&lt;li&gt;
Very poor
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;location_good&#34; value=&#34;1&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;location_good&#34; value=&#34;2&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;location_good&#34; value=&#34;3&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;location_good&#34; value=&#34;4&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;input type=&#34;radio&#34; name=&#34;location_good&#34; value=&#34;5&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;
Excellent
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;&lt;br&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments8&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt; 9. Would you recommend the course to colleagues? &lt;/b&gt;&lt;/label&gt;&lt;br&gt;
Definitely &lt;input type=&#34;radio&#34; name=&#34;recommend&#34; value=&#34;definitely&#34;&gt;&lt;br&gt;
May be &lt;input type=&#34;radio&#34; name=&#34;recommend&#34; value=&#34;maybe&#34;&gt;&lt;br&gt;
Probably not &lt;input type=&#34;radio&#34; name=&#34;recommend&#34; value=&#34;probably not&#34;&gt;&lt;br&gt;
Definitely not &lt;input type=&#34;radio&#34; name=&#34;recommend&#34; value=&#34;definitely not&#34;&gt;&lt;br&gt;
&lt;label&gt;&lt;i&gt;Additional comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;comments9&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;label&gt;&lt;b&gt;10. What did you think about the lecturers?&lt;/label&gt;
&lt;strong&gt;Gianluca Baio&lt;/strong&gt;&lt;br&gt;
&lt;!--
&lt;select name=&#34;gb[]&#34; multiple size=&#34;5&#34; style=&#34;width: 450px !important; min-width: 300px; max-width: 900px; margin-left: 10px;&#34;&gt;
      &lt;option value=&#34;knowledgeable&#34;&gt;a. Knowledgeable&lt;/option&gt;
      &lt;option value=&#34;friendly&#34;&gt;b. Friendly&lt;/option&gt;
      &lt;option value=&#34;helpful&#34;&gt;c. Helpful&lt;/option&gt;
      &lt;option value=&#34;neither&#34;&gt;d. None of the above&lt;/option&gt;
    &lt;/select&gt;
&lt;br&gt;
--&gt;
&lt;label&gt;&lt;i&gt;Comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;commentsgb&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;strong&gt;Anna Heath&lt;/strong&gt;&lt;br&gt;
&lt;!--
&lt;select name=&#34;ah[]&#34; multiple size=&#34;5&#34; style=&#34;width: 450px !important; min-width: 300px; max-width: 900px; margin-left: 10px;&#34;&gt;
      &lt;option value=&#34;knowledgeable&#34;&gt;a. Knowledgeable&lt;/option&gt;
      &lt;option value=&#34;friendly&#34;&gt;b. Friendly&lt;/option&gt;
      &lt;option value=&#34;helpful&#34;&gt;c. Helpful&lt;/option&gt;
      &lt;option value=&#34;neither&#34;&gt;d. None of the above&lt;/option&gt;
    &lt;/select&gt;
&lt;br&gt;
--&gt;
&lt;label&gt;&lt;i&gt;Comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;commentsah&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;strong&gt;Nathan Green&lt;/strong&gt;&lt;br&gt;
&lt;!--
&lt;select name=&#34;ng[]&#34; multiple size=&#34;5&#34; style=&#34;width: 450px !important; min-width: 300px; max-width: 900px; margin-left: 10px;&#34;&gt;
      &lt;option value=&#34;knowledgeable&#34;&gt;a. Knowledgeable&lt;/option&gt;
      &lt;option value=&#34;friendly&#34;&gt;b. Friendly&lt;/option&gt;
      &lt;option value=&#34;helpful&#34;&gt;c. Helpful&lt;/option&gt;
      &lt;option value=&#34;neither&#34;&gt;d. None of the above&lt;/option&gt;
    &lt;/select&gt;
&lt;br&gt;&lt;br&gt; 
--&gt;
&lt;label&gt;&lt;i&gt;Comments&lt;/i&gt;&lt;/label&gt;&lt;br&gt;
&lt;textarea name=&#34;commentsng&#34; cols=&#34;40&#34; rows=&#34;4&#34;&gt;&lt;/textarea&gt;
&lt;p style=&#34;height: 10px;&#34;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;label&gt;&lt;b&gt;11. Any other comment?&lt;/b&gt;:&lt;/label&gt;
&lt;textarea name=&#34;comments&#34; cols=&#34;60&#34; rows=&#34;6&#34;&gt;&lt;/textarea&gt;
&lt;br&gt;&lt;br&gt;
&lt;p&gt;
&lt;button type=&#34;submit&#34;&gt;
Submit
&lt;/button&gt;
&lt;/p&gt;
&lt;/form&gt;
</description>
    </item>
    
    <item>
      <title>Daily schedule</title>
      <link>/daily-schedule/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      <guid>/daily-schedule/</guid>
      <description> 



&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Day 1: Monday, 20 June 2022&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Start &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; End &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Topic &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Lecturer &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Room &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 1: Introduction to Bayesian reasoning, computation and BUGS &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/gianluca-baio&#34;&gt;Gianluca Baio&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 11:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 1. Monte Carlo in BUGS &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 2: Learning from data using MCMC and BUGS &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/nathan-green&#34;&gt;Nathan Green&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 14:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Lunch &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 14:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 2. MCMC in BUGS &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 15:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 15:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 16:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 3: Introduction to health economic evaluation &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/gianluca-baio&#34;&gt;Gianluca Baio&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 16:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 17:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 3. Introduction to R and cost-effectiveness analysis using BCEA &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Day 2: Tuesday, 21 June 2022&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Start &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; End &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Topic &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Lecturer &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Room &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 09:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 4: Individual level data in health economics &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/anna-heath&#34;&gt;Anna Heath&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 4. Cost-effectiveness analysis with individual-level data &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 11:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 12:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 5: Aggregated level data &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/nathan-green&#34;&gt;Nathan Green&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 12:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 5. Evidence synthesis and decision models &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 13:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 14:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Lunch &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 14:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 6: Evidence synthesis and network meta-analysis &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/gianluca-baio&#34;&gt;Gianluca Baio&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 15:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 15:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 16:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 6. Network meta-analysis &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Day 3: Wednesday, 22 June 2022&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Start &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; End &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Topic &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Lecturer &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Room &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 09:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 7: Model error and structural analysis &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/gianluca-baio&#34;&gt;Gianluca Baio&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 7. PSA to structural uncertainty &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 10:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 8: Survival analysis in HTA &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/gianluca-baio&#34;&gt;Gianluca Baio&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Lunch &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 14:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 8. Survival analysis &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 14:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 14:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 14:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 9: Markov models &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/nathan-green&#34;&gt;Nathan Green&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 16:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 9. Markov models &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-126 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Day 4: Thursday, 23 June 2022&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Start &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; End &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Topic &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Lecturer &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Room &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 09:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 10: Missing data in cost-effectiveness modelling &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/nathan-green&#34;&gt;Nathan Green&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-110 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 10. Missing data &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-110 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 10:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 11: Introduction to Value of Information &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/anna-heath&#34;&gt;Anna Heath&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-110 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Lunch &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 11. Computing the EVPI using nested Monte Carlo simulations &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-110 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 14:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 12: Expected value of partial information &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/anna-heath&#34;&gt;Anna Heath&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-110 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 14:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 15:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 16:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 12. Computing the EVPPI in BCEA and SAVI &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-110 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Day 5: Friday, 24 June 2022&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Start &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; End &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Topic &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Lecturer &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Room &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 09:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 09:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 13: Expected value of sample information &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/gianluca-baio&#34;&gt;Gianluca Baio&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-109 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 09:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 14: Generating data for the analysis of the EVSI &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/anna-heath&#34;&gt;Anna Heath&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-109 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 10:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 10:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Coffee break &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 10:30 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 13. Generating data for EVSI &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-109 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 11:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 15: Calculating expected value of sample information &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/anna-heath&#34;&gt;Anna Heath&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-109 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 12:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt; Lunch &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;font-style: italic;&#34;&gt;  &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 14. Computing the EVSI using Monte Carlo simulations &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-109 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 13:45 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 14:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Lecture 16: Regression-based EVSI &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; &lt;a href=&#34;../author/anna-heath&#34;&gt;Anna Heath&lt;/a&gt; &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-109 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 14:15 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 15:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Practical 15. Computing EVSI using regression &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt;  &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Extranef-109 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/p&gt;
&lt;/details&gt;
</description>
    </item>
    
    <item>
      <title>Computer specification</title>
      <link>/tips/computer-specification/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/tips/computer-specification/</guid>
      <description>&lt;h2 id=&#34;on-your-own-machine&#34;&gt;On your own machine&lt;/h2&gt;
&lt;p&gt;Since both &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;BUGS&lt;/code&gt; (or &lt;code&gt;JAGS&lt;/code&gt;, which is an alternative Bayesian software) are free, you can install them on your own laptop/desktop, if you want.&lt;/p&gt;
&lt;p&gt;The following gives you a quick guide on how you install the software that is required for the analyses that we will go through in the practicals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;JAGS&lt;/code&gt; (optional)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;R&lt;/code&gt; and specifically the packages &lt;code&gt;R2OpenBUGS&lt;/code&gt; and &lt;code&gt;BCEA&lt;/code&gt;. Other optional packages (e.g. &lt;code&gt;reshape&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;INLA&lt;/code&gt; and &lt;code&gt;R2jags&lt;/code&gt;) may need to be installed&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;R&lt;/code&gt; front-end &lt;code&gt;Rstudio&lt;/code&gt; (optional)&lt;/li&gt;
&lt;li&gt;A spreadsheet calculator; if you have a valid license for &lt;code&gt;MS Excel&lt;/code&gt; on your machine that is OK. If not, you can download the freely available &lt;code&gt;LibreOffice&lt;/code&gt;, which is a decent surrogate (at least for the tasks that we will require in the practicals).&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The file &lt;a href=&#34;install.R&#34;&gt;&lt;code&gt;install.R&lt;/code&gt;&lt;/a&gt; can be used to install &lt;em&gt;&lt;strong&gt;all&lt;/strong&gt;&lt;/em&gt; the necessary packages (in line with the specification of the &lt;a href=&#34;#virtual-machine&#34;&gt;Binder Virtual Machine&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;You can also download a &lt;code&gt;.zip&lt;/code&gt; file with the code and scripts for &lt;strong&gt;all&lt;/strong&gt; the practicals, &lt;a href=&#34;practicals.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Notice that you don&amp;rsquo;t have to install both &lt;code&gt;OpenBUGS&lt;/code&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;code&gt;JAGS&lt;/code&gt; &amp;mdash; the former is sufficient for the purposes of this course.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;specific-installation-instructions&#34;&gt;Specific installation instructions&lt;/h3&gt;
&lt;div class=&#34;alert alert-help&#34;&gt;
  &lt;div&gt;
    &lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;MS Windows users&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;If you are a &lt;code&gt;Windows&lt;/code&gt; user, your setting should be fairly easy.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the file &lt;code&gt;OpenBUGS323setup.exe&lt;/code&gt; from the webpage &lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2014/07/OpenBUGS323setup.zip&#34;&gt;https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2014/07/OpenBUGS323setup.zip&lt;/a&gt;, extract the &lt;code&gt;exe&lt;/code&gt; file from the downloaded &lt;code&gt;zip&lt;/code&gt; file and run it by double-clicking on it.&lt;/li&gt;
&lt;li&gt;Download &lt;code&gt;R&lt;/code&gt; from &lt;a href=&#34;http://cran.r-project.org/bin/windows/&#34;&gt;http://cran.r-project.org/bin/windows/&lt;/a&gt; (click on the link &amp;ldquo;install R for the first time&amp;rdquo;&amp;quot;).&lt;/li&gt;
&lt;li&gt;Once you have installed &lt;code&gt;R&lt;/code&gt;, open it and type in the terminal the command &lt;code&gt;install.packages(&amp;quot;R2OpenBUGS&amp;quot;)&lt;/code&gt;. This command will download the package &lt;code&gt;R2OpenBUGS&lt;/code&gt;, which is needed to interface &lt;code&gt;OpenBUGS&lt;/code&gt; with &lt;code&gt;R&lt;/code&gt;. Follow the on-screen instructions (you will be asked to select a &amp;ldquo;mirror from which to obtain the file).&lt;/li&gt;
&lt;li&gt;Make sure that the download has worked by typing in the terminal the command &lt;code&gt;library(R2OpenBUGS)&lt;/code&gt;. If you do not see any error message, then the package has been successfully installed.&lt;/li&gt;
&lt;li&gt;Install the &lt;code&gt;BCEA&lt;/code&gt; package that we will use throughout the practicals, by typing &lt;code&gt;install.packages(&amp;quot;BCEA&amp;quot;)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Repeat the installation process for the other packages that are used in the practicals (e.g. &lt;code&gt;reshape&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt;). Since this is optional, you can leave this final step to when it is actually needed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you like, you can also install &lt;code&gt;JAGS&lt;/code&gt;, following these steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;a href=&#34;http://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Windows/&#34;&gt;http://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Windows/&lt;/a&gt; and click on the latest available executable file (currently, &lt;a href=&#34;https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Windows/JAGS-4.3.0.exe/download&#34;&gt;&lt;code&gt;JAGS-4.3.0.exe&lt;/code&gt;&lt;/a&gt;). Running this file will install &lt;code&gt;JAGS&lt;/code&gt; on your machine.&lt;/li&gt;
&lt;li&gt;If you do so, then in the &lt;code&gt;R&lt;/code&gt; terminal type the command &lt;code&gt;install.packages(&amp;quot;R2jags&amp;quot;)&lt;/code&gt;. This will allow you to use &lt;code&gt;JAGS&lt;/code&gt; (instead of &lt;code&gt;OpenBUGS&lt;/code&gt;) when doing the practicals &amp;mdash; notice that to make a package available you will have to load it to the workspace by using the &lt;code&gt;library(name_package)&lt;/code&gt; command.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;/details&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;div class=&#34;alert alert-help&#34;&gt;
  &lt;div&gt;
    &lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;&lt;strong&gt;Linux or Mac OS users&lt;/strong&gt;&lt;/summary&gt;
  &lt;p&gt;&lt;h3 id=&#34;installing-r-and-bcea&#34;&gt;Installing &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;BCEA&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Linux&lt;/code&gt; or &lt;code&gt;Mac OS&lt;/code&gt; users should follow slightly different approaches. The installation of &lt;code&gt;R&lt;/code&gt; is pretty much the same as for &lt;code&gt;MS WIndows&lt;/code&gt; users. From the webpage &lt;a href=&#34;http://cran.r-project.org/&#34;&gt;http://cran.r-project.org/&lt;/a&gt; select your operating system (&lt;code&gt;Linux&lt;/code&gt; or &lt;code&gt;Mac OS&lt;/code&gt;) and then your version (eg &lt;code&gt;debian&lt;/code&gt;, &lt;code&gt;redhat&lt;/code&gt;, &lt;code&gt;suse&lt;/code&gt; or &lt;code&gt;ubuntu&lt;/code&gt;, for &lt;code&gt;Linux&lt;/code&gt;). Follow the instructions to install the software. Once this is done, open &lt;code&gt;R&lt;/code&gt; and install the package &lt;code&gt;BCEA&lt;/code&gt; by typing at the terminal the command &lt;code&gt;install.packages(&amp;quot;BCEA&amp;quot;)&lt;/code&gt;. You can use similar commands to install other packages.&lt;/p&gt;
&lt;h3 id=&#34;installing-openbugs-and-jags-in-linux&#34;&gt;Installing &lt;code&gt;OpenBUGS&lt;/code&gt; and &lt;code&gt;JAGS&lt;/code&gt; in &lt;code&gt;Linux&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;While both &lt;code&gt;OpenBUGS&lt;/code&gt; and &lt;code&gt;JAGS&lt;/code&gt; run natively in &lt;code&gt;Linux&lt;/code&gt; (see below for details on how to install them directly), the graphical interface is not available for &lt;code&gt;OpenBUGS&lt;/code&gt;. Because we will use it for at least the first few practicals, it is advisable to install &lt;code&gt;wine&lt;/code&gt;, a &amp;ldquo;compatibility layer&amp;rdquo;&amp;quot; that allows to run &lt;code&gt;Windows&lt;/code&gt; applications from &lt;code&gt;Linux&lt;/code&gt; or &lt;code&gt;Mac&lt;/code&gt;. Instructions are available at &lt;a href=&#34;https://www.winehq.org/download/&#34;&gt;https://www.winehq.org/download/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you have installed &lt;code&gt;wine&lt;/code&gt;, you can also install &lt;code&gt;OpenBUGS&lt;/code&gt;, which you will be then able to access using the graphical interface, by following these steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to the webpage &lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2014/07/OpenBUGS323setup.zip&#34;&gt;https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2014/07/OpenBUGS323setup.zip&lt;/a&gt;, extract from the &lt;code&gt;.zip&lt;/code&gt; file the windows executable file &lt;code&gt;OpenBUGS323setup.exe&lt;/code&gt; and place it in your default directory for &lt;code&gt;wine&lt;/code&gt; programs (usually &lt;code&gt;~/.wine/drive_c&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;OpenBUGS&lt;/code&gt; installer
&lt;ul&gt;
&lt;li&gt;Open a terminal window;&lt;/li&gt;
&lt;li&gt;Move to the directory where you placed the &lt;code&gt;OpenBUGS&lt;/code&gt; executable you downloaded in step 1, something like: &lt;code&gt;cd ~/.wine/drive_c&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Type: &lt;code&gt;wine OpenBUGS323setup.exe&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Wait for a while and then follow the prompts to install &amp;mdash; by default, the installation folder is &lt;code&gt;~/.wine/drive_c/Program Files/OpenBUGS323&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;NB: There may be an error at the end, this is OK. Close down the Terminal Window&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Test &lt;code&gt;OpenBUGS&lt;/code&gt; by opening a new terminal window typing the command: &lt;code&gt;wine ~/.wine/drive_c/Program Files/OpenBUGS323/OpenBUGS.exe&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Download the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package from &lt;a href=&#34;http://www.openbugs.info/w/UserContributedCode&#34;&gt;http://www.openbugs.info/w/UserContributedCode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;R&lt;/code&gt; and install &lt;code&gt;R2OpneBUGS&lt;/code&gt; by typing the command &lt;code&gt;install.packages(&amp;quot;R2OpenBUGS&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can also install the &lt;code&gt;Linux&lt;/code&gt; version of &lt;code&gt;OpenBUGS&lt;/code&gt; (available from &lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2018/04/OpenBUGS-3.2.3.tar.gz&#34;&gt;here&lt;/a&gt;) by following the instructions given at &lt;a href=&#34;https://www.mrc-bsu.cam.ac.uk/software/bugs/openbugs/building-and-packaging-openbugs/openbugs-linux-installation/&#34;&gt;https://www.mrc-bsu.cam.ac.uk/software/bugs/openbugs/building-and-packaging-openbugs/openbugs-linux-installation/&lt;/a&gt;. This will work just as well, but you won&amp;rsquo;t be able to access the graphical interface.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt;: Under &lt;code&gt;Linux&lt;/code&gt;, you may need to also install additional packages to support the &lt;code&gt;OpenBUGS&lt;/code&gt; installation. For instance, under &lt;code&gt;Debian&lt;/code&gt; or &lt;code&gt;Ubuntu&lt;/code&gt;, you may need to also run in your terminal&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install g++-multilib
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to install the library &lt;code&gt;g++multilib&lt;/code&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In addition, you can also install &lt;code&gt;JAGS&lt;/code&gt;, following these steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;a href=&#34;http://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/&#34;&gt;http://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/&lt;/a&gt; and download the latest &lt;code&gt;tar.gz&lt;/code&gt; file (currently, &lt;a href=&#34;https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.0.tar.gz/download&#34;&gt;&lt;code&gt;JAGS-4.3.0.tar.gz&lt;/code&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Open a terminal window and extract the content of the archive file by typing the command &lt;code&gt;tar xzvf JAGS-4.3.0.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Move to the directory (which has just been created) using the command &lt;code&gt;cd JAGS-4.3.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run the configuration by typing &lt;code&gt;sudo ./configure --prefix=/us &amp;amp;&amp;amp; sudo make &amp;amp;&amp;amp; sudo make install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Clean up the unnecessary files and folder by typing &lt;code&gt;cd .. &amp;amp;&amp;amp; sudo rm -fr JAGS-4.3.0 &amp;amp;&amp;amp; rm JAGS-4.3.0.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you decide to do so, then open &lt;code&gt;R&lt;/code&gt; and install the package &lt;code&gt;R2JAGS&lt;/code&gt; (using the same command as for the installation of the package &lt;code&gt;R2OpneBUGS&lt;/code&gt;). Notice that you don&amp;rsquo;t have to install both &lt;code&gt;OpenBUGS&lt;/code&gt; and &lt;code&gt;JAGS&lt;/code&gt; &amp;mdash; the former is sufficient for the purposes of this course.&lt;/p&gt;
&lt;h3 id=&#34;installing-openbugs-and-jags-in-mac-os&#34;&gt;Installing &lt;code&gt;OpenBUGS&lt;/code&gt; and &lt;code&gt;JAGS&lt;/code&gt; in &lt;code&gt;Mac OS&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;While similar in spirit, installation under &lt;code&gt;Mac OS&lt;/code&gt; is slightly more complex, because the process requires some extra software that is not automatically installed. Basically, you need to follow this procedure, as detailed at the website &lt;a href=&#34;http://www.jkarreth.net/bayes-icpsr.html#bugsmac&#34;&gt;http://www.jkarreth.net/bayes-icpsr.html#bugsmac&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Update your &lt;code&gt;Mac OS&lt;/code&gt; to the newest version.&lt;/li&gt;
&lt;li&gt;Install &lt;code&gt;Xcode&lt;/code&gt; through the &lt;a href=&#34;https://apps.apple.com/us/app/xcode/id497799835?mt=12&#34;&gt;App Store&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Check if you have &lt;code&gt;X11&lt;/code&gt; installed (this is a a windowing system, common on &lt;code&gt;Unix&lt;/code&gt;-like operating systems, which, believe it or not, &lt;code&gt;MaxOs&lt;/code&gt; is!): hit Command-Space, type &lt;code&gt;X11&lt;/code&gt;, and see if the program shows up. If not, install it from &lt;a href=&#34;http://xquartz.macosforge.org/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Download a the stable pre-compiled version of &lt;a href=&#34;https://dl.winehq.org/wine-builds/macosx/download.html&#34;&gt;&lt;code&gt;wine&lt;/code&gt;&lt;/a&gt;. Instructions to install &lt;code&gt;wine&lt;/code&gt; on &lt;code&gt;Mac OS&lt;/code&gt; are available &lt;a href=&#34;https://www.embird.net/sw/embird/tutorial/wine/wine.htm&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Download &lt;code&gt;OpenBUGS323setup.exe&lt;/code&gt; (windows executable) and place it in your default directory for &lt;code&gt;wine&lt;/code&gt; programs (usually &lt;code&gt;~/.wine/drive_c&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;OpenBUGS&lt;/code&gt; installer:
&lt;ul&gt;
&lt;li&gt;Open &lt;code&gt;XQuartz&lt;/code&gt; and a Terminal Window;&lt;/li&gt;
&lt;li&gt;Move to the directory where you placed the &lt;code&gt;OpenBUGS&lt;/code&gt; executable you downloaded in step 2;&lt;/li&gt;
&lt;li&gt;Type: &lt;code&gt;wine OpenBUGS323setup.exe&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Wait for a while and then follow the prompts to install &amp;mdash; remember the directory you created to install it (default is &lt;code&gt;~/[username]/.wine/drive_c/Program Files/OpenBUGS323&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;NB: There may be an error at the end, this is OK. Close down the Terminal Window&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It is possible that you need to specify the installation directory to tell your system specifically where to look for &lt;code&gt;BUGS&lt;/code&gt;. Typically this will mean adding the option &lt;code&gt;bugs.directory = &amp;quot;/Users/yourusername/.wine/drive_c/Program Files/OpenBUGS232&amp;quot;&lt;/code&gt; (or similar, depending on where you have installed the software!) to the call to the &lt;code&gt;bugs&lt;/code&gt; function under &lt;code&gt;R2OpenBUGS&lt;/code&gt;. Note that you need to replace &amp;ldquo;yourusername&amp;rdquo; with your Mac&amp;rsquo;s user name.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that, if you like, you can install &lt;code&gt;R&lt;/code&gt; under &lt;code&gt;wine&lt;/code&gt; (rather than natively in &lt;code&gt;Mac OS&lt;/code&gt;).  Download the &lt;code&gt;MS Windows&lt;/code&gt; executable file from &lt;code&gt;CRAN&lt;/code&gt; and repeat the instructions above, replacing the command &lt;code&gt;wine OpenBUGS323setup.exe&lt;/code&gt; with the command &lt;code&gt;wine R-XXXX.exe&lt;/code&gt;, where &lt;code&gt;R-XXXX.exe&lt;/code&gt; is the name of the executable file.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;There are some reports that &lt;code&gt;OpenBUGS&lt;/code&gt; may fail to work on some &lt;code&gt;Mac OS&lt;/code&gt; versions. Sometimes, when trying to use &lt;code&gt;OpenBUGS&lt;/code&gt; from &lt;code&gt;R&lt;/code&gt;, it will complain that it can&amp;rsquo;t find the programme. The &lt;code&gt;bugs&lt;/code&gt; function in the &lt;code&gt;R2OpenBUGS&lt;/code&gt; package takes an additional input &lt;code&gt;OpenBUGS.pgm&lt;/code&gt;, which should be set to the full path to the &lt;code&gt;OpenBUGS&lt;/code&gt; executable file. You can try and issue the &lt;code&gt;R&lt;/code&gt; command &lt;code&gt;Sys.which(&amp;quot;OpenBUGS&amp;quot;)&lt;/code&gt; at a &lt;code&gt;R&lt;/code&gt; terminal and see whether it returns a full path and then pass &lt;em&gt;that&lt;/em&gt; string as value for &lt;code&gt;OpenBUGS.pgm&lt;/code&gt;, eg if &lt;code&gt;Sys.which(&amp;quot;OpenBUGS&amp;quot;)&lt;/code&gt; returns the string &lt;code&gt;/usr/local/bin/OpenBUGS&lt;/code&gt;, then set&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;bugs(..., OpenBUGS.pgm=&amp;quot;/usr/local/bin/OpenBUGS&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In addition, you can also install &lt;code&gt;JAGS&lt;/code&gt;, following these steps, as detailed at the webpage &lt;a href=&#34;http://www.jkarreth.net/bayes-icpsr.html#jagsmac&#34;&gt;http://www.jkarreth.net/bayes-icpsr.html#jagsmac&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install the most recent version of &lt;code&gt;R&lt;/code&gt; from the &lt;a href=&#34;https://cran.r-project.org/bin/macosx/&#34;&gt;&lt;code&gt;CRAN&lt;/code&gt; website&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;(Optional) Download and install &lt;a href=&#34;https://rstudio.com/products/rstudio/download/#download&#34;&gt;RStudio&lt;/a&gt; (NB: select the version for the your release of &lt;code&gt;MacOS&lt;/code&gt;!).&lt;/li&gt;
&lt;li&gt;Install &lt;code&gt;Clang&lt;/code&gt; (currently &lt;code&gt;clang-8.0.0.pkg&lt;/code&gt;) and GNU Fortran (currently, &lt;code&gt;gfortran-6.1.pkg.dmg&lt;/code&gt;) from the &lt;a href=&#34;http://cran.r-project.org/bin/macosx/tools/&#34;&gt;CRAN tools directory&lt;/a&gt;. Note that the most updated release for these may vary so check you select the correct one.&lt;/li&gt;
&lt;li&gt;Now install &lt;code&gt;JAGS&lt;/code&gt; version 4.3.0 (&lt;code&gt;JAGS-4.3.0.dmg&lt;/code&gt;) from &lt;a href=&#34;http://sourceforge.net/projects/mcmc-jags/files/JAGS/&#34;&gt;here&lt;/a&gt;. Detailed instructions quoted from the &lt;code&gt;JAGS&lt;/code&gt; readme file:
&lt;ul&gt;
&lt;li&gt;Download the disk image from the &lt;code&gt;JAGS&lt;/code&gt; website.&lt;/li&gt;
&lt;li&gt;Double click on the disk image to mount (this may not be required).&lt;/li&gt;
&lt;li&gt;Double click on the &lt;code&gt;JAGS-4.3.0.mpkg&lt;/code&gt; file within the mounted disk image.&lt;/li&gt;
&lt;li&gt;Follow the instructions in the installer. If you receive a warning that this software cannot be installed because it comes from an unidentified developer, you need to go to &amp;ldquo;System Preferences&amp;rdquo;&amp;quot; &amp;gt; &amp;ldquo;Security &amp;amp; Privacy&amp;rdquo;&amp;quot;, and authorize the installation there before proceeding.&lt;/li&gt;
&lt;li&gt;Authenticate as the administrative user. The first user you create when setting up &lt;code&gt;Mac OS X&lt;/code&gt; has administrator privileges by default.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Start the Terminal and type &lt;code&gt;jags&lt;/code&gt; to see if you receive the message: Welcome to &lt;code&gt;JAGS 4.3.0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;R&lt;/code&gt; and install the packages &lt;code&gt;R2jags&lt;/code&gt;, &lt;code&gt;coda&lt;/code&gt;, &lt;code&gt;R2WinBUGS&lt;/code&gt;, &lt;code&gt;lattice&lt;/code&gt;, and &lt;code&gt;rjags&lt;/code&gt;, by typing &lt;code&gt;install.packages(c(&amp;quot;R2jags&amp;quot;, &amp;quot;coda&amp;quot;, &amp;quot;R2WinBUGS&amp;quot;, &amp;quot;lattice&amp;quot;, &amp;quot;rjags&amp;quot;))&lt;/code&gt; in the &lt;code&gt;R&lt;/code&gt; command line.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;NB&lt;/strong&gt;: &lt;a href=&#34;https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/07e08a3605/?limit=25#1160&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This&lt;/a&gt; post may be helpful in finding instructions to install &lt;code&gt;JAGS&lt;/code&gt; under &lt;code&gt;MacOs&lt;/code&gt;. In general, the &lt;a href=&#34;https://sourceforge.net/projects/mcmc-jags/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;JAGS&lt;/code&gt;&lt;/a&gt; &lt;code&gt;sourceforge&lt;/code&gt; page has several posts/requests for help that you may find very similar to your own.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/details&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h2 id=&#34;virtual-machine&#34;&gt;Virtual machine&lt;/h2&gt;
&lt;p&gt;It is of course helpful to set up your own machine with all the relevant software, so you can re-use it later in life. However, you can also use a virtual machine (VM) we have created for you (specifically using &lt;a href=&#34;https://mybinder.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binder&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We have set up a VM (if you are interested, it is based on Linux), in which we have installed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A stable version of &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Rstudio&lt;/code&gt;. Specifically, the current installation uses &lt;code&gt;R 4.1.2 (2021-11-01)&lt;/code&gt; and &lt;code&gt;Rstudio 2022.02.0 Build 443&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;OpenBUGS 3.2.3;&lt;/li&gt;
&lt;li&gt;JAGS 4.3.0;&lt;/li&gt;
&lt;li&gt;The packages &lt;code&gt;R2jags&lt;/code&gt;, &lt;code&gt;R2OpenBUGS&lt;/code&gt;, &lt;code&gt;BCEA&lt;/code&gt;, &lt;code&gt;tidyverse&lt;/code&gt; and all the relevant &amp;ldquo;dependencies&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;The package &lt;code&gt;bmhe&lt;/code&gt;, which contains a set of utility functions to be used specifically for the practicals.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;If you want to install the package &lt;code&gt;bmhe&lt;/code&gt; so you can use it on your local machine, you need to type the following command on your &lt;code&gt;R&lt;/code&gt; terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;install.packages(&amp;quot;remotes&amp;quot;)
remotes::install_github(&amp;quot;giabaio/bmhe_utils&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will first install the package &lt;code&gt;remotes&lt;/code&gt; from the main &lt;code&gt;R&lt;/code&gt; repository (CRAN) and then use it to install &lt;code&gt;bmhe&lt;/code&gt; from its &lt;a href=&#34;https://github.com/giabaio/bmhe_utils&#34;&gt;GitHub repository&lt;/a&gt;. Installing packages from a GitHub repository is becoming a very popular and at times very efficient alternative, so it&amp;rsquo;s helpful to know how to do it&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; The full list of packages installed in the binder VM is given in the file &lt;code&gt;install.R&lt;/code&gt; that is available under the root (the main, landing folder: this is your working directory when you switch the binder VM on).&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This means that when you access the VM, you won&amp;rsquo;t have to fiddle with the setting and will be able to use it for the practicals without further issues with installation.&lt;/p&gt;
&lt;p&gt;To use the binder VM, simply go to the &lt;a href=&#34;https://github.com/StatisticsHealthEconomics/stat0019_binder/tree/bmhe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt; (note that this is the &amp;ldquo;branch&amp;rdquo; named &lt;code&gt;bmhe&lt;/code&gt; and this is the one you need to use &amp;mdash; not the other branches!) and click on the button in the bottom-left corner (marked with the text &amp;ldquo;launch binder&amp;rdquo;). This will open the &amp;ldquo;BinderHub&amp;rdquo; allowing us to share the GitHub repository in the form of a remote &lt;code&gt;Rstudio&lt;/code&gt; environment, with all the relevant files already installing. Alternatively, simply direct your browser to this &lt;a href=&#34;https://mybinder.org/v2/gh/StatisticsHealthEconomics/stat0019_binder/bmhe?urlpath=rstudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;URL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This should load up fairly quickly (as it&amp;rsquo;s been compiled already &amp;mdash; it is possible, though unlikely, that it may need to recompile the binder VM, but even then, it should not take more than a minute or so&amp;hellip;). Once it&amp;rsquo;s done, your browser will have a &lt;code&gt;Rstudio&lt;/code&gt; instance open.&lt;/p&gt;
&lt;p&gt;You can use the bottom-right corner window as a file-navigator. The tab labelled as &lt;code&gt;Files&lt;/code&gt; will show the file system for the VM. You don&amp;rsquo;t need to worry about all the files and can click to &lt;code&gt;practical&lt;/code&gt;, which will navigate to the folder containing the files for the various practicals. If you keep navigating the files, you can reach the relevant path and open the various &lt;code&gt;R&lt;/code&gt; scripts, for instance moving to &lt;code&gt;practical/01_monte-carlo/&lt;/code&gt; shows four &lt;code&gt;.R&lt;/code&gt; files that can be used to do the first practical (effectively only using &lt;code&gt;R&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;The only drawback of using the binder VM is that &lt;strong&gt;for now&lt;/strong&gt; it is not &amp;ldquo;&lt;em&gt;persistent&lt;/em&gt;&amp;rdquo;. This means that the changes you make to the files are not saved from one session to the next (a &lt;a href=&#34;https://notebooks.gesis.org/&#34;&gt;tool&lt;/a&gt; existed to turn the binder into a persistent VM, but it has been temporarily discontinued).&lt;/p&gt;
&lt;p&gt;In addition, it can also be a bit slow (it is after all a remote machine&amp;hellip;), although this obviously depends on how fast/powerful your computer is and you need to be online while working on &lt;code&gt;Rstudio&lt;/code&gt;, which you don&amp;rsquo;t on a local installation.&lt;/p&gt;
&lt;p&gt;For these reasons, I probably would recommend &lt;em&gt;against&lt;/em&gt; using it, if you were doing &amp;ldquo;serious&amp;rdquo; work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But&lt;/strong&gt;: it may be very helpful in terms of getting on the right track without having to worry about the subtleties of the software installation. In addition, tools such as &lt;a href=&#34;https://docs.docker.com/&#34;&gt;&lt;code&gt;docker&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://mybinder.readthedocs.io/en/latest/&#34;&gt;&lt;code&gt;binder&lt;/code&gt;&lt;/a&gt; are generally very helpful to share your work and guarantee reproducible research, so it is probably worth spending a little time investigating how they work&amp;hellip;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h2 id=&#34;helpful-tools&#34;&gt;Helpful tools&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s a list of some other helpful tools for (Bayesian) modelling in HTA applications&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.jeremy-oakley.staff.shef.ac.uk/shelf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SHELF&lt;/a&gt;: a package of documents, templates and software to carry out elicitation of probability distributions for uncertain quantities from a group of experts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://savi.shef.ac.uk/SAVI/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAVI&lt;/a&gt;: a web-app to do Value of Information computations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://egon.stats.ucl.ac.uk/projects/BCEAweb/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BCEAweb&lt;/a&gt;: a web-app to run &lt;code&gt;BCEA&lt;/code&gt; with PSA and VoI applications&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Evidence Synthesis Series</title>
      <link>/publication/nice-dsu-es/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/nice-dsu-es/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Partitioned Survival Analysis</title>
      <link>/publication/nice-dsu-partition-survival/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/nice-dsu-partition-survival/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Survival Analysis</title>
      <link>/publication/nice-dsu-survival/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/nice-dsu-survival/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Doing Meta-Analysis in R</title>
      <link>/publication/harrer-etal-2020/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/harrer-etal-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction to statistical concepts</title>
      <link>/publication/baio-notes/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/publication/baio-notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>survHE: Survival analysis for health economic evaluation and cost-effectiveness modelling</title>
      <link>/publication/baio-2019/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/publication/baio-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using BUGS</title>
      <link>/tips/using-bugs/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/tips/using-bugs/</guid>
      <description>


&lt;div id=&#34;running-a-model-in-openbugs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running a model in &lt;code&gt;OpenBUGS&lt;/code&gt;&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Start &lt;code&gt;OpenBUGS&lt;/code&gt; by double clicking on the &lt;code&gt;OpenBUGS&lt;/code&gt; icon (or double click on the file &lt;code&gt;OpenBUGS.exe&lt;/code&gt;, typically in somewhere like the &lt;code&gt;OpenBUGS\OpenBUGS322&lt;/code&gt; directory in &lt;code&gt;C:\Program Files (x86)&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the file containing model code as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Point to &lt;code&gt;File&lt;/code&gt; on the tool bar and click once with left mouse button (LMB);&lt;/li&gt;
&lt;li&gt;Highlight &lt;code&gt;Open&lt;/code&gt; option and click once with LMB;&lt;/li&gt;
&lt;li&gt;Select appropriate directory and double click on file to open. Files for &lt;code&gt;OpenBUGS&lt;/code&gt; input are sometimes in &lt;code&gt;.txt&lt;/code&gt; plain text format rather than the default &lt;code&gt;.odc&lt;/code&gt; “compound document” format — so you may need to select “Files of type” &lt;code&gt;.txt&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the &lt;code&gt;Model&lt;/code&gt; menu as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Point to &lt;code&gt;Model&lt;/code&gt; on the tool bar and click once with LMB.&lt;/li&gt;
&lt;li&gt;Highlight &lt;code&gt;Specification&lt;/code&gt; option and click once with LMB.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Focus the window containing the model code by clicking the LMB once anywhere in the window — the top panel of the window should then become highlighted in blue to indicate that the window is currently in focus.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Highlight the word &lt;code&gt;model&lt;/code&gt; at the beginning of the code by dragging the mouse over the word whilst holding down the LMB.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check the model syntax by moving the mouse over the check model box in the &lt;code&gt;Specification Tool&lt;/code&gt; window and clicking once with the LMB.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A message saying&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;model is syntactically correct&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;should appear in the bottom left of the &lt;code&gt;OpenBUGS&lt;/code&gt; program window. Any error messages will appear in the same place if there is a syntax error.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the data, if there are observed data in the model. The data can either be stored in a separate file, in which case open this file (or multiple files), or they may be stored in the same file as the model code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Load the data as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Highlight the word &lt;code&gt;list&lt;/code&gt; at the beginning of the data file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click once with the LMB on the load data box in the &lt;code&gt;Specification Tool&lt;/code&gt; window.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A message saying&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;data loaded&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;should appear in the bottom left of the &lt;code&gt;OpenBUGS&lt;/code&gt; program window.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select number of chains (sets of samples to simulate) by typing the number of chains required in the white box in the &lt;code&gt;Specification Tool&lt;/code&gt; window.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The default is 1, but we will typically use 2 or more. Running more than one chain, starting from different initial values, makes convergence checking easier (see later).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compile the model by clicking once with the LMB on the &lt;code&gt;compile&lt;/code&gt; box in the &lt;code&gt;Specification Tool&lt;/code&gt; window.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A message saying&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;model compiled&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;should appear in the bottom left of the &lt;code&gt;OpenBUGS&lt;/code&gt; program window.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the initial values files. The initial values can either be stored in separate file(s), in which case open these files, or they may be stored in the same file as the model code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Load any initial values as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Highlight the word &lt;code&gt;list&lt;/code&gt; at the beginning of the first set of initial values.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click once with LMB on the load inits box in the &lt;code&gt;Specification Tool&lt;/code&gt; window.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A message saying&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;initial values loaded: model contains uninitialized nodes (try running gen inits or loading more files)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;should appear in the bottom left of the &lt;code&gt;OpenBUGS&lt;/code&gt; program window.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat process for the second set of initial values, if running two chains.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A message saying&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;initial values loaded: model initialized&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;should now appear in the bottom left of the &lt;code&gt;OpenBUGS&lt;/code&gt; program window.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sometimes we can get away with not supplying any initial values ourselves, and we can just click gen inits to let &lt;code&gt;OpenBUGS&lt;/code&gt; generate these automatically. Though this won’t generally work if any of the priors are vague. Also it is handy to start different chains at widely dispersed initial values to assess convergence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Close the &lt;code&gt;Specification Tool&lt;/code&gt; window by clicking once with LMB on the &lt;code&gt;X&lt;/code&gt; button in top right corner of window.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You are now ready to start running the simulation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before doing so, you may want to set some monitors to store the sampled values for selected parameters (see section Monitoring parameter values below).&lt;/li&gt;
&lt;li&gt;To run the simulation, select the &lt;code&gt;Update&lt;/code&gt; option from the &lt;code&gt;Model&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the number of updates (iterations of the simulation) you require in the appropriate white box (default is 1000).&lt;/li&gt;
&lt;li&gt;Click once on the &lt;code&gt;update&lt;/code&gt; box. The program will now start simulating values for each parameter in the model. This may take a few seconds — the box marked iteration will tell you how many updates have currently been completed. The number of times this value is revised depends on the value you have set for refresh (see white box above iteration). The default is every 100 iterations. If the model is very fast, you should increase this to, e.g. 1000 or 10000, then the model will run even faster since &lt;code&gt;OpenBUGS&lt;/code&gt; is not unnecessarily redrawing the screen hundreds of times in a split second. If the model is very slow, you may like to decrease it to, e.g. 10 or 1 so that &lt;code&gt;OpenBUGS&lt;/code&gt; does not appear to “freeze” during sampling.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the updates are finished, the message&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;updates took *** s&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;will appear in the bottom left of the &lt;code&gt;OpenBUGS&lt;/code&gt; program window (where &lt;code&gt;***&lt;/code&gt; is the number of seconds taking to complete the simulation).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you set monitors for any parameters you can now check convergence and view graphical and numerical summaries of the samples (see below).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To save any files created during your &lt;code&gt;OpenBUGS&lt;/code&gt; run, focus the window containing the information you want to save, and select the &lt;code&gt;Save As&lt;/code&gt; option from the &lt;code&gt;File&lt;/code&gt; menu.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To quit &lt;code&gt;OpenBUGS&lt;/code&gt;, select the &lt;code&gt;Exit&lt;/code&gt; option from the &lt;code&gt;File&lt;/code&gt; menu.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;monitoring-parameter-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monitoring parameter values&lt;/h2&gt;
&lt;p&gt;In order to check convergence and obtain posterior summaries of the model parameters, you first need to set monitors for each parameter of interest. This tells &lt;code&gt;OpenBUGS&lt;/code&gt; to store the values sampled for those parameters; otherwise, &lt;code&gt;OpenBUGS&lt;/code&gt; automatically discards the simulated values. There are
two types of monitors in &lt;code&gt;OpenBUGS&lt;/code&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sample monitors
&lt;ul&gt;
&lt;li&gt;Setting a sample monitor tells &lt;code&gt;OpenBUGS&lt;/code&gt; to store every value it simulates for that parameter.&lt;/li&gt;
&lt;li&gt;You will need to set sample monitors if you want to view trace plots of the samples to check convergence (see section Checking convergence below) and if you want to obtain posterior quantiles, for example, the posterior 95% Bayesian credible interval for that parameter.&lt;/li&gt;
&lt;li&gt;To set a sample monitor:
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;Samples&lt;/code&gt; from the &lt;code&gt;Inference&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the name of the parameter to be monitored in the white box marked node.&lt;/li&gt;
&lt;li&gt;Click once with the LMB on the box marked set&lt;/li&gt;
&lt;li&gt;Repeat for each parameter to be monitored.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Summary monitors
&lt;ul&gt;
&lt;li&gt;Setting a summary monitor tells &lt;code&gt;OpenBUGS&lt;/code&gt; to store the running mean and standard deviation for the parameter.&lt;/li&gt;
&lt;li&gt;The values saved contain less information than saving each individual sample in the simulation, but require much less storage. This is an important consideration when running long simulations (1000’s of iterations) and storing values for many variables.&lt;/li&gt;
&lt;li&gt;To set a summary monitor:
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;Summary&lt;/code&gt; from the &lt;code&gt;Inference&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the name of the parameter to be monitored in the white box marked node.&lt;/li&gt;
&lt;li&gt;Click once with the LMB on the box marked set&lt;/li&gt;
&lt;li&gt;Repeat for each parameter to be monitored.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Note&lt;/strong&gt;: you should not set a summary monitor until you are happy that convergence has been reached (see next section), since it is not possible to discard any of the pre-convergence (‘burn-in’) values from the running mean summary once it is set.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking convergence&lt;/h2&gt;
&lt;p&gt;Checking convergence requires considerable care. It is very difficult to say conclusively that a chain (simulation) has converged, only to diagnose when it definitely hasn’t converged. The following are practical guidelines for assessing convergence:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For models with many parameters, it is impractical to check convergence for every parameter, so just chose a random selection of relevant parameters to monitor.
&lt;ul&gt;
&lt;li&gt;For example, rather than checking convergence for every element of a vector of random effects, just chose a random subset (say, the first 5 or 10).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Examine trace plots of the sample values versus iteration to look for evidence of when the simulation appears to have stabilised:
&lt;ul&gt;
&lt;li&gt;To obtain ‘live’ trace plots for a parameter:
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;Samples&lt;/code&gt; from the &lt;code&gt;Inference&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the name of the parameter in the white box marked node.&lt;/li&gt;
&lt;li&gt;Click once with the LMB on the box marked trace: an empty graphics window will appear on screen.&lt;/li&gt;
&lt;li&gt;Repeat for each parameter required.&lt;/li&gt;
&lt;li&gt;Once you start running the simulations (using the &lt;code&gt;Update Tool&lt;/code&gt;, trace plots for these parameters will appear ‘live’ in the graphics windows.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;To obtain a trace plot showing the full history of the samples for any parameter for which you have previously set a sample monitor and carried out some updates:
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;Samples&lt;/code&gt; from the &lt;code&gt;Inference&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the name of the parameter in the white box marked node (or select name from pull down list).&lt;/li&gt;
&lt;li&gt;Click once with the LMB on the box marked history: a graphics window showing the sample trace will appear.&lt;/li&gt;
&lt;li&gt;Repeat for each parameter required.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In the graph &lt;a href=&#34;../../slides/03_MCMC/#31&#34;&gt;here&lt;/a&gt; (Panel a) the chain converges by about 250 iterations. Running the simulation for longer (5000 iterations) and discarding the first 1000 as a “burn-in” produces the history plot in (Panel b). This looks like a “fat hairy caterpillar”, the typical appearance of a chain which has converged to the target distribution, and can be treated as a sequence of independent samples from that distribution.&lt;/li&gt;
&lt;li&gt;The graph &lt;a href=&#34;../../slides/03_MCMC/#31&#34;&gt;here&lt;/a&gt; (Panel c), is the typical appearance of a chain which has converged to the target posterior distribution, but is slow to mix around that distribution. That is, the successive draws from the distribution are highly autocorrelated. In this case we should run the chain for longer (as in (Panel d)) to get sufficiently precise summaries of the posterior — see the next section for more advice.&lt;/li&gt;
&lt;li&gt;If you are running more than 1 chain simultaneously, the trace and history plots will show each chain in a different colour. This is shown in &lt;a href=&#34;../../slides/03_MCMC/#31&#34;&gt;here&lt;/a&gt; (Panel e). We can be reasonably confident that convergence has been achieved when all the chains appear to be overlapping one another.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-iterations-after-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How many iterations after convergence?&lt;/h2&gt;
&lt;p&gt;Once you are happy that convergence has been achieved, you will need to run the simulation for a further number of iterations to obtain samples that can be used for posterior inference. The more samples you save, the more accurate will be your posterior estimates. Therefore the number of samples to run depends on how many significant figures, for example, you need in your results.&lt;/p&gt;
&lt;p&gt;To assess the accuracy of the posterior mean for each parameter, you can simply look at the Monte Carlo standard error, which is provided in the summaries given by &lt;code&gt;stats&lt;/code&gt; (see the next section). This is an estimate of the difference between the mean of the sampled values (which we are using as our estimate of the posterior mean for each parameter) and the true posterior mean.&lt;/p&gt;
&lt;p&gt;To assess the accuracy of the whole distribution, you can compare the Monte Carlo standard error &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\hat\mu)\)&lt;/span&gt; to the sample standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma\)&lt;/span&gt; (also reported in the summary statistics table given by &lt;code&gt;stats&lt;/code&gt;). There is some theory which suggests that for the reported 95% posterior quantiles to have about 94.5% to 95.5% true coverage, &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\hat\mu)\)&lt;/span&gt; should be 1% or less of &lt;span class=&#34;math inline&#34;&gt;\(\hat\sigma\)&lt;/span&gt;. Equivalently the effective sample size of the chain&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; should be greater than about 4000.&lt;/p&gt;
&lt;p&gt;Alternatively you can just take an informal approach, and run a sufficient number of samples to ensure that the posterior summaries of interest don’t appear to change within the desired number of significant figures.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;obtaining-summary-statistics-of-the-posterior-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Obtaining summary statistics of the posterior distribution&lt;/h2&gt;
&lt;p&gt;To obtain summaries of the monitored samples, to be used for posterior inference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;Samples&lt;/code&gt; from the &lt;code&gt;Inference&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the name of the parameter to be summarised in the white box marked node (or select name from the pull down list, or type &lt;code&gt;*&lt;/code&gt; to select all monitored parameters).&lt;/li&gt;
&lt;li&gt;Type the iteration number which you want to start your summary from in the white box marked &lt;code&gt;beg&lt;/code&gt;: this allows the pre-convergence ‘burn-in’ samples to be discarded.&lt;/li&gt;
&lt;li&gt;Click once with LMB on box marked &lt;code&gt;stats&lt;/code&gt;: a table reporting various summary statistics based on the sampled values of the selected parameter will appear.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-summaries-of-the-posterior&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting summaries of the posterior&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;OpenBUGS&lt;/code&gt; includes options for producing various plots of posterior summary statistics. The plot options include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Kernel density plot: plots an estimate of the shape of the (univariate) marginal posterior distribution of a parameter see online manual for further details (select &lt;code&gt;OpenBUGS User Manual&lt;/code&gt; from the &lt;code&gt;Manuals&lt;/code&gt; menu in &lt;code&gt;OpenBUGS&lt;/code&gt; and then take a look at the subsection on &lt;code&gt;Density plots&lt;/code&gt; in
the &lt;code&gt;OpenBUGS Graphics&lt;/code&gt; section).&lt;/li&gt;
&lt;li&gt;Box plots, caterpillar plots or density strips: these plots show a side-by-side comparison of the posterior distributions of each element of a vector of parameters summarised either as a point estimate and 95% interval (caterpillar plot), by the mean, interquartile range and 95% interval
(box plot), or a representation of the whole distribution using varying shading (density strips). This is often used for random effects parameters. For example, suppose you have a vector of random effects called &lt;code&gt;p&lt;/code&gt; in your model:
&lt;ul&gt;
&lt;li&gt;You should have already set a samples monitor on the appropriate vector (p) and carried out a suitable number of updates.&lt;/li&gt;
&lt;li&gt;Then select &lt;code&gt;Compare&lt;/code&gt; from the &lt;code&gt;Inference&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the name of the parameter vector to be plotted (in this case &lt;code&gt;p&lt;/code&gt;) in the white box marked node.&lt;/li&gt;
&lt;li&gt;If you want to discard any pre-convergence burn-in samples before plotting, type the appropriate iteration number in the white box marked &lt;code&gt;beg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Click once with LMB on the button marked either box plot, caterpillar or density strips
as required.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Model fit: this option produces a ‘time series’ type plot and is suitable for plotting an ordered sequence of parameter estimates against corresponding values of a known variable, e.g. plotting posterior estimates of the fitted values of a growth curve against time. For example, in the &lt;code&gt;rats&lt;/code&gt; model from the &lt;code&gt;OpenBUGS&lt;/code&gt; examples Vol I, you could use this option to produce a plot of the vector of 5 fitted values for the weight of each rat (&lt;code&gt;mu[i,]&lt;/code&gt;) against age (&lt;code&gt;x&lt;/code&gt;), as follows:
&lt;ul&gt;
&lt;li&gt;You should have already set a samples monitor on the appropriate vector (i.e. &lt;code&gt;mu&lt;/code&gt;, the mean of the normal distribution assumed for the responses, &lt;code&gt;Y&lt;/code&gt;) and carried out an appropriate number of updates.&lt;/li&gt;
&lt;li&gt;Then select &lt;code&gt;Compare&lt;/code&gt; from the &lt;code&gt;Inference&lt;/code&gt; menu.&lt;/li&gt;
&lt;li&gt;Type the name of the (stochastic) parameter vector to be plotted on the vertical axis in the white box marked node (e.g. &lt;code&gt;mu[1,]&lt;/code&gt; to produce the plot for rat 1).&lt;/li&gt;
&lt;li&gt;Type the name of the known (i.e. not stochastic) variable to be plotted on the horizontal axis in the white box marked axis (in this case, &lt;code&gt;x&lt;/code&gt;, the name of the vector of ages at which each rat was measured).&lt;/li&gt;
&lt;li&gt;An optional argument is to type the name of another known (i.e. not stochastic) variable in the white box marked other (for example, &lt;code&gt;Y[1,]&lt;/code&gt; — this would plot the observed measurements for rat 1 as well as the fitted values &lt;code&gt;mu[1,]&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;If you want to discard any pre-convergence burn-in samples before plotting, type the appropriate iteration number in the white box marked &lt;code&gt;beg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Click once with LMB on the button marked model fit. The resulting plot shows the posterior median (solid red line) and posterior 95% interval (dashed blue line) for the values of node (in this case, the fitted values &lt;code&gt;mu[1,]&lt;/code&gt; for rat 1) against the values of the variable specified in
the axis box (in this case, &lt;code&gt;x&lt;/code&gt;, the age of the rat at each measurement). The black dots show the values of the variable specified in the other box (in this case,&lt;code&gt;Y[1,]&lt;/code&gt;, the observed weights for rat 1).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;There are various options for customising all these plots (e.g. changing the order in which the elements of the vector are plotted, switching the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis, etc.). To access these options, click on the window containing the plot to focus it, then place the mouse somewhere in the plot
window and click once with the right mouse button. A menu will appear and you should select the &lt;code&gt;Properties&lt;/code&gt; option. This will open another menu called &lt;code&gt;Plot Properties&lt;/code&gt; which provides options for editing plot margins, axis labels and fonts etc. (these are generic options for all &lt;code&gt;OpenBUGS&lt;/code&gt; plots), plus some special options specific only to certain plots (click on the &lt;code&gt;Special&lt;/code&gt; button at the bottom of the &lt;code&gt;Plot Properties&lt;/code&gt; menu). See the online manual for further details (select &lt;code&gt;OpenBUGS User Manual&lt;/code&gt; from the &lt;code&gt;Manuals&lt;/code&gt; menu in &lt;code&gt;OpenBUGS&lt;/code&gt;, then go to &lt;code&gt;OpenBUGS Graphics&lt;/code&gt;, or the &lt;code&gt;Inference&lt;/code&gt; Menu then &lt;code&gt;Compare&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;some-notes-on-the-bugs-language&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some notes on the &lt;code&gt;BUGS&lt;/code&gt; language&lt;/h2&gt;
&lt;div id=&#34;basic-syntax&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basic syntax&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The symbol &lt;code&gt;&amp;lt;-&lt;/code&gt; represents &lt;em&gt;logical&lt;/em&gt; dependence, ie &lt;code&gt;m &amp;lt;- a + b*x&lt;/code&gt; indicates that &lt;code&gt;m&lt;/code&gt; has the same value as the expression to the right hand side.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The symbol &lt;code&gt;~&lt;/code&gt; represents &lt;em&gt;stochastic&lt;/em&gt; dependence, eg &lt;code&gt;y ~ dunif(a,b)&lt;/code&gt; indicates that the variable &lt;code&gt;y&lt;/code&gt; is modelled using a Uniform(&lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt;) distribution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can use arrays and loops&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:n){
   r[i] ~ dbin(p[i],n[i])
   p[i] ~ dunif(0,1)
}&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some functions can appear on left-hand-side of an expression, e.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit(p[i])&amp;lt;- a + b*x[i]
log(m[i]) &amp;lt;- c + d*y[i]&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;mean(p[])&lt;/code&gt; to take mean of whole array, or &lt;code&gt;mean(p[m:n])&lt;/code&gt; to take mean of elements &lt;code&gt;m&lt;/code&gt; to &lt;code&gt;n&lt;/code&gt;. Also for &lt;code&gt;sum(p[])&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;dnorm(0,1)I(0,)&lt;/code&gt; means the distribution will be restricted to the range &lt;span class=&#34;math inline&#34;&gt;\((0,\infty)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;functions-in-the-bugs-language&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions in the &lt;code&gt;BUGS&lt;/code&gt; language&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;p &amp;lt;- step(x - 0.7)&lt;/code&gt; = 1 if &lt;code&gt;x&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(geq\)&lt;/span&gt; 0.7 and 0 otherwise. Hence monitoring p and recording its mean will give the probability that &lt;code&gt;x&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\geq\)&lt;/span&gt; 0.7.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p &amp;lt;- equals(x, 0.7)&lt;/code&gt; = 1 if &lt;code&gt;x&lt;/code&gt; = 0.7 and 0 otherwise.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tau &amp;lt;- 1/pow(s,2)&lt;/code&gt; sets &lt;span class=&#34;math inline&#34;&gt;\(\tau = 1/s^2\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s &amp;lt;- 1/ sqrt(tau)&lt;/code&gt; sets &lt;span class=&#34;math inline&#34;&gt;\(s=1/\tau\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p[i,k] &amp;lt;- inprod(pi[], Lambda[i,,k])&lt;/code&gt; sets &lt;span class=&#34;math inline&#34;&gt;\(p_{ik} = \sum_{j} \pi_j \Lambda_{ijk}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Many other mathematical functions: see &lt;code&gt;Functions&lt;/code&gt; under the &lt;code&gt;Help&lt;/code&gt; menu in &lt;code&gt;OpenBUGS&lt;/code&gt; for full syntax.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;openbugs-data-formats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;OpenBUGS&lt;/code&gt; data formats&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;OpenBUGS&lt;/code&gt; accepts data files in:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Rectangular format&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n[] r[]
47 0
148 18
...
360 24
END&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;R&lt;/code&gt; / &lt;code&gt;S-Plus&lt;/code&gt;-like ‘list’ format:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list(
   N=12,
   n = c(47,148,119,810,211,196,148,215,207,97,256,360),
   r = c(0,18,8,46,8,13,9,31,14,8,29,24)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The more flexible ‘list’ format is recommended, since data often consist of mixtures of scalars and vectors/arrays, or vectors/arrays of different lengths.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt; The Monte Carlo standard error is higher in chains which are autocorrelated (i.e. look less like independent samples from a posterior, and are slower to explore the posterior distribution, so give less accurate estimates of the posterior mean). The “effective sample size” &lt;span class=&#34;math inline&#34;&gt;\(n_{eff}\)&lt;/span&gt; denotes that a chain of &lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; n_{eff}\)&lt;/span&gt; autocorrelated samples as much information as &lt;span class=&#34;math inline&#34;&gt;\(n_{eff}\)&lt;/span&gt; independent samples. There are several ways of calculating &lt;span class=&#34;math inline&#34;&gt;\(n_{eff}\)&lt;/span&gt; — three different methods are used in Bayesian Methods in Health Economics, The BUGS Book and &lt;code&gt;R2OpenBUGS&lt;/code&gt;!&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment</title>
      <link>/publication/gabrioetal-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/publication/gabrioetal-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data</title>
      <link>/publication/gabrio-etal-2018/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      <guid>/publication/gabrio-etal-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Cost-Effectiveness Analysis with the R package BCEA</title>
      <link>/publication/baioetal-2016/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>/publication/baioetal-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R for Data Science</title>
      <link>/publication/wickham-etal-2020/</link>
      <pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate>
      <guid>/publication/wickham-etal-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Doing Bayesian Data Analysis</title>
      <link>/publication/kruschke-2015/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      <guid>/publication/kruschke-2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hands-On Programming with R</title>
      <link>/publication/gromelund-2014/</link>
      <pubDate>Sun, 01 Jun 2014 00:00:00 +0000</pubDate>
      <guid>/publication/gromelund-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Data Analysis, 3rd edition</title>
      <link>/publication/gelman-etal-2013/</link>
      <pubDate>Mon, 04 Nov 2013 00:00:00 +0000</pubDate>
      <guid>/publication/gelman-etal-2013/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Methods in Health Economics</title>
      <link>/publication/baio-2012/</link>
      <pubDate>Thu, 01 Nov 2012 00:00:00 +0000</pubDate>
      <guid>/publication/baio-2012/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The BUGS Book</title>
      <link>/publication/lunn-etal-2012/</link>
      <pubDate>Thu, 01 Nov 2012 00:00:00 +0000</pubDate>
      <guid>/publication/lunn-etal-2012/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evidence Synthesis for Decision Making in Healthcare</title>
      <link>/publication/welton-etal-2012/</link>
      <pubDate>Tue, 01 May 2012 00:00:00 +0000</pubDate>
      <guid>/publication/welton-etal-2012/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Missing data in longitudinal studies: strategies for Bayesian modeling and sensitivity analysis</title>
      <link>/publication/daniels-hogan-2008/</link>
      <pubDate>Sun, 01 Jun 2008 00:00:00 +0000</pubDate>
      <guid>/publication/daniels-hogan-2008/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Analysis Using Regression and Multilevel/Hierarchical Models</title>
      <link>/publication/gelman-hill-2007/</link>
      <pubDate>Sat, 01 Sep 2007 00:00:00 +0000</pubDate>
      <guid>/publication/gelman-hill-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decision Modelling for Health Economic Evaluation</title>
      <link>/publication/briggs-etal-2006/</link>
      <pubDate>Thu, 01 Jun 2006 00:00:00 +0000</pubDate>
      <guid>/publication/briggs-etal-2006/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian approaches to clinical trials and health care evaluation</title>
      <link>/publication/spiegelhalter-etal-2004/</link>
      <pubDate>Mon, 01 Nov 2004 00:00:00 +0000</pubDate>
      <guid>/publication/spiegelhalter-etal-2004/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
