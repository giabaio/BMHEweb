[{"authors":["gianluca-baio"],"categories":null,"content":"My main interests are in Bayesian statistical modelling for cost effectiveness analysis and decision-making problems in the health systems, hierarchical/multilevel models and causal inference using the decision-theoretic approach.\nI lead the Statistics for Health Economic Evaluation research group within the department of Statistical Science. Our activity revolves around the development and application of Bayesian statistical methodology for health economic evaluation, e.g. cost-effectiveness or cost-utility analysis. We work in close collaboration with academics both within UCL and at other institutions and our activities include a series of seminars aimed at statisticians, health economists and clinicians working in economic evaluations. I collaborate with the UK National Institute for Health and Care Excellence (NICE) as a Scientific Advisor on Health Technology Appraisal projects.\n","date":1656061200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1656061200,"objectID":"2cbb7559ba13c7aa6a31b9af6b1adc43","permalink":"/author/gianluca-baio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/gianluca-baio/","section":"authors","summary":"My main interests are in Bayesian statistical modelling for cost effectiveness analysis and decision-making problems in the health systems, hierarchical/multilevel models and causal inference using the decision-theoretic approach.\nI lead the Statistics for Health Economic Evaluation research group within the department of Statistical Science.","tags":null,"title":"Gianluca Baio","type":"authors"},{"authors":["anna-heath"],"categories":null,"content":"Anna Heath is a biostatistician at The Hospital for Sick Children, Toronto, an assistant professor at the University of Toronto, and an honorary research fellow at University College London. She completed her PhD at University College London in 2018. Her research focuses on novel methodology for clinical trial design and research prioritisation. In particular, she is interested in novel computation methods for Value of Information, a measure of the monetary or health benefit that is expected to be generated from undertaking future research. She is a member of the Collaborative Network for Value of Information (ConVOI), a network of researchers working to improve the calculation and adoption of Value of Information in public health research. She is also interested the Bayesian methods for trial design and analysis and works as a statistician on four innovative clinical trials.\n","date":1656078300,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1656078300,"objectID":"e0491f876922da551a7f5d35229e1f3c","permalink":"/author/anna-heath/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/anna-heath/","section":"authors","summary":"Anna Heath is a biostatistician at The Hospital for Sick Children, Toronto, an assistant professor at the University of Toronto, and an honorary research fellow at University College London. She completed her PhD at University College London in 2018.","tags":null,"title":"Anna Heath","type":"authors"},{"authors":["nathan-green"],"categories":null,"content":"I am a Senior Research Fellow in the Department of Statistical Science at University College London. I am also involved with the Statistics, Modelling and Economics Department at Public Health England and I am a member of the Scientific Committee of R-HTA consortium.\nI have a number of years experience working on a wide range of projects across government and academia in defence and health. I studied mathematics and statistics at the University of Newcastle-Upon-Tyne and obtained a PhD in applied probability from the University of Liverpool. After working for the Ministry of Defence for several years, applying novel Bayesian Inference ideas, I moved back into academia and the field of public health in 2010.\n","date":1655974800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1655974800,"objectID":"ff4179058415ab03722a35c2e31e3e66","permalink":"/author/nathan-green/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nathan-green/","section":"authors","summary":"I am a Senior Research Fellow in the Department of Statistical Science at University College London. I am also involved with the Statistics, Modelling and Economics Department at Public Health England and I am a member of the Scientific Committee of R-HTA consortium.","tags":null,"title":"Nathan Green","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":[],"content":" 1. Coins example  Start OpenBUGS\n Load the file coins.odc from the appropriate folder — for example, this could look something  C:\\bayes-hecourse\\1_monte-carlo. This program will simulate throws of 10 balanced coins and record which give 8 or more heads.\n First run the program from a script. Load the file coins-script.odc and check that the path to the working directory is appropriate. Check the script makes sense. With this script window open, click on Info  Open Log.  NB: we use the notation Command1  Command2 to indicate that you need to click on the menu labelled as Command1 and then on the sub-menu labelled as Command2).   This opens a new window (containing the log of your BUGS session) — if you do not then you will not be able to see the results. Then Model  Script.\n Now try running using the interactive interface.  Open up the Model  Specification window.\n Making sure that coins.odc is open, click on check model.\n Then compile and gen inits. Open up the Model  Update window and generate 1000 iterations.\n Then open up the Inference  Samples window, type Y in the node window and then click set. This sets the monitor. Repeat for P8. Type * in the node window to indicate all monitored quantities. Click trace to generate traces of the simulated values.\n Then do another 1000 updates. stats then gives summary statistics.  Assume now that the coin is actually unbalanced and the probability of a head is 0.7. Find the probability that 30 throws will show 15 heads or fewer.\n Hint: you could use the notation step(15.5 - Y) or 1 - step(Y - 15.5)      2. Drug example  Open the file drug-MC.odc and carry out a BUGS run for this model, obtaining the results shown in the lectures. You should be able to run it using the previous instructions (question 1) and the short list given in the lectures. Otherwise full details are given in Running a model in OpenBUGS of the “Hints on using OpenBUGS” chapter of this handout. If stuck, a script drug-MC-script.odc is available.\n Edit the model code to specify a Uniform(0, 1) prior on the response rate theta, and re-run the analysis.\n Note: the syntax for the uniform prior in BUGS is dunif(a,b) where a and b are the lower and upper bounds. The values of a and b can either be specified in the data file, or directly in the BUGS code (e.g. a \u0026lt;- 1), or just replace a and b by their values in the dunif statement).\n   Plot the predictive distribution for the number of successes.\n What is now the predictive probability that 15 or more patients will experience a positive response out of 20 new patients affected?\n   3. Simulating functions of random quantities  Write a model for a variable with a normal distribution with mean 0 and standard deviation 1 (remember that BUGS parameterises the normal in terms of precision = 1/Variance).\n Simulate 10000 values and plot their density.\n Simulate 10000 values of a variable \\(Y\\) with a normal distribution with mean 1 and standard deviation 2.\n For the same \\(Y\\), create a new variable \\(Z = Y^3\\) , simulate 10000 values of \\(Z\\), and find the mean and variance of \\(Z\\), and the probability that \\(Z\\) is greater than 10. Are these results surprising?\n Plot the density of \\(Z\\).   If you are using the binder VM, you will also have access to a series of R scripts that essentially replicate these analyses using R only (without accessing BUGS).\nIt may be helpful to use them, particularly to get even more familiar with the underlying concepts of simulations (which underpin Monte Carlo and Markov Chain Monte Carlo methods). If you want to use them, you can download them from here.\nNB: Instructions to install all the relevant packages that are available in the binder VM on your local machine are available here.    ","date":1655723700,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655723700,"objectID":"a22b082091367e2b70da12600feceaaa","permalink":"/practical/01_monte-carlo/","publishdate":"2022-03-12T11:15:00Z","relpermalink":"/practical/01_monte-carlo/","section":"practical","summary":"1. Coins example  Start OpenBUGS\n Load the file coins.odc from the appropriate folder — for example, this could look something  C:\\bayes-hecourse\\1_monte-carlo. This program will simulate throws of 10 balanced coins and record which give 8 or more heads.","tags":[],"title":"Practical 1. Monte Carlo in BUGS","type":"book"},{"authors":null,"categories":[],"content":" 1. Understanding Gibbs sampling The file GibbsSampling.xls is a spreadsheet which you can open using MS Excel (or any similar spreadsheet software, such as LibreOffice Calc or OpenOffice Calc) and contains a very simple example of Gibbs Sampling at work. The first worksheet sets up a simple bivariate Normal model: \\[\\boldsymbol{y}=(y_1,y_2) \\sim \\mbox{Normal}(\\boldsymbol{\\mu},\\boldsymbol\\Sigma)\\] with \\(\\boldsymbol\\mu=(\\mu_1,\\mu_2)\\) and \\(\\boldsymbol\\Sigma=\\left(\\begin{array}{cc}\\sigma^2_1 \u0026amp; \\sigma_1\\sigma_2\\rho \\\\ \\sigma_1\\sigma_2\\rho \u0026amp; \\sigma^2_2\\end{array}\\right )\\).\nGiven this set up, standard Normal theory says that the full conditionals are\n \\(y_2 \\mid y_1 \\sim \\mbox{Normal}\\left(\\mu_2 + \\frac{\\sigma_2}{\\sigma_1}\\rho\\left(y_1-\\mu_1\\right),\\left(1-\\rho^2\\right)\\sigma^2_2\\right)\\), and\n \\(y_1 \\mid y_2 \\sim \\mbox{Normal}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho\\left(y_2-\\mu_2\\right),\\left(1-\\rho^2\\right)\\sigma^1_2\\right)\\).\n  In order to familiarise yourself with the MCMC process:\nInspect cells J4 and K4; the former sets the initial value of \\(y_1\\) (which is stored in cell B16), while the latter simulates from the conditional distribution of \\(y_2\\) given the current value for \\(y_1\\). (NB: the notation NORM.INV(rand(),mean,standard_dev) instructs Excel to simulate a random draw from a Normal distribution with parameters mean and standard_dev).\n Move to the second spreadsheet (named 10iters). This shows the \\((y_1,y_2)\\) plane with the first 10 simulated values; compare the graph with those in the spreadsheets 100iters, 500iters and 1000iters (showing 100, 500 and 1000 iterations of the Gibbs sampler, respectively). Assess convergence on the basis of the graphs. Are 100 iterations enough?\n Move to the spreadsheet named Tr_y1: this shows the traceplot for \\(y_1\\) over 1000 simulations. Would you assess that convergence has been satisfactorly reached for this variable? Repeat this for the worksheet Tr_y2, which shows the traceplot for \\(y_2\\).\n Go back to the first spreadsheet and modify the initial values for \\(y_1,y_2\\). Go to cell B16 and type the Excel command =D16 and to cell B17 and type the command =D17 (these will copy over two random initial values drawn from a Normal distribution with mean 0 and standard deviation 10). Inspect the other spreadsheets; how is convergence affected for \\((y_1,y_2)\\)?\n Go back to the first spreadsheet and modify the value for the correlation coefficient in cell E8, by typing 0.99. Inspect the other spreadsheets; how is convergence affected for \\((y_1,y_2)\\)?\n   NB: This is not really a problem that requires Gibbs sampling. In fact, that’s not even a full Bayesian model (where observed data are used to learn about the model parameters). And more to the point, it’s not even a “forward sampling” problem (like the ones we saw in practical 1), because in this case we’re actually fixing the parameters of the model to some fixed values…\nNevertheless, it may be handy to understand and visualise in the spreadsheet the process of repeated sampling from the full conditional distributions that characterises the Gibbs sampling algorithm.\nAnd you should check out the MCMC tutorial to see the code used to make the “semi-conjugated” example discussed in class.    2. MCMC in BUGS Consider again the drug example discussed in the lecture. The file drug-MCMC.odc contains the code to describe the model as well as data and initial values for a simple analysis. From , open the file and, using the commands and actions discussed in the first practical, execute the following analyses.\n Click on Model and then Specification to open the Specification Tool. Check the model (which is contained in the code prefixed by the comment “### Model description”, which describes in general the distributional assumptions); load the data (contained in the code prefixed by the comment “### DATA in separate list (better in most cases)”); compile the model; and load the initial values (contained in the code prefixed by the comment “### INITIAL VALUES). Click on Inference and then Samples to open the Sample Monitor Tool. Choose the nodes you think should be monitored and then click on Model and then Update to set up the MCMC run, selecting a suitable number of replications. Finally, produce summary statistics and graphical description of the results.\n You can replicate the above analysis by using the code prefixed by the comment\n ### Data supplied with model description ### (only feasible for very few data points)  In this case, you do not need to load the data, as they are already given in the model code and thus are automatically loaded when the model is compiled.\n Try and monitor the node y. Why do you think does not let you do this?\n   If you are using the binder VM, you will also have access to a series of R scripts that essentially replicate these analyses using R only (without accessing BUGS).\nIt may be helpful to use them, particularly to get even more familiar with the underlying concepts of simulations (which underpin Monte Carlo and Markov Chain Monte Carlo methods). If you want to use them, you can download them from here.\nNB: Instructions to install all the relevant packages that are available in the binder VM on your local machine are available here.    ","date":1655733600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655733600,"objectID":"b0954a81c4a8e4abb2ea686d78c932ba","permalink":"/practical/02_mcmc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/02_mcmc/","section":"practical","summary":"1. Understanding Gibbs sampling The file GibbsSampling.xls is a spreadsheet which you can open using MS Excel (or any similar spreadsheet software, such as LibreOffice Calc or OpenOffice Calc) and contains a very simple example of Gibbs Sampling at work.","tags":[],"title":"Practical 2. Markov Chain Monte Carlo","type":"book"},{"authors":null,"categories":[],"content":" This is a very brief introduction to R (which can be downloaded from the website https://cran.r-project.org/) and its capabilities. R is available on the UCL Desktops and you can actually use the R frontend software called Rstudio. You do not have to use Rstudio and in fact, the standard R terminal would do just as well. However, Rstudio has some nice features and it is a better text editor/file manager, so it is probably a good idea to start using it. It will be extremely focussed on the characteristics that are instrumental to do health economic evaluations using a combination of R, BUGS and some useful packages (such as BCEA). Thus it is by no means exhaustive!\n(Ridiculously short and incomplete) Introduction to R When you open the R terminal, you are presented with the possibility of typing commands. You may want to open a text editor (e.g. the simple one built into the R Windows interface) in which you can type directly these commands, and save them to a script for future use. Another possibility is to use R from within a more sophisticated “integrated development environment”, such as Rstudio, which has many more features than the basic Windows interface.\nIn any case, R is a very powerful tool; more importantly, it is free and you can find a wealth of documentation on the internet. R has a set of built-in commands, which you can use for basic operations. However, there are also many add-on packages containing sets of functions designed to perform specific statistical tasks. These packages can be installed to your R by typing the command\n install.packages(\u0026quot;package_name\u0026quot;) (assuming you have an internet connection). This command only needs to be executed one time. Once a package is installed in your local library (a collection of packages) you can make it available to the current R session by typing the command\n library(package_name) For these practicals, you will need to install and load the packages:\n BCEA, which can be used to post-process the results of a (Bayesian) model to perform a health economic evaluation.\n R2OpenBUGS, which can be used to interface R and BUGS .\n  You do this by typing in your R terminal the commands\ninstall.packages(\u0026quot;BCEA\u0026quot;) install.packages(\u0026quot;R2OpenBUGS\u0026quot;) library(BCEA) library(R2OpenBUGS) Both BCEAand R2OpenBUGS will automatically load other packages that they depend on — this means that in order to work, they need to access functions that are part of other packages.\nIf you wish so, you can use JAGS in the practicals. To this end (and assuming you have actually installed the current version of JAGS to your computer), you will need to also install the package R2jagsS, which you can do by typing in your R terminal\ninstall.packages(\u0026quot;R2jags\u0026quot;) Notice that if you decide to use JAGS instead of BUGS, you will need to slightly modify some of the commands.\nOnce a package is loaded to your R workspace, you can type the command help(package_name), which will open a window displaying a description of the package. For example help(BCEA) provides some basic information (including details of the current version). You can use the command help also on specific functions within the package, e.g. typing help(bcea) describes in detail how to use the bcea function (notice that in this case the package name is typeset in uppercase, while the function is lowercase!).\nThe very basic commands that are required to do a typical R session working with BUGS and BCEAwill be given and described later or in the scripts that we refer to in the practicals.\n MCMC in R/BUGS Consider Laplace’s analysis of birth data, which included \\(y=241\\,945\\) females out of \\(n=493\\,527\\) babies born in Paris between 1745 and 1770. Assume the following modelling assumptions: \\[\\begin{aligned} y \u0026amp; \\sim \\mbox{Binomial}(\\theta,n)\\\\ \\theta \u0026amp; \\sim \\mbox{Uniform}(0,1).\\end{aligned}\\]\nWrite BUGS code to encode the modelling assumptions above and save it to a .txt file (you can choose whatever name and location you want. We assume you’ve chosen ModelLaplace.txt, which you have saved in the current directory).\n Open R , which will be used to pre-process the data, call BUGS in background to perform the MCMC estimation and then post-process the results. At the terminal, type the following commands\ny \u0026lt;- 241945 n \u0026lt;- 493527 data \u0026lt;- list(y=y,n=n) This defines the variables y and n into the R workspace and create a list (named data), which contains them.\n (Notice that in R you can use the notation “\u0026lt;-” to indicate assignment to a variable, as well as the more straightforward notation “=”. Thus the commands a \u0026lt;- b and c = b create two variables a and c which both take the value associated with the variable b — irrespective of the sign used to define the assignment operator). Technically (and at the level of the data stored in the computer memory) the two statements are not identical, but for all intent and purposes, they are.   Now, set up some utility variables, such as the location of the BUGS model file, the parameters to be monitored and the initial values, by typing\nfilein \u0026lt;- \u0026quot;PATH_TO_FILE/NAME_OF_FILE.txt\u0026quot; params \u0026lt;- \u0026quot;theta\u0026quot; inits_det \u0026lt;- list(list(theta=.1),list(theta=.9)) inits_ran \u0026lt;- function(){list(theta=runif(1))} The R workspace now contains a variable filein (a text string with the path to your model file, e.g. C:/bayes-hecourse/4_bcea/ModelLaplace.txt); a variable params (another text string containing the name of the variable you want to monitor); the two variables inits_det and inits_ran can be used to instruct BUGS which initial values to use. The former is a list containing two initial values for the node theta (they are arbitrarily set to 0.1 and 0.9, so that two chains can be run, starting from different points). The latter is an R function, which creates a list with a variable theta, which is assigned a random draw from a Uniform(0, 1) distribution.\n Call BUGS from within R by using the following commands\nmodel \u0026lt;- bugs(data=data,inits=inits_det, parameters.to.save=params,model.file=filein, n.chains=2,n.iter=10000,n.burnin=4500, n.thin=1,DIC=TRUE) — notice that you can either use the syntax inits=inits_det or inits=inits_ran. BUGS is called in background and executes the MCMC analysis. When it has completed (which should be extremely quick in this simple case), you will regain use of the R terminal.\nThe R object model contains many variables; typing\nnames(model) will print a list of them. Each can be accessed using the R “$” notation. For example, typing\nmodel$n.iter will print the number of iterations used by BUGS .\n Check the results by using the command\nprint(model,digits=3,intervals=c(0.025,0.975)) This instructs R to give a tabular output reporting summary statistics (specifically reporting the 2.5% and 97.5% percentiles) from the posterior distribution(s) of the node(s) monitored, using 3 significant digits.\nWhat can you say about convergence by just looking at the resulting tabular display that R will provide?\n Attach the MCMC simulations to the R workspace by typing\nattach.bugs(model) This makes all the elements of the object model available in your R session.\nPlot a histogram of the posterior distribution for theta by typing\nhist(theta) What can you say about the underlying probability of a female birth?\n   MCMC in R/JAGS If you want to use JAGS instead of BUGS, there are not many difference with respect to the previous code/analysis. You can use these guidelines to perform the analyses in the practicals using JAGS .\nThe first thing to do is to load the package R2jags, which you do by typing in your R terminal the command library(R2jags). You can now replicate steps 1. and 2. from the previous section to write your model and prepare the data and relevant variables. Notice, however, that while most of the time the BUGS code will apply with no problems to JAGS, there are a few differences that may prevent your model code from working. One such example is the way in which BUGS and JAGS manage truncation and censoring (see, for instance page 205 of BMHE).\nThe first change is that (unsurprisingly!) you need to call the function jags, instead of the function bugs. Thus, point 3. above becomes\nmodel2 \u0026lt;- jags(data=data,inits=inits_det, parameters.to.save=params,model.file=filein, n.chains=2,n.iter=10000,n.burnin=4500, n.thin=1,DIC=TRUE) This time, JAGS is called in background and performs the MCMC estimation. By default, R2jags shows a text bar with the progression through the required simulations; a running series of asterisks is printed and the counter is incremented while the iterations are generated.\nThe second and perhaps most important difference is in the nature of the objects created. If you type in your R terminal the command names(model2), R will show you the names of the elements of the object model2\n[1] \u0026quot;model\u0026quot; \u0026quot;BUGSoutput\u0026quot; \u0026quot;parameters.to.save\u0026quot; [4] \u0026quot;model.file\u0026quot; \u0026quot;n.iter\u0026quot; \u0026quot;DIC\u0026quot;  You can access each of these elements using the syntax object$element, so for example R will respond to the command model2$n.iter by printing the value for the number of iterations that have been used (in this case 10000).\nIf, for instance, you had run the steps described in the previous section and had produced the BUGS object model, R reponse to a command names(model) would show different elements. In fact, the difference is that JAGS stores these elements in the object model2$BUGSoutput; thus typing names(model2$BUGSOutput) gives the following output\n[1] \u0026quot;n.chains\u0026quot; \u0026quot;n.iter\u0026quot; \u0026quot;n.burnin\u0026quot; \u0026quot;n.thin\u0026quot; [5] \u0026quot;n.keep\u0026quot; \u0026quot;n.sims\u0026quot; \u0026quot;sims.array\u0026quot; \u0026quot;sims.list\u0026quot; [9] \u0026quot;sims.matrix\u0026quot; \u0026quot;summary\u0026quot; \u0026quot;mean\u0026quot; \u0026quot;sd\u0026quot; [13] \u0026quot;median\u0026quot; \u0026quot;root.short\u0026quot; \u0026quot;long.short\u0026quot; \u0026quot;dimension.short\u0026quot; [17] \u0026quot;indexes.short\u0026quot; \u0026quot;last.values\u0026quot; \u0026quot;program\u0026quot; \u0026quot;model.file\u0026quot; [21] \u0026quot;isDIC\u0026quot; \u0026quot;DICbyR\u0026quot; \u0026quot;pD\u0026quot; \u0026quot;DIC\u0026quot;  which is basically the same as that produced by a call to BUGS .\nThe third basic difference is that if you want to make the results of your simulations available to the R workspace, you need to use the command attach.jags(model2) (or whatever the name of the object created with the call to the JAGS function).\n Health economic evaluation in R using BCEA Suppose the interest is in an infectious disease, e.g. influenza, for which a new vaccine has been produced. Under the current management of the disease some individuals treat the infection by taking over the counter (OTC) medications. Some subjects visit their GP and, depending on the severity of the infection, may receive treatment with antiviral drugs, which usually cures the infection. However, in some cases complications may occur. Minor complications will need a second GP visit after which the patients become more likely to receive antiviral treatment. Major complications are represented by pneumonia and can result in hospitalisation and possibly death. In this scenario, the costs generated by the management of the disease are represented by OTC medications, GP visits, the prescription of antiviral drugs, hospital episodes and indirect costs such as time off work. QALYs are used as a measure of clinical benefit. The focus is on the clinical and economic evaluation of the policy that makes the vaccine available to those who wish to use it (\\(t=1\\)) against the null option (\\(t=0\\)) under which the vaccine will remain unavailable.\n The file Vaccine.RData contains the results of a economic model, which are saved in an object he.  NB: To download the file, right-click on the link and select “Save Link As…”   From your R terminal, load the data and examine the object he using the commands\nload(\u0026quot;Vaccine.RData\u0026quot;) names(he) Load BCEAby typing the command\nlibrary(BCEA) and compute the ICER using the elements delta.e and delta.c, which are included in the object he and contain the simulations from the posterior distributions of the variables \\((\\Delta_e,\\Delta_c)\\) (i.e. the effectiveness and cost differential, respectively), obtained by running a suitable Bayesian model.  Hint: the command a = b / c instructs R to compute a variable a defined as the ratio between the variables b and c; similarly, the command d = mean(e) saves the mean of the values contained in a vector e to a new variable d.    Assuming a fixed value of the willingness-to-pay threshold of \\(k=30\\,000\\), compute the value of the EIB.  Hint: consider the definition of EIB and use the values for delta.e and delta.c. In R , the commands a = b + c and d = e - f define variables a and d as the sum and the difference of other variables, previously defined in the workspace.    In the R terminal, execute the command\nceplane.plot(he) to produce the cost-effectiveness plane for the comparison of \\(t=1\\) vs \\(t=0\\), using a default value of \\(k=25\\,000\\).\nHow can you interpret the resulting graph, in terms of economic evaluation?\n In the R terminal, execute the command\nceplane.plot(he,wtp=10000) to produce the cost-effectiveness plane for the comparison of \\(t=1\\) vs \\(t=0\\), using a value of \\(k=10\\,000\\).\nHow is the economic interpretation of the results modified, in comparison to point c?\n In the R terminal, execute the command\neib.plot(he, plot.cri=FALSE) to obtain a graph showing the EIB as a function of the willingness-to-pay. (Notice that you need to add the option plot.cri=FALSE, which prevents BCEA from drawing a credible interval around the EIB, in this particular case. The reason is that the BCEA object he has been tampered with for the purpose of running this exercise. In general, you do not need to specify the option and by default BCEA will also show the credible interval).\nHow can you interpret this graph in terms of cost-effectiveness analysis?\n In the R terminal, execute the command\ncontour(he) to produce the cost-effectiveness plane for the comparison of \\(t=1\\) vs \\(t=0\\) together with a contour plot of the bivariate distribution for \\((\\Delta_e,\\Delta_c)\\). The graph also reports the proportion of simulated points in each quadrant.\nComment on the cost-effectiveness results. What is the probability that vaccination (\\(t=1\\)) is dominated by the status quo (\\(t=0\\))?\n   ","date":1655742600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655742600,"objectID":"b9f309820c78d2a9b2d10c0bdb91e661","permalink":"/practical/03_bcea/","publishdate":"2022-03-12T16:30:00Z","relpermalink":"/practical/03_bcea/","section":"practical","summary":"This is a very brief introduction to R (which can be downloaded from the website https://cran.r-project.org/) and its capabilities. R is available on the UCL Desktops and you can actually use the R frontend software called Rstudio.","tags":[],"title":"Practical 3. Introduction to R and cost-effectiveness analysis using BCEA","type":"book"},{"authors":null,"categories":[],"content":" 1. Cost-effectiveness analysis with individual-level data Work through the script IPD_analysis.R to run the model included in the file normal-mod.txt. The relevant data are stored in the file cost-data.txt.\n Load the data into R using the source(...) function. Inspect the loaded data. Note that costs are in £1000, so that OpenBUGS does not “overflow” during calculations with large numbers.\n Run the model using BUGS through R. Make sure you understand how to use the bugs(...) function in R.  NB: The script IPD_analysis.R sets the initial values to NULL for this part. This means that we are instructing R to generate the initial values automatically. This would be the equivalent to clicking the button gen inits in the OpenBUGS interface. However, you can also use some deterministic values stored in the files normal-inits1.txt, normal-inits2.txt and normal-inits3.txt. You can simply open them and copy the code onto the R terminal. Or see later in the script (lines 65-67), for R code to load up the values into the R workspace.    Plot the posterior distribution of the costs. This can be plotted either as the joint posterior distribution of the cost in each arm of the posterior distribution of the cost differential.\n Find the mean cost in each arm and mean difference in costs between arms. Compare these with the figures in the slides.\n  In the file normal-mod.txt, some code is included to calculate the deviance, a measure of model fit. The quantity -2\\(\\times\\)log likelihood of a normal distribution is written out in BUGS model code.\nWhat is the deviance of the model, using this method?\n Find the deviance using the monitor set by BUGS automatically.\n Find the deviance using the DIC. (Hint: See lecture slides).\n  Make sure the answers from methods a, b and c match.  This part requires you to work from the results produced by the model.    Continuing through the script IPD_analysis.R, run the model contained in the file cgeg-mod.txt.\n Load the data stored in the file cost-util-data.txt. Inspect the loaded data. Set the initial values for three Markov chains within BUGS — you can use the values stored in the files cgeg-inits1.txt, cgeg-inits2.txt and cgeg-inits3.txt.  NB: you don’t necessarily need to use 3 chains (and so 3 sets of initial values). In general, it is good practice to select at least 2 parallel chains, so that you can (more) easily assess convergence.    Run the model using BUGS through R.\n Draw a scatter plot of the difference in costs and effects.\n Compare the incremental mean costs and effects with the figures in the slides in the lecture. In the slides, do the point estimates and the sizes of the confidence ellipses from this model agree (roughly) with statistics for delta.e and delta.c you have calculated?\n  Using the model contained in cgeg-mod.txt, investigate the cost-effectiveness results for a number of willingness-to-pay thresholds\n For a willingness-to-pay of £500, calculate the mean incremental net benefit. Which treatment should we implement? Calculate the 95% credible interval for the incremental net benefit and investigate its distribution. Are we uncertain about which treatment is optimal? What is the probability of cost-effectiveness for a willingness-to-pay of £500\n Calculate the cost-effectiveness acceptability curve using the CEAC variable in the BUGS model. Which treatment should be implemented?\n    ","date":1655805600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655805600,"objectID":"ec0a3b92ecd449a9c2d2f1705825582b","permalink":"/practical/04_ild/","publishdate":"2022-03-13T10:00:00Z","relpermalink":"/practical/04_ild/","section":"practical","summary":"1. Cost-effectiveness analysis with individual-level data Work through the script IPD_analysis.R to run the model included in the file normal-mod.txt. The relevant data are stored in the file cost-data.txt.","tags":[],"title":"Practical 4. Cost-effectiveness analysis with individual-level data","type":"book"},{"authors":null,"categories":[],"content":" 1. Aggregated level data The file EvSynth.R is a script that can be used to perform the Bayesian analysis of the influenza model, described in class. This can in turn be used to also extend the analysis to the cost-effectiveness evaluation of prophylaxis.\nFirst, open the file EvSynth.txt, which contains the BUGS translation of the model. Make sure you understand the structure and the underlying assumptions.\n Following the script, load the data available from the published studies on the effectiveness of prophylaxis with NIs and the incidence of influenza, and load the data on costs and the other relevant variables for the model.\n Following the script, set up the data list, the pointer to the file containing the model code and the parameters list.\n Following the script, write a function that creates random initial values to be associated to the parameters of the model.  NB: the R notation alpha=rnorm(S,0,1) creates S random draws from a Normal of mean 0 and standard deviation 1.   Are all the necessary nodes initialised by the function inits? If not, add the missing nodes (together with a suitable random initialisation) to the function. (Hint: you need to check the model code and that all the unobserved random quantities are initialised).\n Following the script, set up the number of iterations, burn-in and thinning to be used and run the MCMC simulation calling R2OpenBUGS from within R. You can tweak with the thinning parameter by increasing (or decreasing) it to assess its impact to model convergence.\n Following the script, use the R command print to display the summary statistics of the MCMC run.  NB: the arguments digits=3 and intervals=c(0.025, 0.975) instruct R to show up to 3 significant digits in the table and to display the 95% central credible intervals, based on the 2.5% and the 97.5% percentiles of the posterior distributions.)    Using the table displayed by R with the summary for the posterior distributions of the monitored notes, comment on convergence, specifically with reference to the \\(\\hat{R}\\) statistics and the effective sample size.\n Following the code, produce a traceplot for the node p1 and comment on convergence.  NB: in R , the command plot is used to produce graphs. The number and type of arguments that it can take vary depending on the type of plot you are trying to make. In this case, you are selecting the first 500 simulations for the node p1. These are stored in the element sims.list$p1 of the object es — the R notation [1:500] indicates that you want to select the values from the first to the five-hundredth — these are the values associated with the first chain. The additional option ylabel=p1 instructs R to use the string p1 as label for the y-axis of the graph. To add another layer to an existing plot, you can use the R command points. In this case, you give a similar argument, except you are requiring the values from 501 to 1000 of the element sims.list$p1 — these are the simulations for the second chain)    Try and produce a trace (history) plot for other nodes.\n Now, still following the script, prepare to run the economic analysis of this model. First you need to attach the object es to your current R session. Then you need to combine the model output (in terms of the MCMC simulations) so that suitable measures of costs and effectiveness are created.\n Following the script, load the package BCEA and perform the basic cost-effectiveness analysis.\n If you wish to, you can replicate the MCMC analysis by running R2OpenBUGS directly (i.e. without interfacing it with R). The files script.txt, data.txt, inits1.txt and inits2.txt contain the script to run the entire MCMC analysis from R2OpenBUGS, the data and two sets of initial values.\n   ","date":1655813700,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655813700,"objectID":"d5de820cd2d5b8dd62e393f51cf4b806","permalink":"/practical/05_ald/","publishdate":"2022-03-13T12:15:00Z","relpermalink":"/practical/05_ald/","section":"practical","summary":"1. Aggregated level data The file EvSynth.R is a script that can be used to perform the Bayesian analysis of the influenza model, described in class. This can in turn be used to also extend the analysis to the cost-effectiveness evaluation of prophylaxis.","tags":[],"title":"Practical 5. Evidence synthesis and decision models","type":"book"},{"authors":null,"categories":[],"content":" 1. Introduction The data from the smoking cessation studies discussed in the lecture are included in the file smoke.Rdata, which includes a list with the relevant variables. The data include the following variables:\n NS: the total number of studies included in our analysis (24);\n NT: the total number of interventions considered (4);\n na: a vector containing the number of arms included in each of the studies;\n r: a matrix with NS rows and NT columns, containing the number of subjects that in each study and under each treatment arms have been observed to quit smoking;\n n: a matrix with NS rows and NT columns, containing the total number of subjects observed in each study and under each treatment arms;\n t: a matrix with NS rows and 3 columns, identifying the label associated with the treatments included in each of the studies. Notice that there are 3 columns because all studies have at most 3 treatment involved (i.e. all studies compare either 2 or 3 treatments — cfr. the lecture slides). The treatments are labelled as 1 = No intervention; 2 = Self-help; 3 = Individual counselling; 4 = Group counselling.\n  The R script NMA.R guides you through the analysis.\n 2. Fixed effects NMA Save the .Rdata file to your computer and then load the data into your R workspace. Explore the data list and check you understand what the data mean.\n Save the file smokefix_model.txt to your computer. Open it and go through the code, making sure you understand it.\n Follow the R script and run the model calling OpenBUGS in the background, firstly without burn-in iterations.\n Follow the R script and produce traceplots to check convergence.\n Follow the R script and re-run the model, this time monitoring all the relevant parameters.\n   3. Random effects NMA Save the file smokere_model.txt to your computer. Open it and go through the code, making sure you understand it. Inspect particularly the difference with the fixed effects model.\n Follow the R script and run the model calling OpenBUGS in the background. Comment on convergence and compare the output with the fixed effect model. Are there any striking differences?\n Follow the R script and perform the economic analysis, combining the output of your Bayesian Random Effects NMA and using BCEA to post-process the results.\n   4. Visualising heterogeneity and “random” vs “fixed” effects The R script make_plots.R can be used to post-process the data and the outputs from the two models above and produce the visualisations shown in the lecture slides.\nThe script uses the current version of the package bmhe, which includes all the utility functions helpful for the various practicals in the module. If you are on the Binder VM, this is automatically included. If you are on a UCL desktop or your own machine, you need to install it from GitHub, using the following commands.\n# First installs the package \u0026#39;remotes\u0026#39;, which can be used to install packages directly from GitHub install.packages(\u0026quot;remotes\u0026quot;) # Then use it to install \u0026#39;bmhe\u0026#39; remotes::install_github(\u0026quot;giabaio/bmhe_utils\u0026quot;) The code assumes you have downloaded the data and model codes referred above. If you’ve actually run the models in points 2. and 3. you don’t need to run the following lines\n# FIXED EFFECTS MODEL # Initial values inits \u0026lt;- list(list(mu=rep(0,24), d=c(NA,0,0,0)), list(mu=rep(-1,24), d=c(NA,1,1,1))) res \u0026lt;- bugs(model=\u0026quot;smokefix_model.txt\u0026quot;, data=smoke.list, inits=inits, parameters=c(\u0026quot;d\u0026quot;,\u0026quot;or\u0026quot;,\u0026quot;L\u0026quot;,\u0026quot;pq\u0026quot;), n.chains=2, n.burnin=1000, n.iter=20000) and\n# RANDOM EFFECTS MODEL. Check convergence of random effects SD in particular inits \u0026lt;- list(list(mu=rep(0,24), d=c(NA,0,0,0), sd=1), list(mu=rep(-1,24), d=c(NA,1,1,1), sd=2)) res2 \u0026lt;- bugs(model=\u0026quot;smokere_model.txt\u0026quot;, data=smoke.list, inits=inits, parameters=c(\u0026quot;or\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;sd\u0026quot;, \u0026quot;pq\u0026quot;, \u0026quot;L\u0026quot;), n.chains=2, n.burnin=1000, n.iter=20000) from the file make_plots.R: if you’ve already run the script NMA.R, then the objects res and res2 will already be present in your R workspace.\n ","date":1655825400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655825400,"objectID":"f7ec512b2b11f284390e40ad48acf6d6","permalink":"/practical/06_nma/","publishdate":"2022-03-13T15:30:00Z","relpermalink":"/practical/06_nma/","section":"practical","summary":"1. Introduction The data from the smoking cessation studies discussed in the lecture are included in the file smoke.Rdata, which includes a list with the relevant variables. The data include the following variables:","tags":[],"title":"Practical 6. Network meta-analysis","type":"book"},{"authors":null,"categories":[],"content":" Introduction As mentioned in the lecture, the Bayesian analysis has been performed using two different model specifications. The second one assumes a robust prior for each statin-specific effectiveness (in comparison to placebo), in preventing cardiovascular events. This has been achieved by using a Half-Cauchy distribution, instead of a Normal model.\nFollowing the script, load the R datasets contained in the files named respectively statins_base.Rdata and statins_HC.Rdata. These have suitable bugs objects statins_base and statins_HC in which the results of the two model specifications are stored. Use the R command print() on each of them to produce the summary statistics for the nodes that have been monitored.\n Following the script, use BCEA to perform the economic analysis using the output of the two Bayesian~models.\n Following the script, re-arrange the objects in your workspace to create suitable lists, that can be used to perform the PSA to the structural assumptions.  The first one is a list containing the two bugs objects in which the output of the calls to OpenBUGS are stored.\n The second one is a list containing the simulated values from the posterior distributions for the variables of effectiveness from the two models. These are stored in the bugs objects and can be accessed by using the command name_BUGS_object$sims.list$name_variable.\n The third one is a list containing the simulated values from the posterior distributions for the costs from the two models. Type the R command head(costs[[1]]) to visualise the first few values of the first element in the object costs. Type the R commands dim(costs[[1]]) and dim(effects[[1]]), which return the dimensions (i.e. number of rows and number of columns) of the objects costs and effects. Make sure you understand these results.  Following the script, execute the PSA to the structural assumptions assumed above, using the BCEA function struct.psa. Type the R command names(m3), which displays the elements contained in the object m3. Visualise the weights associated with each model by typing the R command m3$w. Which model is the most supported? NB: you can also visualise the DIC associated with each of the models considered by typing the R command m3$DIC.\n Use the element m3$he and the BCEA functions (e.g. plot, ceac.plot, etc.) to visualise the results of the economic analysis on the “average” model (obtained by combining the two different specifications).   ","date":1655892000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655892000,"objectID":"a91b559b9281142239ba29c4d2ad05b3","permalink":"/practical/07_structural/","publishdate":"2022-03-14T10:00:00Z","relpermalink":"/practical/07_structural/","section":"practical","summary":"Introduction As mentioned in the lecture, the Bayesian analysis has been performed using two different model specifications. The second one assumes a robust prior for each statin-specific effectiveness (in comparison to placebo), in preventing cardiovascular events.","tags":[],"title":"Practical 7. PSA to structural uncertainty","type":"book"},{"authors":null,"categories":[],"content":" The file survival_data.Rdata contains a dataset from a randomised trial of a drug which is assumed to target a particular cancer. Data are observed for \\(n=367\\) patients randomised to either the control (\\(n_1=189\\)) or the active treatment (\\(n_2=178\\)) and are stored in the data frame dat.\nThe data report the patients ID; the time of progression to a more severe stage of cancer; an indicator for the event of interest (mortality); an indicator for the treatment arm (coded as 0 = control and 1 = active treatment); an indicator for the patients’ sex (0 = male; 1 = female); the patients’ age (in years); and the Index of Multiple Deprivation (IMD) score (this is a census-based, area-level measure of socio-economic circumstances. It is coded as categorical variable taking values in the interval \\([1;5]\\), where 1 indicates the least deprived and 5 indicates the most deprived areas).\nFollow the R script survival.R to fit the Exponential and the Weibull parametric survival models under a simple MLE approach, using the following specification for the location parameter. \\[g(\\mu_i) = \\log(\\mu_i) = \\beta_0 + \\beta_1\\texttt{arm}_i.\\] Make sure you understand R and survHE notation to define the “formula” specifying the model above.\n Follow the script to explore the output (stored in the object m1).\n Based on the output provided by R , including the graphical representation of the survival curves, what is your preferred model to described the underlying original data? Discuss how you would justify your answer.\n Follow the script to fit the same model under a Bayesian approach and using both INLA and HMC as the inferential engine.\n Compare the output from the models specified using MLE and those obtained under a Bayesian modelling.\n Follow the script to produce a combine plot with all the survival curves. Comment on similarities and differences in the models output.\n Follow the script to perform PSA and produce suitable graphs for the survival curves distributions.\n  ","date":1647255600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1647255600,"objectID":"d6b1e8817f35c902c1768ee44d7f7b7a","permalink":"/practical/08_survival/","publishdate":"2022-03-14T11:00:00Z","relpermalink":"/practical/08_survival/","section":"practical","summary":"The file survival_data.Rdata contains a dataset from a randomised trial of a drug which is assumed to target a particular cancer. Data are observed for \\(n=367\\) patients randomised to either the control (\\(n_1=189\\)) or the active treatment (\\(n_2=178\\)) and are stored in the data frame dat.","tags":[],"title":"Practical 8. Survival analysis","type":"book"},{"authors":null,"categories":[],"content":" 1. Introduction The actual practical considers the example shown in class and it considers a more complex (and perhaps realistic/useful) analysis based on individual level data for the complete “event history”, which we can use to derive the full set of transition probabilities to feed into the Markov model.\nThere is also a tutorial using the example discussed in section 5.4 of Bayesian methods in health economics. This is an example of a “cohort discrete time state transition model” (based on this paper). This example is fairly simple and it is described in all the details (including the distributional assumptions) in BMHE.\nBoth problems share the features of being specified as a “Markov structure”, the need to estimate the relevant transition probabilities and then to use the matrix algebra to “run” the actual economic evaluation using the “Markov model engine”.\n 2. Individual level data on event history and Markov models The data contained in the file data.txt have been recorded on a sample of individuals in a cancer trial. There are two treatment arms (standard of care and innovative treatment) and for each individual, the progression and death indicators as well as the observed times for the two events (or censoring) are recorded.\nThe script MarkovModel2.R will guide you through the analysis; first by formatting the dataset in the format seen in Lecture 09, then running the survival analysis and then creating the approximation to the transition probabilities and, finally, running the Markov model. The script survHE_utils.R contains a few functions that can be used to compute the transition probabilities and then run the Markov model. You should inspect the functions saved in survHE_utils.R and try and understand how they work.\nMake sure you understand the output produced by the various functions and that you can map it out onto the results shown in class.\n ","date":1655910900,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655910900,"objectID":"288fc02600deacb9e40170087f97ac59","permalink":"/practical/09_mm/","publishdate":"2022-03-14T15:15:00Z","relpermalink":"/practical/09_mm/","section":"practical","summary":"1. Introduction The actual practical considers the example shown in class and it considers a more complex (and perhaps realistic/useful) analysis based on individual level data for the complete “event history”, which we can use to derive the full set of transition probabilities to feed into the Markov model.","tags":[],"title":"Practical 9 Markov models in health economic evaluations","type":"book"},{"authors":null,"categories":[],"content":" Missing data The file missing_data.rds contains a dataset with individual level data on two competing treatments. The data include \\(n_1=75\\) and \\(n_2=84\\) records for suitable measures of effectiveness (QALYs, \\(e_i\\)) and total costs (\\(c_i\\)) for each individual. In addition, data on the baseline utility \\(u_i\\) are also recorded for each individual. The data are stored in the list format. The file MissingData.R is an R script to guide you through the analysis.\n Following the script, load the data and inspect the list; for instance, you can use the command hist to produce a histogram of the relevant variables, e.g. hist(data$c[[1]]) (notice that you need to subset the elements of the object data, because they are part of a list. Inspect the file Normal_Normal.txt, which codes up the bivariate Normal model. Make sure you can follow the code and match it with the slides from Lecture 10. Following the script, run the model assuming a marginal Normal model for the effectiveness (controlling for the baseline utility) and a conditional Normal model for the costs (given the effectiveness~variable). Following the script, feed the output from the Bayesian model run in BUGS to BCEA, to perform the economic analysis. Following the script, check the distribution of the imputed values — for example, you can consider the first individual with missing data and the variable of effectiveness as in the script, but you can explore other variables too. Now inspect the file Beta_Gamma.txt, which encodes a Beta marginal model for the QALYs and a Gamma conditional model for the costs, given the effectiveness. Make sure you can follow the code and match it with the slides in the lecture. Following the script, replicate the analysis using this second model and then perform the economic evaluation in BCEA. Check that in this case, the imputed values are within the natural range of the underlying variables (specifically, for the QALYs).   ","date":1624320000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1624320000,"objectID":"5b8f941997de2cee5d035337bc06f031","permalink":"/practical/10_missing/","publishdate":"2021-06-22T00:00:00Z","relpermalink":"/practical/10_missing/","section":"practical","summary":"Missing data The file missing_data.rds contains a dataset with individual level data on two competing treatments. The data include \\(n_1=75\\) and \\(n_2=84\\) records for suitable measures of effectiveness (QALYs, \\(e_i\\)) and total costs (\\(c_i\\)) for each individual.","tags":[],"title":"Practical 10. Missing data","type":"book"},{"authors":null,"categories":[],"content":" Examples  Maternal screening for HIV:\nAdes AE, Cliffe S. Markov Chain Monte Carlo estimation of a multi-parameter decision model: consistency of evidence and the accurate assessment of uncertainty. Medical Decision Making. 2002; 22:359-371.\nWound infection:\nCooper NJ, Sutton AJ, Abrams KR. Decision analytical economic modeling within a Bayesian framework: Application to prophylactic antibiotics use for caesarean section. Statistical Methods in Medical Research. 2002; 11: 491-512.\n  HIV Example: Maternal screening for HIV Ades and Cliffe (2002) conducted a multi-parameter evidence synthesis to compare universal screening of pregnant women for HIV with a strategy that targets high risk groups. The decision tree for the HIV example (used also in the lecture) is shown below. Incremental costs and benefits for universal screening compared with targeted screening of high risk groups is summarised in the decision tree below.\nThe test cost is \\(T=\\mbox{£}3\\), which is assumed fixed and known. The population size of pregnant women is 105,000 per year. All other parameters \\(a, b, e, h,\\) and \\(M\\) have been estimated, and we have PSA simulations available for them (from MCMC).\nWrite out the net-benefit function, and check it agrees with the formula given in the lecture. Open the file hiv.R and follow the script to read the PSA samples (stored in the file hiv150.txt) in and then compute the EVPI. NB: Make sure you specify the correct path to the folder in which you have saved this file! Following the script, compute the Expected Net Benefit and the probability that universal screening is indeed cost-effective. Following the script, check convergence of the EVPI, by computing it repeatedly for increasing sample sizes. Plotting your simulations against iteration number, we would expect the EVPI to settle down. Are you happy convergence has been achieved? How precise would you want to be? What would you do if you are convergence not adequate? Finally, use directly BCEA to compute the EVPI — again, the script can help you identify the relevant~commands).   Wound Infection following Caesarean Section Cooper et al (2002) presented the decision model shown below to assess the cost-effectiveness of using prophylactic antibiotics to prevent wound infections following Caesarean section.\nThe probability of a wound infection without antibiotics was based on a meta-analysis giving the log-odds of infection, \\(\\psi\\). The probability of a wound infection with antibiotics was based on a meta-analysis giving the log-odds ratio, \\(\\phi\\), so that: \\[\\begin{eqnarray*} p(t) = \\left\\{ \\begin{array}{lcl} \\displaystyle\\frac{e^{\\phi+\\psi}}{1+e^{\\phi+\\psi}} \u0026amp;\u0026amp; t=2 \\mbox{ (antibiotics)} \\\\[10pt] \\displaystyle\\frac{e^\\psi}{1+e^\\psi} \u0026amp;\u0026amp; t=1 \\mbox{ (no antibiotics).} \\end{array} \\right. \\end{eqnarray*}\\]\nThe parameters \\(Qwi\\) and \\(Qnwi\\) indicate respectively the QALYs with and without infection. Similarly, the costs are indicated as \\(Cwi\\) and \\(Cnwi\\) with or without infection. The drug cost is \\(Cdrug_t\\), with \\(Cdrug_1=0\\). The net benefit can be then written as \\[ \\mbox{NB}_t(\\boldsymbol\\theta) = k\\left[p(t) Qwi + \\left(1-p(t)\\right)Qnwi\\right] - \\left[p(t) Cwi + \\left(1-p(t)\\right)Cnwi +Cdrug(t)\\right]. \\]\nThe file wounds.txt contains 150,000 PSA simulations for all the model parameters \\[\\boldsymbol\\theta=(Qwi, Qnwi, Cwi, Cnwi, \\boldsymbol{Cdrug}, \\phi, \\psi).\\]\nFollow the script contained in the file wounds.R to read the PSA in, compute the effects and costs under both options. Follow the script and use BCEA to compute the expected net-benefit, probability that antibiotics are cost-effective, and 1-year EVPI per woman, for a range of willingness to pay values from £ 1,000 to £ 40,000 (use ?bcea to help you with syntax). What are the values when \\(k=\\) £ 20,000? Use the BCEA commands ceac.plot and evi.plot to produce a CEAC and a plot of EVPI over different values of the willingness to pay. Comment on what these graphs show. There were 166,081 cesearean sections in England and Wales in 2013-14. Assuming this is constant over time, compute the population EVPI for a 10-year lifetime of technology (discounted to 7.7217). What is the population EVPI when \\(k=\\) £ 20,000? Advanced: Code the EVPI yourself for \\(k=\\) £ 20,000, so that you can check convergence. Check the results agree with those from BCEA. Hint 1: You have already computed effects and costs, so easy to compute net-benefit. Hint 2: After computing net-benefit, follow the code used for the HIV example.    ","date":1654819200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1654819200,"objectID":"f57dc69ec0ce88caddf326677532c6d4","permalink":"/practical/11_intro_voi/","publishdate":"2022-06-10T00:00:00Z","relpermalink":"/practical/11_intro_voi/","section":"practical","summary":"Examples  Maternal screening for HIV:\nAdes AE, Cliffe S. Markov Chain Monte Carlo estimation of a multi-parameter decision model: consistency of evidence and the accurate assessment of uncertainty. Medical Decision Making.","tags":[],"title":"Practical 11: Computing the EVPI using nested Monte Carlo simulations","type":"book"},{"authors":null,"categories":[],"content":"  Relevant literature  Partial EVPI:\nStrong M, Oakley JE, Brennan A. Estimating multi-parameter partial Expected Value of Perfect Information from a probabilistic sensitivity analysis sample: a non-parametric regression approach Medical Decision Making. 2014; 34(3):311-26.\nReview of methods for the computation of the EVPPI\nHeath A, Manoloupolou I, Baio G. A Review of Methods for the Analysis of the Expected Value of Information. Medical Decision Making. 2017\nComputing the EVPPI using INLA\nHeath A, Manolopoulou I, Baio G. Estimating the expected value of partial perfect information in health economic evaluations using Integrated Nested Laplace Approximation. Statistics in Medicine. 2016;\n  Case study 1 - Model description The model for the first part of this practical is based on the hypothetical three-state Markov model used for illustrative purposes in Strong et al (2013).\nThe model predicts the net benefit, \\(\\mbox{NB}_t(\\boldsymbol\\theta)\\), under two decision options (\\(t=1,2\\)), over a time horizon of 20 time cycles — as usual, \\(\\boldsymbol\\theta\\) is the vector of all model parameters. Patients are assumed to be in one of three states: responding to treatment, not responding, or dead. The figure below illustrates that all transitions are allowed except out of the absorbing state, dead.\nThe model can be written as \\[\\begin{align} \\mbox{NB}_1(\\boldsymbol\\theta) \u0026amp;= \\left\\{\\sum_{n=1}^{20} \\left(\\boldsymbol{S}_1^{\\text T} \\boldsymbol{M}_1^{n} \\boldsymbol{U}_1 \\right) +\\theta_{8}\\theta_{9}\\theta_{10}\\right\\} K -(\\theta_{1} + \\theta_{2}\\theta_{3}\\theta_{4}),\\\\ \\mbox{NB}_2(\\boldsymbol\\theta)\u0026amp;= \\left\\{\\sum_{n=1}^{20} \\left(\\boldsymbol{S}_2^{\\text T} \\boldsymbol{M}_2^{n} \\boldsymbol{U}_2 \\right)+\\theta_{17}\\theta_{18}\\theta_{19}\\right\\}K- (\\theta_{11}+ \\theta_{12}\\theta_{13}\\theta_{4}), \\end{align}\\] where the vectors \\(\\boldsymbol{S}_t\\) and \\(\\boldsymbol{U}_t\\) are defined as \\(\\boldsymbol{S}_1=(\\theta_5,1-\\theta_5,0)^{\\text T}\\), \\(\\boldsymbol{S}_2=(\\theta_{14},1-\\theta_{14},0)^{\\text T}\\), \\(\\boldsymbol{U}_1=(\\theta_6,0,0)^{\\text T}\\) and \\(\\boldsymbol{U}_2=(\\theta_{15},0,0)^{\\text T}\\) and where each parameter \\(\\phi\\) is defined in the Table below. The value of one unit of health benefit, \\(K\\), is £10,000 / QALY. The transition matrices \\(\\boldsymbol{M}_1\\) and \\(\\boldsymbol{M}_2\\) are defined as \\[\\begin{equation} \\boldsymbol{M}_1 = \\left( \\begin{array}{ccc} \\theta_{20}\u0026amp;\\theta_{21}\u0026amp;\\theta_{22}\\\\ \\theta_{23}\u0026amp;\\theta_{24}\u0026amp;\\theta_{25}\\\\ 0\u0026amp;0\u0026amp;1 \\end{array}\\right) \\; \\mathrm{and}\\; \\boldsymbol{M}_2= \\left( \\begin{array}{ccc} \\theta_{26}\u0026amp;\\theta_{27}\u0026amp;\\theta_{28}\\\\ \\theta_{29}\u0026amp;\\theta_{30}\u0026amp;\\theta_{31}\\\\ 0\u0026amp;0\u0026amp;1 \\end{array}\\right). \\end{equation}\\]\n  Parameter   Mean (sd)      \\(t=0\\)   \\(t=1\\)       Drug Cost (£, \\(\\theta_{1},\\theta_{11}\\))  1000 (1)  1500 (1)    Probability of Admission (\\(\\theta_2,\\theta_{12}\\))  0.1 (0.02)  0.08 (0.02)    Days in Hospital (\\(\\theta_3,\\theta_{13}\\))  5.2 (1)  6.1 (1)    Hospital Cost per Day (£, \\(\\theta_4\\))  400 (200)  400 (200)    Probability of Responding (\\(\\theta_5,\\theta_{14}\\))  0.7 (0.1)  0.8 (0.1)    Utility Change due to Response (\\(\\theta_6,\\theta_{15}\\))  0.3 (0.1)  0.3 (0.05)    Duration of Response (years, \\(\\theta_7,\\theta_{16}\\))  3 (0.5)  3 (1)    Probability of Side Effects (\\(\\theta_8,\\theta_{17}\\))  0.25 (0.1)  0.2 (0.05)    Utility Change due to Side Effects (\\(\\theta_9,\\theta_{18}\\))  -0.1 (0.02)  -0.1 (0.02)    Duration of Side Effects (years, \\(\\theta_{10},\\theta_{19}\\))  0.5 (0.2)  0.5 (0.2)      Overall EVPI Start R Start R and set the working directory to the folder that contains the course materials.\nNote that the symbol # comments out code in R, i.e. anything following a # character in the code chunks below will not be run.\n Read values in R The model has been run and a PSA of size 10,000 stored in the .csv file psa_model_1.csv. You can read this into R with:\npsa \u0026lt;- read.csv(\u0026quot;psa_model_1.csv\u0026quot;) head(psa) dim(psa) In this model, effects are measured in QALYs, and costs in £. Define \\(K\\), the value of one QALY as £\\(10,000\\). Calculate net benefits and the incremental net benefit.\nK \u0026lt;- 10000 nb1 \u0026lt;- psa$effects1 * K - psa$costs1 nb2 \u0026lt;- psa$effects2 * K - psa$costs2 inb \u0026lt;- nb2 - nb1 Examine the mean net benefits and mean incremental net benefit.\nmean(nb1) mean(nb2) mean(inb)  Calculate overall EVPI The overall EVPI is given by \\[\\begin{equation} \\mbox{EVPI} = \\mbox{E}_\\theta\\left[ \\max_t \\mbox{NB}_t(\\boldsymbol\\theta)\\right] - \\max_t \\mbox{E}_\\theta \\left[\\mbox{NB}_t(\\boldsymbol\\theta)\\right], \\end{equation}\\] where \\(\\mbox{NB}_t(\\boldsymbol\\theta)\\) is the model function, and \\(\\boldsymbol\\theta = (\\theta_1, \\ldots, \\theta_{31})\\). This has Monte Carlo estimator \\[\\begin{equation} \\widehat{\\mbox{EVPI}} = \\frac{1}{N}\\sum_{n=1}^N \\max_t \\mbox{NB}_t(\\boldsymbol\\theta^{(n)}) - \\max_t \\frac{1}{N}\\sum_{n=1}^N \\mbox{NB}_t(\\boldsymbol\\theta^{(n)}), \\end{equation}\\] where \\(\\boldsymbol\\theta^{(n)}\\) is a draw from the joint distribution \\(p(\\boldsymbol\\theta)\\).\nWe can evaluate this Monte Carlo estimator for EVPI in R using the following expression:1\nEVPI \u0026lt;- mean(pmax(nb1, nb2)) - max(mean(nb1), mean(nb2)) EVPI # equivalently, we can calculate EVPI using the incremental net benefit. # This is useful later when we use regression to estimate EVPPI. EVPI \u0026lt;- mean(pmax(0, inb)) - max(0, mean(inb)) EVPI   Partial EVPI via regression Firstly, we’ll show how partial EVPI for a single parameter is calculated in a simple two decision option problem. We’ll use a generalised additive model, implemented in the mgcv R package.\nWe’ll calculate partial EVPI for \\(\\theta_5\\). First we need to load the mgcv library2 that contains the gam function, and then run the GAM model:\nlibrary(mgcv) theta \u0026lt;- psa[,1:31] model \u0026lt;- gam(inb ~ s(theta[, 5])) Here, we also define the matrix theta, which contains all the simulations for the model parameters (stored in the first 31 columns of the matrix psa). When using the s() syntax the default spline type is the thin plate regression' spline. See?s` for more details.\nCheck the model residuals\nplot(fitted(model), residuals(model)) and more visual checks are generated by gam.check.\ngam.check(model) As you can see, this function produces some output relating to the optimisation process and the model fit. The function also produces a series of diagnostic plots. The most critical is the upper right, which should be a mean zero scatter of plots with no obvious structure. We can tolerate a degree of heteroskedasticity since this should not bias the model fitted values. Normality of residuals is less critical (top left plot). For more information see the help pages via typing ?gam.check.\nWe then extract the model fitted values using fitted(model) and then calculate partial EVPI via\nfittedValues \u0026lt;- fitted(model) evppi_theta5 \u0026lt;- mean(pmax(0, fittedValues)) - max(0, mean(fittedValues)) evppi_theta5 Partial EVPI of each parameter in turn If we wanted to estimate the partial EVPI of each parameter in turn we could do this by writing a for loop.\n# initialise a vector to hold the results. evppi_vector \u0026lt;- numeric(31) for (i in (1:31)[-c(7, 16)]) { # theta 7 and 16 are not defined print(i) parameter_of_interest \u0026lt;- theta[, i] model \u0026lt;- gam(inb ~ s(parameter_of_interest)) fittedValues \u0026lt;- fitted(model) evppi_vector[i] \u0026lt;- mean(pmax(0, fittedValues)) - max(0, mean(fittedValues)) } names(evppi_vector) \u0026lt;- paste(\u0026quot;theta\u0026quot;, 1:31) print(round(evppi_vector[-c(7, 16)], 1)) We can put this on a barchart with\nx.points \u0026lt;- barplot(evppi_vector, ylab = \u0026quot;partial EVPI\u0026quot;, xaxt = \u0026quot;n\u0026quot;, ylim = c(0, 500)) axis(side = 1, at = x.points, labels = names(evppi_vector), tick = FALSE, hadj = 0.8, las = 3) The syntax for this plot is not straightforward if you are unfamiliar with R. One of the great things about R is the flexibility in creating plots, but the flexibility comes at a cost in terms of complexity of syntax.\n Partial EVPI of groups of parameters Now we will calculate partial EVPI of groups of parameters. First, let’s calculate EVPI for \\(\\theta_5\\) and \\(\\theta_{14}\\). Note that we now include the parameters within the te() smoothing function (this function generates a tensor product of marginal smooth functions and allows for interactions between the parameters). The problem with this is that struggles to cope with more than about five parameters within the te() function (see lecture slides).\nmodel \u0026lt;- gam(inb ~ te(theta[, 5], theta[, 14])) fittedValues \u0026lt;- fitted(model) mean(pmax(0, fittedValues)) - max(0, mean(fittedValues)) Check the model residuals.\ngam.check(model) Now, let’s calculate EVPI for the group \\((\\theta_{5}\\), \\(\\theta_{6}\\), \\(\\theta_{14}, \\theta_{15})\\). Even with four parameters it is quite slow.\nsystem.time(model \u0026lt;- gam(inb ~ te(theta[, 5], theta[, 6], theta[, 14], theta[, 15]))) fittedValues \u0026lt;- fitted(model) mean(pmax(0, fittedValues)) - max(0, mean(fittedValues)) gam.check(model) Some alternative non-parametric regression approaches include neural networks, projection pursuit regression, Gaussian process regression and multivariate adaptive regression splines (MARS). A fast and efficient implementation of MARS regression is available in the earth package in R3\nlibrary(earth) # theta 5, 6, 14, 15 system.time(model \u0026lt;- earth(theta[, c(5, 6, 14, 15)], inb, degree = 10, nk = 1000, thres = 1e-3)) fittedValues \u0026lt;- fitted(model) mean(pmax(0, fittedValues)) - max(0, mean(fittedValues)) plot(fitted(model), residuals(model)) # theta 20 to theta 31 system.time(model \u0026lt;- earth(theta[, 20:31], inb, degree = 10, nk = 1000, thres = 1e-3)) fittedValues \u0026lt;- fitted(model) mean(pmax(0, fittedValues)) - max(0, mean(fittedValues)) plot(fitted(model), residuals(model)) Try some different groups.\n  Partial EVPI if there are more than two decision options Now let’s imagine we have three decision options rather than two. The file psa_model_2.csv contains a set of 10,000 PSA samples from a model for a three-decision option problem. In the file are 19 input parameters, and costs and effects for each of the three decision options.\nImport the PSA sample and create objects for costs, effects and the parameters.\n# first remove the objects from the previous analysis, to be safe... rm(theta, nb1, nb2, inb, model, psa) psa \u0026lt;- read.csv(\u0026quot;psa_model_2.csv\u0026quot;) head(psa) dim(psa) costs \u0026lt;- psa[, grep(\u0026quot;costs\u0026quot;, names(psa))] # grep matches text strings effects \u0026lt;- psa[, grep(\u0026quot;effects\u0026quot;, names(psa))] theta \u0026lt;- psa[, grep(\u0026quot;theta\u0026quot;, names(psa))] dim(costs) dim(effects) dim(theta) K \u0026lt;- 10000 # set K to 10000 again Calculate absolute net benefits and incremental net benefits versus option 1.\nnb \u0026lt;- effects * K - costs inb \u0026lt;- nb - nb[, 1] colMeans(nb) colMeans(inb) Which is now the best option, and what is the overall EVPI?\nWe use the function pmax again to find the maximum net benefit for each row of the PSA. The do.call allows us to use the function pmax with an arbitrary number of columns (here, there are three). It is very fast and efficient, but don’t worry about the syntax (unless you are interested!).\ninb \u0026lt;- as.data.frame(inb) # convert to data.frame so do.call works mean(do.call(pmax, inb)) - max(colMeans(inb)) For a single parameter partial EVPI (e.g. for \\(\\theta_5\\)) we now have two incremental net benefits, and therefore we need two GAM regression models. We therefore do:\nmodel_2v1 \u0026lt;- gam(inb[, 2] ~ s(theta[, 5])) model_3v1 \u0026lt;- gam(inb[, 3] ~ s(theta[, 5])) fittedValues \u0026lt;- data.frame( fittedValue_1v1 = 0, # note that the incremental NB of option 1 vs 1 is zero fittedValue_2v1 = fitted(model_2v1), fittedValue_3v1 = fitted(model_3v1) ) mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) Calculate partial EVPI for all 19 input parameters.\nNext, try some groups.\n# first with gam model_2v1 \u0026lt;- gam(inb[, 2] ~ te(theta[, 5], theta[, 6], theta[, 7])) model_3v1 \u0026lt;- gam(inb[, 3] ~ te(theta[, 5], theta[, 6], theta[, 7])) fittedValues \u0026lt;- data.frame( fittedValue_1v1 = 0, # note that the incremental NB of option 1 vs 1 is zero fittedValue_2v1 = fitted(model_2v1), fittedValue_3v1 = fitted(model_3v1) ) mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) # or use earth model_2v1 \u0026lt;- earth(theta[, c(5, 6, 7)], inb[, 2], degree = 10, nk = 1000, thres = 1e-3) model_3v1 \u0026lt;- earth(theta[, c(5, 6, 7)], inb[, 3], degree = 10, nk = 1000, thres = 1e-3) fittedValues \u0026lt;- data.frame( fittedValue_1v1 = 0, # note that the incremental NB of option 1 vs 1 is zero fittedValue_2v1 = fitted(model_2v1), fittedValue_3v1 = fitted(model_3v1) ) mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) Try some other groups.\n Using BCEA to calculate EVPPI First, we need to load BCEA to the R workspace.\nlibrary(BCEA) Then, we create a bcea object from the costs and QALYs.\nbceaObject \u0026lt;- bcea(e = as.matrix(effects), c = as.matrix(costs), ref = 1, wtp = 10000, plot = FALSE) # note that the bcea function requires matrices as inputs, not data frames! Check out the help page ?bcea to see what the various options are.\nCalculate EVPPI for \\(\\theta_5\\), including checking the residual plots.\n\u0026gt; evppiObject \u0026lt;- evppi(5, theta, bceaObject) \u0026gt; diag.evppi(evppiObject, bceaObject, int = 1) \u0026gt; diag.evppi(evppiObject, bceaObject, int = 2) \u0026gt; evppiObject$evppi Check out the help page ?evppi to see all the various options. Try a range of different parameters.\nBefore we calculate groups we are going to reduce the PSA size to avoid lengthy computations in the practical session.\nCreate a new bcea object from the costs and QALYs.\n\u0026gt; bceaObject \u0026lt;- bcea(e = as.matrix(effects[1:1000, ]), c = as.matrix(costs[1:1000, ]), ref = 1, wtp = 10000, plot = FALSE) Calculate EVPPI for the group \\(\\theta_5, \\theta_6, \\theta_7\\) via the GAM method.\n\u0026gt; evppiObject \u0026lt;- evppi(c(5, 6, 7), theta[1:1000, ], bceaObject, method = \u0026quot;GAM\u0026quot;) \u0026gt; diag.evppi(evppiObject, bceaObject, int = 1) \u0026gt; diag.evppi(evppiObject, bceaObject, int = 2) \u0026gt; evppiObject$evppi The GAM method will only work for groups of up to five inputs.\n Web-apps for value of information calculations Regression-based methods for calculating EVPPI are currently implemented in two user-friendly web applications: Gianluca Baio and Anna Heath’s BCEAweb, and Mark Strong’s SAVI. In each case the PSA sample is uploaded to the web-app, and server-side R code is used to generate model summaries and value of information results. The results can then be downloaded in a report.\nThere any many similarities between the applications. Two key differences are that SAVI is available to download as a stand alone web-app that can be run locally on your own machine, whereas BCEAweb has functionality for checking regression diagnostics.\n Using SAVI / BCEAweb to calculate EVPPI We’ve included instructions for using SAVI in this practical, but you could equally use BCEAweb. There are csv files containing PSA results in the right format for upload to SAVI and BCEAweb in the course materials.\nDownload local version of SAVI To avoid overloading the version of SAVI hosted on the web, download SAVI for your own machine (BCEAweb should be better able to cope with multiple concurrent users). First install the devtools package, which is then used to download and install the SAVI package from GitHub.\ninstall.packages(\u0026#39;devtools\u0026#39;) library(devtools) install_github(\u0026#39;Sheffield-Accelerated-VoI/SAVI-package\u0026#39;) Windows users may be prompted to install Rtools. You do not need Rtools to install SAVI. You may be prompted to install other package dependencies.\nThe SAVI report download feature requires Pandoc, which can be downloaded and installed from http://pandoc.org/. Pandoc is not essential for the rest of SAVI to work.\nTo start SAVI, load the library and type SAVI().\nlibrary(SAVI) SAVI() SAVI should then open in your machine’s default web browser. Any analysis you do is processed entirely on your own machine. Once downloaded, the R package version of SAVI does not require an internet~connection.\n Prepare PSA files SAVI requires three files that contain the results of the PSA analysis: a parameter file that records the sampled values of all the input parameters of the model, a file that contains the costs for each decision option, and a file that contains the health effects (typically QALYs).\nEach file must be in csv format. SAVI assumes that the first row of the parameter file contains the parameter names. SAVI assumes that the first row of the costs file holds the decision option names. The first row of the effects file should also hold names, but these names are not used by SAVI.\nThe .csv files must each have the same number of rows, and the rows must correspond, i.e. the parameters in row 1 must be those that correspond to the costs and effects in row 1, and so on.\nCosts and effects are assumed to be per-person, and to be absolute rather than incremental (i.e. there must be the same number of columns as decision options, including the baseline decision).\n The “About Your Model” tab The “About Your Model” tab allows users to set the following:\n Model name The threshold value of one unit of health effect, \\(\\lambda\\) (often indicated by \\(k\\), in the rest of the material) — defults to 20,000 Definition of effectiveness measure (e.g. discounted lifetime QALYs) Definition of cost measure (e.g. discounted lifetime £) Annual prevalence (for calculation of population EVPI / EVPPI) Time horizon (for calculation of population EVPI / EVPPI) Currency units used for costs (e.g. £) Units used for benefits (e.g. QALYs) Name of jurisdiction (e.g. country, region, city)  With the exception of \\(\\lambda\\), these settings only affect the labels on figures and in tables on the SAVI web page and in the SAVI downloadable report. Changing \\(\\lambda\\) during a SAVI session will cause the PSA, EVPI, and single parameter EVPI results to update. Group EVPI results do not update at present.\n The “Import Files” tab Upload the three files on this tab: parameters, cost and effects.\nThis tab also contains download buttons for three test files. These test files were generated from the same model that was used as case study 1 in our partial EVPI paper in Medical Decision Making.\n The “Check Upload” tab The first five rows of each dataset are shown here. On the list of things to do is to add more detail about each file (number of columns and number of rows).\n The “PSA Results” tab Clicking on this tab will cause SAVI to calculate a range of summary measures and construct a number of figures (the CE plane, CEAC and net benefit density plots).\nThe user has the option to select the intervention and the comparator for display on the cost-effectiveness plane.\nSummary tables can be downloaded as .csv files.\nAll tables and figures appear in the SAVI report, available from the “Report” tab.\n The “EVPI” tab Overall EVPI is calculated, both in cost units and health effect units. Figures showing EVPI against \\(\\lambda\\) are constructed.\n The “EVPI Single Parameters” tab Clicking this tab tells SAVI to calculate the partial EVPI for each parameter, along with the standard error of the approximation. The standard error relates to the sampling uncertainty due to a finite PSA sample. Increasing the number of rows in the PSA will reduce the standard errors.\n The “EVPI Groups” tab Groups of parameters can be selected using this tab. For groups of up to four parameters GAM regression is used to compute partial EVPI. For groups of five or more parameters GP regression is used. There is more detail about these methods in our MDM paper.\nNote that GP regression can be relatively slow (a few minutes) compared with the GAM regression. The larger the number of parameters in the group, the slower the computation. The larger the number of PSA samples, again, the slower the computation. At present the GP regression method uses only the first 7,500 rows4 of the PSA due to computational constraints5.\n The “Report” tab A report that contains all the tables and figures from SAVI can be downloaded using the download button on this tab. The options for download format are .pdf, .html and .doc for the online version of SAVI, and .html and .doc for the downloadable version of SAVI.\nAs mentioned above, Pandoc is requred for the download feature to work.\n   To learn about pmax type ?pmax at the R console.↩︎\n A library only needs to be loaded once during each R session↩︎\n The term “MARS” is trademarked, hence “earth”!↩︎\n 5,000 rows on the local version of SAVI.↩︎\n The need to invert an \\(n\\) x \\(n\\) matrix where \\(n\\) is the number of rows in the PSA.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"8ded6a794db03755569c53bfe430f03c","permalink":"/practical/12_evppi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/12_evppi/","section":"practical","summary":"Relevant literature  Partial EVPI:\nStrong M, Oakley JE, Brennan A. Estimating multi-parameter partial Expected Value of Perfect Information from a probabilistic sensitivity analysis sample: a non-parametric regression approach Medical Decision Making.","tags":[],"title":"Practical 12: Computing the EVPPI in BCEA and SAVI","type":"book"},{"authors":null,"categories":[],"content":" Relevant literature  Tutorial paper:\nHeath, A., Strong, M., Glynn, D., Kunst, N., Welton, N., Goldhaber-Fiebert, J. Simulating Study Data to Support Expected Value of Sample Information Calculations: A Tutorial. Medical Decision Making. 2022\n This practical focuses on simulating different study outcomes for EVSI calculations, the PSA distributions for this analysis are provided in the code for this practical. The full R script is available here.\n Exercise 1: Survival Data The transition probability \\(\\theta_3\\) represents the probability that a patient’s cancer progresses within a 1-month period on the current standard treatment.\ntheta_3 \u0026lt;- runif(1000, 0.2, 0.3) # Hypothetical distribution for theta_3  Assuming that the rate of progression is constant over time, we can simulate time-to-progression data from an exponential distribution with rate, \\(r = -\\log(1 - \\theta_3)\\). Using the loop structure from the lecture, generate 1000 datasets which record the time to progression 100 individuals. Use an exponential distribution for the data generating process rexp.\n Alternative survival distributions are also available. Assume that our decision-analytic model is a partitioned survival model with a Weibull distribution estimating progression-free survival times for the current standard treatment and parameterized in terms of \\(\\theta_4\\) and \\(\\theta_5\\). Uncertainty in \\(\\theta_4\\) and \\(\\theta_5\\) is represented by the joint distribution \\(p(\\theta_4, \\theta_5)\\).  # Hypothetical distribution for theta_4 and theta_5 theta_4_5 \u0026lt;- MASS::mvrnorm( S,c(5,6),matrix(c(0.3, 0.1, 0.1, 0.5), nrow = 2) )  Generate 1000 datasets which record the time to progression 100 individuals. Use a Weibull distribution for the data generating process rweibull\n Assume that each of the previous studies will stop after two years of follow up and all individuals who experience the event after two years will be censored. Censoring can be generated using a “censoring indicator” that is 0 if the data point is censored and 1 if it is not. Data that are censored must then be changed to be equal to the censoring time of 6 months. Generate a censored dataset for both of the previous questions.\n   Exercise 2: Utility Data \\(\\theta_6\\) is the mean utility for the preprogression state and uncertainty about \\(\\theta_6\\) is encoded in a beta prior distribution \\(p(\\theta_6)\\).\ntheta_6 \u0026lt;- rbeta(1000, 70, 15) # Hypothetical distribution for theta_3 We will collect data in a future study where the individual-level variance \\(v = 0.04\\). Generate 1000 datasets which record the utility for 100 individuals. To simulate these data, the mean \\(\\theta_6\\) and variance \\(v\\) must be translated into the parameters of the beta distribution, which we achieve using the function calculate_beta_parameters.\ncalculate_beta_parameters \u0026lt;- function(mean, sd){ # Function to estimate beta parameters from mean and standard deviation shape1 \u0026lt;- ((1 - mean) / sd ^ 2 - 1 / mean) * mean ^ 2 shape2 \u0026lt;- shape1 * (1 / mean - 1) # Return the calculated parameters. return(list(shape1 = shape1,shape2 = shape2)) }  We assume that individuals with a longer time in pre-progression also have higher utilities in the pre-progression state, implying that there is a positive correlation of 0.3 between their time of progression and their utility. Use the SimJoint package to generate correlated study data for a study collecting time to progression and preprogression utility.    ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c88b77af22de0fdcde5348f49e0f2bc9","permalink":"/practical/13_data_evsi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/13_data_evsi/","section":"practical","summary":"Relevant literature  Tutorial paper:\nHeath, A., Strong, M., Glynn, D., Kunst, N., Welton, N., Goldhaber-Fiebert, J. Simulating Study Data to Support Expected Value of Sample Information Calculations: A Tutorial.","tags":[],"title":"Practical 13: Generating data for EVSI","type":"book"},{"authors":null,"categories":[],"content":"  Relevant literature  Computing the EVSI using Moment Matching\nHeath A, Manoloupolou I, Baio G. Efficient Monte Carlo Estimation of the Expected Value of Sample Information Using Moment Matching. Medical Decision Making. 2018\nHeath A, Baio G. Calculating the Expected Value of Sample Information using Efficient Nested Monte Carlo: A Tutorial. Accepted: Value in Health. 2018\nHeath A, Manoloupolou I, Baio G. Estimating the Expected Value of Sample Information across Different Sample Sizes using Moment Matching and Non-Linear Regression. Medical Decision Making 2019\nThe Example\nMenzies N. An Efficient Estimator for the Expected Value of Sample Information. Medical Decision Making 2017\n This practical uses the EVSI package and to perform and present the results of an EVSI analysis using the nested Monte Carlo (Moment Matching Method). The code to compute the EVSI is provided in the course folder in the EVSI-calculate.R file.\n A Health Economic Decision Model We briefly introduce the health economic decision model for which we will compute EVSI. The model has two potential treatments, a novel treatment or the current standard care. For each treatment, a patient can either respond to the treatment or experience side effects and potentially visit hospital for a certain length of time. A utility value is assigned to each of these possible outcomes and costs are associated with the drugs and hospital stays. The novel treatment has an increased chance of responding to treatment, avoiding hospitalisation and a reduced chance of side effects but has a higher cost. All the model parameters are assumed to be normal with the mean and standard deviation given in Table ??. The sampling distributions for the future data collection are also assumed to be normal distributions, with the standard deviations given in the table below.\nWe assume that the parameters \\(\\theta_5, \\theta_7,\\theta_{14}\\) and \\(\\theta_{16}\\) are correlated with correlation coefficient 0.6 and the parameters \\(\\theta_6\\) and \\(\\theta_{15}\\) are also correlated with a correlation coefficient 0.6 and independent of the other set of parameters. This means that we use a multivariate normal distribution in our Bayesian model.\n  Parameter   Mean (sd)      \\(t=0\\)   \\(t=1\\)       Drug Cost (£, \\(\\theta_{1},\\theta_{11}\\))  10000 (10)  15000 (10)    Probability of Admission (\\(\\theta_2,\\theta_{12}\\))  0.1 (0.02)  0.08 (0.02)    Days in Hospital (\\(\\theta_3,\\theta_{13}\\))  5.2 (1)  6.1 (1)    Hospital Cost per Day (£, \\(\\theta_4\\))  4000 (2000)  4000 (2000)    Probability of Responding (\\(\\theta_5,\\theta_{14}\\))  0.7 (0.1)  0.8 (0.1)    Utility Change due to Response (\\(\\theta_6,\\theta_{15}\\))  0.3 (0.1)  0.3 (0.05)    Duration of Response (years, \\(\\theta_7,\\theta_{16}\\))  3 (0.5)  3 (1)    Probability of Side Effects (\\(\\theta_8,\\theta_{17}\\))  0.25 (0.1)  0.2 (0.05)    Utility Change due to Side Effects (\\(\\theta_9,\\theta_{18}\\))  -0.1 (0.02)  -0.1 (0.02)    Duration of Side Effects (years, \\(\\theta_{10},\\theta_{19}\\))  0.5 (0.2)  0.5 (0.2)     The effectiveness for the treatments are calculated from a decision tree model as: \\[e_1 = \\theta_5\\theta_6\\theta_7 + \\theta_8\\theta_9\\theta_{10} \\]\\[e_2 = \\theta_{14}\\theta_{15}\\theta_{16} +\\theta_{17}\\theta_{18}\\theta_{19}\\] and the costs as: \\[c_1 = \\theta_1 +\\theta_2\\theta_3\\theta_4\\] \\[c_2=\\theta_{11}+\\theta_{12}\\theta_{13}\\theta_4. \\]\nTo calculate the EVSI, we consider collecting two data points for each patient which would gather additional information about \\(\\theta_5\\) and \\(\\theta_{14}\\). Here, the sampling distribution for the data is a multivariate normal distribution, with the mean conditional on the parameters of interest, with correlation 0.6 and an individual-level marginal variance 0.2.\n Calculating the EVSI using the EVSI package The first step is to define the Bayesian Model for the model parameters and the sampling distribution of the data. All the parameters have normal distributions so are parameterised in terms of mean and precision which has to be calculated from the standard deviations shown in the table. Look carefully at how multi-variate normal priors are defined, this contrasts with the multivariate normal for the sampling distribution of the data which has to be defined using the conditional and marginal distributions.\nModel\u0026lt;-function(){ #PSA distributions for the parameters #All parameters are normal distributions #Parameterised in terms of mean and precision theta.1~dnorm(10000,0.01) theta.11~dnorm(15000,0.01) theta.2~dnorm(0.1,2500) theta.12~dnorm(.08,2500) theta.3~dnorm(5.2,1) theta.13~dnorm(6.1,1) theta.4~dnorm(4000,2.5E-07) theta.8~dnorm(.25,100) theta.17~dnorm(.20,400) theta.9~dnorm(-.1,2500) theta.18~dnorm(-.1,2500) theta.10~dnorm(.5,25) theta.19~dnorm(.5,25) #Correlated PSA distributions requires multivariate normal distribution #Multivariate normal parameters are specified as data. #Parameterised in terms of a mean vector and a precision matrix cor.theta.1[1:4]~dmnorm(Mu.1[],Tau.1[,]) theta.5\u0026lt;-cor.theta.1[1] theta.7\u0026lt;-cor.theta.1[3] theta.14\u0026lt;-cor.theta.1[2] theta.16\u0026lt;-cor.theta.1[4] cor.theta.2[1:2]~dmnorm(Mu.2[],Tau.2[,]) theta.6\u0026lt;-cor.theta.2[1] theta.15\u0026lt;-cor.theta.2[2] ##Sampling distribution for the data #Sampling from a multivariate normal distribution #Within EVSI package - cannot have multivariate data collection #Decomposition from BMHE Baio (2012), Page 156 tau.14\u0026lt;-tau.X.14/(1-rho.X*rho.X) for(i in 1:N){ X.5[i]~dnorm(theta.5,tau.X.5) mu.14[i]\u0026lt;-theta.14+sqrt(tau.X.5/tau.X.14)*rho.X*(X.5[i]-theta.5) X.14[i]~dnorm(mu.14[i],tau.14) } } #This writes the model as a text file for use within BUGS filein.model \u0026lt;- \u0026quot;Model_File.txt\u0026quot; write.model(Model,filein.model)  Next, we define parameters for the multivariate normal priors. These must be defined as matrices and vectors in R and so cannot be directly included in the model file (as we did for the scalar quantities). Vectors in R are defined using the c(...) function, whilst matrices use the matrix(...) function. As normal distributions must be defined using a precision matrix, we use the function solve(...) to find the matrix inverse of the covariance matrix.\n Following this, we need to compute the costs and effects for each of the treatments, conditional on these model parameters. To use the EVSI package, we must create functions that calculate the costs and effects using the formulas stated above:\neffects\u0026lt;-function(theta.5,theta.6,theta.7,theta.8,theta.9,theta.10, theta.14,theta.15,theta.16,theta.17,theta.18,theta.19){ #This is a decision tree model so the effects are in the sum/product form. e.treatment1\u0026lt;-(theta.5*theta.6*theta.7)+(theta.8*theta.9*theta.10) e.treatment2\u0026lt;-(theta.14*theta.15*theta.16)+(theta.17*theta.18*theta.19) e\u0026lt;-c(e.treatment1,e.treatment2) return(e) } costs\u0026lt;-function(theta.1,theta.2,theta.3,theta.4, theta.11,theta.12,theta.13){ #This is a decision tree model so the costs are in the sum/product form. c.treatment1\u0026lt;-(theta.1+theta.2*theta.3*theta.4) c.treatment2\u0026lt;-(theta.11+theta.12*theta.13*theta.4) c\u0026lt;-c(c.treatment1,c.treatment2) return(c) } Here, we can see the R syntax for creating two functions called effects and costs. Firstly, we start with the command function. Within the rounded brackets () the names of the variables we are going to use in the function are listed, in this case these are the names of the parameters we defined in the Bayesian model. Within the fancy brackets {}, we list all the calculations that should be performed. Here, we are calculating the costs or effects for each treatment option and then saving them as c or e respectively. Finally, the command return(...) indicates which of the calculated elements should be returned by the function, in this case, we only wish to return c or e and not the intermediate calculations i.e. e.treatment1.\n To calculate the EVSI, we need to create a list of the data for our Bayesian model, which are the matrices and vectors that define our multivariate normal priors and the parameters for the sampling distribution of the data.\n We can now use the EVSI package to calculate the EVSI across sample size and willingness-to-pay parameters. There are two key functions. Firstly, the mm.post.var(...) function calculates the variance of the posterior across different sample sizes. Once this regression fit is available, evsi.calc(...) calculates the EVSI across different sample sizes and willingness-to-pay parameters. This allows you to calculate the EVSI for different sample sizes if required once the computationally intensive variance estimation has been achieved. Both these functions can take quite a while to perform their calculations — while the computation is being undertaken have a look through the code and make sure you are clear what each section is doing.\n Finally, we can investigate our EVSI analysis using the launch.App(...) function which displays the EVSI results graphically. Have a look through all the tabs of this graphical display. What do the different graphs display? How can we interpret the results of this EVSI analysis? Under what conditions should we find a treatment of this design?\n We can have greater control over our EVSI analysis if we use BCEA externally to the functions within the EVSI package. We can also generate our future data externally to the mm.post.var(...) function which gives greater flexibility. The second section demonstrates how the EVSI package can be used in this setting. How does this change the analysis? You will notice that there was a warning in the first section that the data are correlated and, therefore, the data should not be generated within the mm.post.var(...) function. Thus, the results in section 2 are the correct EVSI values.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c12d8bbd74852ebdfde828fc502877bd","permalink":"/practical/14_evsi_mc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/14_evsi_mc/","section":"practical","summary":"Relevant literature  Computing the EVSI using Moment Matching\nHeath A, Manoloupolou I, Baio G. Efficient Monte Carlo Estimation of the Expected Value of Sample Information Using Moment Matching.","tags":[],"title":"Practical 14: Computing the EVSI using Monte Carlo simulations","type":"book"},{"authors":null,"categories":[],"content":" Relevant literature  Partial EVPI:\nStrong M, Oakley JE, Brennan A. Estimating multi-parameter partial Expected Value of Perfect Information from a probabilistic sensitivity analysis sample: a non-parametric regression approach Medical Decision Making. 2014; 34(3):311-26.\nEVSI:\nStrong M, Oakley JE, Brennan A, Breeze P. Estimating the Expected Value of Sample Information using the Probabilistic Sensitivity Analysis Sample. A Fast Non-Parametric Regression Based Method. Medical Decision Making. 2015; 35(5):570-583.\n  Import PSA sample This example is the hypothetical model used in the first case study in Strong et al (2013). There are two decision options, and 19 parameters. The file psa_model_3.csv contains the PSA.\nRead in the PSA and create objects for costs, effects and parameters:\npsa \u0026lt;- read.csv(\u0026quot;psa_model_3.csv\u0026quot;) head(psa) dim(psa) costs \u0026lt;- psa[, grep(\u0026quot;costs\u0026quot;, names(psa))] effects \u0026lt;- psa[, grep(\u0026quot;effects\u0026quot;, names(psa))] theta \u0026lt;- psa[, grep(\u0026quot;theta\u0026quot;, names(psa))] dim(costs) dim(effects) dim(theta) K \u0026lt;- 10000 # set K to 10000  Calculate absolute net benefits and incremental net benefits versus option 1.\nnb \u0026lt;- effects * K - costs inb \u0026lt;- nb[, 2] - nb[, 1] colMeans(nb) mean(inb) Which is the best option, and what is the overall EVPI?\nWe use the function again to find the maximum net benefit for each row of the PSA. The allows us to use the function with an arbitrary number of columns (here, there are two). It is very fast and efficient, but don’t worry about the syntax (unless you are interested!).\nnb \u0026lt;- as.data.frame(nb) # convert to data.frame so do.call works mean(do.call(pmax, nb)) - max(colMeans(nb))  Calculate Partial EVPI for each parameter To calculate EVPPI for each parameter we can use the following loop:\nlibrary(mgcv) # to get gam function # initialise a vector to hold the results. evppi_vector \u0026lt;- numeric(19) for (i in 1:19) { print(i) parameter_of_interest \u0026lt;- theta[, i] model \u0026lt;- gam(inb ~ s(parameter_of_interest)) fittedValues \u0026lt;- data.frame(0, fitted(model)) evppi_vector[i] \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) } names(evppi_vector) \u0026lt;- paste(\u0026quot;theta\u0026quot;, 1:19) print(round(evppi_vector, 1))  Calculate EVSI for some study designs A simple binomial example In order to compute EVSI for some proposed trial, or data collection exercise we need to make a judgement about what the data generating mechanism is. So, if one of the model parameters, \\(\\theta_{14}\\), is a proportion (e.g. the number of patients who respond to some intervention), and if we are trying to estimate this with a cross sectional study, we might suppose that the number of responses observed in a sample of size \\(N\\) is binomially distributed with parameter \\(\\theta_{14}\\), where \\[ X \\sim \\textrm{Binomial}(\\theta_{14}, N). \\]\nWhen we compute EVSI, we first have to generate a set of simulated datasets. So, in this example above we would have to generate a set of simulated cross sectional study outcomes.\nTo fix ideas let’s have a look at our sample from the distribution of \\(\\theta_{14}\\) in the PSA.\n# create an object called theta14 that holds the 14th column of theta theta14 \u0026lt;- theta[, 14] theta14[1:10] summary(theta14) You could also do, for example,\nhist(theta14) Conditional on the first value of \\(\\theta_{14}\\) in the PSA sample, we could generate a simulated number of responses in a sample of size 100:\nset.seed(1) N \u0026lt;- 100 x \u0026lt;- rbinom(1, N, theta14[1]) x Now, we could calculate a summary statistic for this study. The natural summary would be the sample proportion, \\(p = x / N\\).\nsummary.statistics \u0026lt;- x / N summary.statistics Now, conditional on the first, say 10 values of \\(\\theta_{14}\\) in the PSA sample, we can generate 10 sample datasets (each with sample size 100):\nset.seed(1) N \u0026lt;- 100 x \u0026lt;- rbinom(10, N, theta14[1:10]) summary.statistics \u0026lt;- x / N summary.statistics To sample a dataset for all the sampled values of \\(\\theta_{14}\\) in the PSA we do:\nN \u0026lt;- 100 x \u0026lt;- rbinom(length(theta14), N, theta14) summary.statistics \u0026lt;- x / N Now that we have our simulated study outcomes, we do the regression step, simply replacing the parameter of interest with the simulated study outcome.\nmodel \u0026lt;- gam(inb ~ s(summary.statistics)) fittedValues \u0026lt;- data.frame(0, fitted(model)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi Check the diagnostics:\ngam.check(model) We could look at a range of study sizes:\nset.seed(1) evsi_values \u0026lt;- c() # initialise empty vector for results study_sizes \u0026lt;- c(10, 20, 50, 100, 200, 500, 1000) for (N in study_sizes) { print(N) x \u0026lt;- rbinom(length(theta14), N, theta14) summary.statistics \u0026lt;- x / N model \u0026lt;- gam(inb ~ s(summary.statistics)) fittedValues \u0026lt;- data.frame(0, fitted(model)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi_values \u0026lt;- c(evsi_values, evsi) } data.frame(study_sizes, evsi_values) plot(study_sizes, evsi_values, type = \u0026quot;b\u0026quot;) We can compare these values with the partial EVPI for \\(\\theta_{14}\\), which we can get via\nevppi_vector[14] Note how the EVSI converges to the partial EVPI.\nLet’s now compute EVSI for a similar study to inform \\(\\theta_{5}\\), again for a range of study sizes.\nset.seed(1) theta5 \u0026lt;- theta[, 5] evsi_values \u0026lt;- NULL study_sizes \u0026lt;- c(10, 20, 50, 100, 200, 500, 1000) for (N in study_sizes) { print(N) x \u0026lt;- rbinom(length(theta5), N, theta5) summary.statistics \u0026lt;- x / N model \u0026lt;- gam(inb ~ s(summary.statistics)) fittedValues \u0026lt;- data.frame(0, fitted(model)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi_values \u0026lt;- c(evsi_values, evsi) } data.frame(study_sizes, evsi_values) plot(study_sizes, evsi_values, type = \u0026quot;b\u0026quot;) The partial EVPI for \\(\\theta_{5}\\) is\nevppi_vector[5] Again, note how EVSI converges to partial EVPI.\nIf we examine the EVSI values in the last example we notice that they are not monotonically increasing with study size. This is due to Monte Carlo sampling noise. We can reduce this Monte Carlo sampling noise by calculating EVSI using a replicated PSA. To do this we generate replicate parameter sets and corresponding replicate sets of incremental net benefits. We then calculate EVSI values using these replicated sets.\nDoing this is R is straightforward.\nFirst we generate replicated parameters and incremental net benefits. Let’s generate 100 sets.\ntheta5_rep \u0026lt;- rep(theta5, 100) inb_rep \u0026lt;- rep(inb, 100) Then, we repeat the analysis above with the replicated PSA, and replicated\nset.seed(1) evsi_values \u0026lt;- NULL study_sizes \u0026lt;- c(10, 20, 50, 100, 200, 500, 1000) for (N in study_sizes) { print(N) x \u0026lt;- rbinom(length(theta5_rep), N, theta5_rep) summary.statistics \u0026lt;- x / N model \u0026lt;- gam(inb_rep ~ s(summary.statistics)) fittedValues \u0026lt;- data.frame(0, fitted(model)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi_values \u0026lt;- c(evsi_values, evsi) } data.frame(study_sizes, evsi_values) plot(study_sizes, evsi_values, type = \u0026quot;b\u0026quot;)  Simulating an RCT Now, for something a little more complex. The parameters \\(\\theta_5\\) and \\(\\theta_{14}\\) represent response proportions under the decision option 1 intervention and the decision option intervention 2 respectively. We would usually learn about a difference between these response proportions in an RCT. So, we can simulate RCT outcomes as follows: \\[ X_1 \\sim \\textrm{Binomial}(\\theta_{5}, 100). \\] \\[ X_2 \\sim \\textrm{Binomial}(\\theta_{14}, 100). \\]\nWith R code:\nset.seed(1) N_arm1 \u0026lt;- 100 N_arm2 \u0026lt;- 100 x_arm1 \u0026lt;- rbinom(length(theta5), N_arm1, theta5) x_arm2 \u0026lt;- rbinom(length(theta14), N_arm2, theta14) Now, we have to decide how we summarise the data. Usually, we would only expect an RCT to update knowledge about the relative treatment effect. So, we could summarise each trial as a log odds ratio. We need to be careful about division by zero, so if \\(x\\) is equal to \\(0\\) or \\(N\\) we would usually add a half to \\(x\\), and 1 to \\(N\\). So\nN_arm1_adj \u0026lt;- ifelse(x_arm1 == 0 | x_arm1 == N_arm1, N_arm1 + 1, N_arm1) N_arm2_adj \u0026lt;- ifelse(x_arm2 == 0 | x_arm2 == N_arm2, N_arm2 + 1, N_arm2) x_arm1_adj \u0026lt;- ifelse(x_arm1 == 0 | x_arm1 == N_arm1, x_arm1 + 0.5, x_arm1) x_arm2_adj \u0026lt;- ifelse(x_arm2 == 0 | x_arm2 == N_arm2, x_arm2 + 0.5, x_arm2) p_arm1_adj \u0026lt;- x_arm1_adj / N_arm1_adj p_arm2_adj \u0026lt;- x_arm2_adj / N_arm2_adj odds_ratio \u0026lt;- (p_arm1_adj / (1 - p_arm1_adj) ) / (p_arm2_adj / (1 - p_arm2_adj)) l_OR \u0026lt;- log(odds_ratio) The regression step would then be\nmodel_OR \u0026lt;- gam(inb ~ s(l_OR)) fittedValues \u0026lt;- data.frame(0, fitted(model_OR)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi Check the diagnostics:\ngam.check(model_OR) If we expect that we would use the RCT to tell us about the absolute risks \\(\\theta_{5}\\) and \\(\\theta_{14}\\) directly then we could do this regression instead.\nmodel_p1_p2 \u0026lt;- gam(inb ~ te(p_arm1_adj, p_arm2_adj)) fittedValues \u0026lt;- data.frame(0, fitted(model_p1_p2)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi gam.check(model_p1_p2) Notice that this gives us more information, and therefore the EVSI is higher. Now try adding in the odds ratio:\nmodel_p1_p2_OR \u0026lt;- gam(inb ~ te(p_arm1_adj, p_arm2_adj, l_OR)) fittedValues \u0026lt;- data.frame(0, fitted(model_p1_p2_OR)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi gam.check(model_p1_p2_OR) Adding in the log(OR) doesn’t change the EVSI. This gives us no extra information.\n Simulating time to event data Finally, let’s have a look at \\(\\theta_{16}\\). This is the duration of response for someone under treatment option 2. We think that if we were to observe duration of response in a follow up study of some sort, the duration would have a Weibull distribution with shape parameter \\(k=0.8\\). Conditional on a value of \\(\\theta_{16}\\), we could then simulalate the duration for a sample size of, say, 10 patients. We could ensure that this has expectation equal to \\(\\theta_{16}\\) by choosing the scale parameter to be equal to \\(\\theta_{16}/\\Gamma(1 + 1 / k)\\), where \\(\\Gamma(\\cdot)\\) is the gamma function.\ntheta16 \u0026lt;- theta[, 16] set.seed(1) shape \u0026lt;- 0.8 scale \u0026lt;- theta16[1] / gamma(1 + 1 / shape) duration \u0026lt;- rweibull(10, shape, scale) duration We then need to decide how we would summarise the data. We could use the mean duration, or perhaps the median. This would the value we would use in the regression.\nSo the whole set up would be as follows. First, simulate datasets for each of the values of \\(\\theta_{16}\\) in the PSA sample, and in each case, calculate the median. Let’s say that we follow up 50 patients.\nx \u0026lt;- numeric(length(theta16)) sample_size \u0026lt;- 20 set.seed(1) for (i in 1:length(theta16)) { shape \u0026lt;- 0.8 scale \u0026lt;- theta16[i] / gamma(1 + 1 / shape) duration \u0026lt;- rweibull(sample_size, shape, scale) x[i] \u0026lt;- median(duration) } model \u0026lt;- gam(inb ~ s(x)) fittedValues \u0026lt;- data.frame(0, fitted(model)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi Check the diagnostics:\ngam.check(model) The partial EVPI for \\(\\theta_{16}\\) is\nevppi_vector[16] You could try the same for some different sample sizes, e.g.\nx \u0026lt;- numeric(length(theta16)) sample_size \u0026lt;- 50 set.seed(1) for (i in 1:length(theta16)) { shape \u0026lt;- 0.8 scale \u0026lt;- theta16[i] / gamma(1 + 1 / shape) duration \u0026lt;- rweibull(sample_size, shape, scale) x[i] \u0026lt;- median(duration) } model \u0026lt;- gam(inb ~ s(x)) fittedValues \u0026lt;- data.frame(0, fitted(model)) evsi \u0026lt;- mean(do.call(pmax, fittedValues)) - max(colMeans(fittedValues)) evsi Write a loop to calculate EVSI for a range of sample sizes. Plot these against sample size.\nMore complex data generating scenarios can be developed. For example, we may want to incorporate a censoring process in the data generating step. For time to event measures, censoring of outcome due to completion of the study follow up period is common. If we include this censoring process in the EVSI data generation step we can then compute the EVSI for a range of study durations.\n  Calculating EVSI using SAVI / BCEAweb SAVI or BCEAweb can be used to calculate EVSI, once the trial datasets have been generated.\nTo calculate EVSI using Excel and SAVI / BCEAweb do the following.\nRun the usual PSA and store parameters, costs and health effects. We assume that there are \\(n\\) rows in the PSA, Select the parameter(s) that will be updated by the proposed study, Generate simulated trial data for each row of the PSA by sampling from the study data likelihood1, conditional on the value(s) of the parameter(s) in the row in question, Summarise the trial data for each row of the PSA to give \\(n\\) summary statistics, Include these summary statistics as an extra column in the parameter spreadsheet. The partial EVPI of this new “parameter” is the EVSI for the study.  So, to replicate the first example above, a cross-sectional survey to learn about \\(\\theta_{14}\\), we would generate binomial random variables in a new column in the Excel spreadsheet. Have a look at the Excel file parameters_for_evsi.xslx to see how we’ve done this for trials of size 10 and 20. Do the same for a range of trial sizes up to 1000.\nThen, save the parameter file in .csv format and import into SAVI / BCEAweb. Compare results with the EVSI calculated above. Note again the Monte Carlo noise. Replicated PSA sets would reduce this noise.\n  This likelihood that should be sampled is that which would be assumed in the analysis of the trial data. The trial statistician should know what this is. This step is required in the usual Monte Carlo approach to computing EVSI.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2bae6c085ee582d720fee7b8ac94e1d8","permalink":"/practical/15_evsi_regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/15_evsi_regression/","section":"practical","summary":"Relevant literature  Partial EVPI:\nStrong M, Oakley JE, Brennan A. Estimating multi-parameter partial Expected Value of Perfect Information from a probabilistic sensitivity analysis sample: a non-parametric regression approach Medical Decision Making.","tags":[],"title":"Practical 15: Computing EVSI using regression","type":"book"},{"authors":null,"categories":null,"content":"","date":1655978400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1655978400,"objectID":"c8c4dce3338a1516bd1a837af56f18bf","permalink":"/practical/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/","section":"practical","summary":"","tags":null,"title":"","type":"book"},{"authors":null,"categories":[],"content":" 1. “Coins” example (see lectures) The first thing we need to do is to run the programme from the script provided in the file coins-script.odc. The commands in the script make automatic the process of pointing-and-clicking that we would otherwise need to perform the analysis from BUGS. We need to be careful in telling BUGS what the “working directory” is. In the script, the default path is set as c:/bayes-hecourse/1_monte-carlo, which grants a couple of comments.\nFirstly, you may not have saved the script file onto this directory (which in fact may not even exist in your computer!). So you need to make sure you update this to the actual path to the folder in which the file coins-script.odc is stored, or else BUGS will complain that it can’t find the file. Secondly, the default text uses Unix notation, where folders are separated using a “/” symbol. Under other operating systems (notably MS Windows), this is no longer valid and paths are indicated using a “\\” symbol. You then need to be careful in providing the correct string. For instance, suppose your file under MS Windows is saved in the directory N:\\DesktopSetting\\Desktop\\STAT0019, then you would need to copy and paste this string into the model script.\nIn order to run the script, you need to click anywhere on the file coins-script.odc (assuming you have opened it in BUGS) and then click on the menu Model  Script\n NB: we use the notation Command1  Command2 to indicate that you need to click on the menu labelled as Command1 and then on the sub-menu labelled as Command2).   Unlike the older version (WinBUGS), OpenBUGS does not open the log file automatically. Thus, if there is an error, only a message in the bottom-left corner of the window will appear (but it may not be very noticeable). To open the log file you can click on Info  Open Log. If you do this, before running the script, remember to click back on the part of the window occupied by the script file and then click on Model  Script. In this case, you will see on the log file messages from OpenBUGS informing you of what is happening. For instance, if all works, these messages will be printed.\nmodel is syntactically correct model compiled initial values generated, model initialized model is updating 1000 updates took 0 s In addition, two more windows will automatically open, showing the output of the BUGSprocedure. The first one shows the “Node statistics, i.e. the summary of the simulations performed for the node monitored (you can check that the script instructs BUGSto monitor the nodes Y and P8 with the commands\nsamplesSet(Y) samplesSet(P8) The second window shows the graphs of the posterior densities for the nodes monitored. In this case, you will see the histograms of P8 and Y. The latter represents the predictive distribution of \\(Y\\), the number of coin tosses showing up “heads”. In line with the summary statistics, this distribution has a mean of approximately 5 and most of the probability mass (in fact, 95%) is between 2 and 8. As for \\(P_8\\), which represents the probability of observing 8 or more “heads”, the histogram has mostly 0s, to indicate that in all the simulations it is most likely that this event does not happen, than it does. Again, this is in line with the summary statistics, showing that the sample mean computed from the simulations \\[\\mbox{E}[P_8] = \\frac{1}{S}\\sum_{s=1}^S P_8\\] is just above 5%. Notice that because of how the node P8 is defined in the model code (see Lecture notes), it either takes value 0 (if the event is not true) or 1 (otherwise). Thus the mean effectively represents the estimated probability that the event (i.e. at least 8 “heads”) is true.\nWe can also run the model using the point-and-click facility of BUGS (this can be done by simply following all the steps in the practical question). The output will be identical to the one just described.\nFinally, we can modify the original model code to encode the assumption that the coin is actually unbalanced (with a probaility of “heads” equal to 0.7). To do so, we simply need to modify the model code as follows.\nmodel { Y ~ dbin(0.7, 30) P15 \u0026lt;- step(15.5 - Y) } and then re-run the BUGSprocedure. Of course you can either overwrite the original file and then run the script, or save the new code to another file, update the command modelCheck to provide the new file name, or simply use the point-and-click procedure (this latter option does not require you to save the file).\nThe predictive distribution for \\(Y\\) in this new setting is centered around 20, with most of the mass between 16 and 26, while the probability of observing at most 15 “heads” is estimated to be around 1.7%.\n 2. “Drug” example (see lectures) To run the model, you can again use the script provided or simply point-and-click to run the model code file, also provided. As discussed in the class, the model code instructs BUGSon the assumptions underlying the model.\nmodel{ theta ~ dbeta(9.2, 13.8) y ~ dbin(theta, 20) P.crit \u0026lt;- step(y - 14.5) } The structure of the model is pretty much the same as in the previous example — except that in this case, we are not willing to assume a “fixed” and known value for the probability of the event of interest (e.g. “heads” in the previous example, or that the drug is successful, in the current one). So, instead of assigning it a value (e.g. 0.5 or 0.7), we describe our knowledge using a distribution. Given that we believe the “true” success rate to be between 0.2 and 0.6, we can express this using a Beta distribution with parameters \\(\\alpha=9.2\\) and \\(\\beta=13.8\\). As discussed in class, by the mathematical properties of the Beta distribution, this implies that we believe \\[\\mbox{E}[\\theta] = \\frac{\\alpha}{\\alpha+\\beta}=\\frac{9.2}{23}=0.4\\] and \\[\\mbox{Var}[\\theta] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}=0.01\\] (which implies \\(\\mbox{sd}[\\theta]=\\sqrt{0.001}=0.1\\), which in turn implies that, approximately, 95% of the mass for \\(\\theta\\) is included in \\(0.4\\pm 2(0.1)=[0.2; 0.6]\\), as required).\nIn the next part of the exercise, you are required to modify the prior distribution for \\(\\theta\\) to a Uniform(0,1). This effectively means that you are not implying any specific prior knowledge on the “true” success rate, apart from the fact that it can take a value between 0 and 1. Graphically, a Uniform distribution in (0,1) looks like in the following graph. This prior distribution is “non informative” in the sense that it does not provide much information or clue as to what value of the probability of success is more likely to generate the data. Consequently, it is not surprising that the resulting predictive distribution for \\(Y\\) is spread over the entire range of possible values (with a mean of around 10 and a 95% interval covering 0 to 20 — i.e. the whole range). Basically, if all you’re prepared to say before seeing any data is that the true success rate is something between 0 and 1, then the prediction of the number of successes cannot be anything else than anything between 0 (no successes) and 20 (all patients are cured)…\n 3. Simulating functions of random quantities One of the main features of Bayesian analysis using simulation methods (such as those underpinning BUGS) is that it is possible to obtain, almost as a simple by-product of the estimation procedure, simulations for any function of the main model parameters. We shall see later that this is very helpful when using complex models, e.g. for economic evaluation.\nIn this case, we need to write a model for a variable \\(y \\sim \\mbox{Normal}(\\mbox{mean}=0, \\mbox{ sd}=1)\\). As suggested in the practical exercise, we need to be careful and remember that BUGSparameterise the Normal distribution in terms of mean and precision (=1/variance). If we are imposing a sd of 1, then this is irrelevant because if \\(\\sigma=1\\), then \\(\\mbox{variance} = \\mbox{precision} = \\frac{1}{\\mbox{variance}} = 1\\). The model can be written in BUGSlanguage as\nmodel { y ~ dnorm(0, 1) } and run using the same procedure described above (for such a simple model, we can just use the point-and-click approach).\nThis produces simulations from a Normal variable with 0 mean and unit variance. The summary statistics of this distribution are as follows (your output may vary, due to pseudo-random variation).\n mean sd 2.5% 25% 50% 75% 97.5% -0.00153 1.00520 -1.95402 -0.67965 0.00295 0.66825 1.98802  based on a sample of 10000 simulations. The resulting density looks like the following (you can produce the graph in BUGSusing the density button in the Sample monitor tool).\nIf we want to assume that \\(y\\sim\\mbox{Normal}(\\mbox{mean}=1, \\mbox{ sd=2})\\), then this implies that precision \\(=1/2^2=0.25\\). So we need to modify the model code to\nmodel { y ~ dnorm(1, 0.25) } We can run this new model and obtain summaries and graphs for the predictive distribution of \\(y\\) (notice that the term “predictive” here highlights the fact that we have not actually observed \\(y\\) — we might in the future, but not just yet. In fact the most appropriate term here would be “prior predictive” distribution, again to highlight the fact that this is only based on our prior knowledge about the parameters, i.e. mean and standard deviation).\nThe output of the analysis is as follows (again, if you see slight differences, these would be down to simulation error).\n mean sd 2.5% 25% 50% 75% 97.5% 0.997 2.010 -2.909 -0.359 1.006 2.336 4.975  As is reasonable, the second case (with a larger sd) gives estimates that are more spread out (as the variability is higher, by definition).\nNow, we are asked to create a new variable \\(Z=Y^3\\) and estimate its distribution. We also need to compute the probability that \\(Z\u0026gt;10\\). This is extremely easy in BUGS — the model code needs to be modified simply to the following.\nmodel { y ~ dnorm(1, 0.25) z \u0026lt;- pow(y, 3) # Alternatively, we can write # z \u0026lt;- y * y * y above10 \u0026lt;- step(z - 10) }  mean sd 2.5% 25% 50% 75% 97.5% y 0.997 2.01 -2.91 -0.3593 1.01 2.34 4.98 z 13.089 39.75 -24.61 -0.0464 1.02 12.75 123.20 above10 0.283 0.45 0.00 0.0000 0.00 1.00 1.00  ","date":1655723700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655723700,"objectID":"43a4ea9f29258d5e71a696fd253db03c","permalink":"/practical/01_monte-carlo/solutions/","publishdate":"2022-06-19T00:00:00Z","relpermalink":"/practical/01_monte-carlo/solutions/","section":"practical","summary":"1. “Coins” example (see lectures) The first thing we need to do is to run the programme from the script provided in the file coins-script.odc. The commands in the script make automatic the process of pointing-and-clicking that we would otherwise need to perform the analysis from BUGS.","tags":[],"title":"Practical 1. Monte Carlo in BUGS - SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" Understanding Gibbs sampling Most of this exercise is actually fairly easy as it is performed directly in Excel. In fact, the model does not need MCMC to be analysed and we could obtain analytic estimates for the two variables \\((y_1,y_2)\\). Some clarifications are perhaps useful, though.\nThe main point is perhaps to realise that correlation across the model parameters may be a crucial factor in terms of convergence and the speed at which this may be reached when running MCMC. This is due to the fact that larger correlation implies that the joint distribution of the parameters has a ``narrower’’ shape. The figure below clarifies this point in the simplest two-dimensional, bi-variate Normal case — but the situation generalises to higher dimensions and different distributional shapes.\n Low correlation \\(\\rho=0.08\\)    High correlation \\(\\rho=0.98\\)      The graphs show the first 30 simulations in a Gibbs sampling process for the same set up as in the Excel spreadsheet; the “true” underlying mean is \\(\\boldsymbol\\theta=(0,0)\\) and the “true” underlying variances are \\(\\boldsymbol\\sigma^2=(1,1)\\). In the plot (panel a) we assume correlation \\(\\rho=0.08\\) (i.e.~very low). This generates a joint distribution for (y_1,y_2)$ that is like a “ball”, with a wide radius. This means that even a relatively low number of simulations (and more importantly long moves across the parametric space) is almost sufficient to cover the underlying true joint distribution.\nIn panel (b), however, because we are assuming a large correlation (\\(\\rho=0.98\\)), then the resulting distribution is very narrow and thus 30 simulations cannot cover the target portion of the space. In fact, in this case, the initial values make a difference — starting from values that are far awy from the bulk of the target distribution will generally mean that a longer running time is necessary.\n MCMC in BUGS As discussed in the lecture, the model is very similar to the one seen in Practical 1.\nmodel { theta ~ dbeta(a,b) # prior distribution y ~ dbin(theta,m) # sampling distribution y.pred ~ dbin(theta,n) # predictive distribution P.crit \u0026lt;- step(y.pred - ncrit + 0.5) # =1 if y.pred \u0026gt;= ncrit, } Again, we model our uncertainty on the underlying success rate for the drug using an informative, conjugate Beta prior distribution, which implies a mean of around 0.4 and a 95% interval spanning from 0.2 to 0.6. This time, however, we do have observed data on \\(y\\) — effectively now the objective of our analysis is estimate the posterior distribution of \\(\\theta\\), \\(p(\\theta\\mid y)\\) and to predict the outcome of the next round of the trial, when we assume to observed \\(n=40\\) patients alltogether.\nNotice that in the model code, we are making the assumption that the new patients are effectively drawn from the same population of those that we have already observed. This is expressed by assuming that they have the same distributional form — both y and y.pred are modelled using a sampling distribution that is assumed to be Binomial\\((\\theta, \\mbox{sample size})\\) and effectively all that we assume to vary is the sample size (\\(m=20\\) for the observed \\(y\\) and \\(n=40\\) for the new patients).\nThe actual .odc file contains all the relevant bits that BUGS needs, in one place. In addition to the model code, it also contains the data, in the following format.\nlist( a = 9.2, b = 13.8, # prior parameters y = 15, # number of successes m = 20, # number of trials n = 40, # future number of trials ncrit = 25) # critical value of future successes This is a “list” (we shall see a lot more of this when we start working in R). As mentioned in the class, it is generally a good idea to keep the code general and then pass values that can be changed separately, rather than “hard-coding” them in the model code. In this simple case, however, you could “fix” the values of the parameters into the model code, as in the following.\nmodel { theta ~ dbeta(a,b) # prior distribution y ~ dbin(theta,m) # sampling distribution y.pred ~ dbin(theta,n) # predictive distribution P.crit \u0026lt;- step(y.pred - ncrit + 0.5) # =1 if y.pred \u0026gt;= ncrit, y \u0026lt;- 15 # 0 otherwise } If you wanted to use this latter format, you would not need to pass BUGS any data, because all the relevant numbers would be included in the model code. Notice BUGS accepts a `double'' definition for the nodey` — first as a random variable associated with a Binomial distribution and then as a fixed number (15, indicating the observed number of successess in the current trial). In general, however, you need to be careful — for example it is not possible to define a node twice with the same nature (i.e. twice as observed data, or twice with two different probabilistic assignments).\nFinally, the odc file contains the list for the initial values. In a BUGS file, all variables that are associated with a probabilistic statement (i.e. a “~” symbol after the variable name) and are not observed need to be initialised so that BUGS can run the Gibbs sampler. So, in this particular case, although y is defined as a random variable, because it is also observed, then there is no uncertainty about its value and so we should not initialise it. Conversely, although y.pred has exactly the same nature as y, because it is not observed, then we need to initialise it. In the odc file we actually let BUGS do this (effectively when clicking the gen init button in the Specification tool), but we do provide initial values for the other unobserved, random node, theta.\nFor the same reason mentioned above, BUGS does not let us monitor the node y — it has been already observed, so, as said above, there is nothing more to learn about it. It can be only used to learn about theta, y.pred and P.crit, which will all be associated with a posterior distribution, given the evidence provided by the fact that \\(y=15\\) out of \\(m=20\\) trials.\nWhen we actually run the model, we can monitor the relevant nodes and produce the following summary statistics.\n mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff y.pred 22.578 4.2649 14.000 20.000 23.000 25.000 31.000 1 10000 theta 0.563 0.0749 0.414 0.512 0.563 0.614 0.706 1 10000 P.crit 0.330 0.4701 0.000 0.000 0.000 1.000 1.000 1 10000 These indicate that, given current evidence, we revise our uncertainty about the true success rate of the drug from a distribution centered on 0.4 and with most of the mass in [0.2; 0.6] to another centered around 0.563 and with most of the probability mass included in [0.414; 0.706].\nIn fact, because we are using a Beta prior combined with a Binomial sampling distribution, then the model is conjugated and so we know that the form of the posterior distribution is another Beta (as seen in class), where the update from prior to posterior only involves the value of its parameters. This implies that we don’t need to worry about convergence of the MCMC model. This is confirmed by the inspection of the traceplots.\n   In all cases, already at iteration 1 of the MCMC the two chains that we’ve run in parallel are on top of each other and evidently visiting the same portion of the parametric space.\nAs for y.pred and P.crit we can see that the expected number of successes in the next phase of the trial is around 23 and the probability of exceeding the critical threshold (25) is 32.97.\nNotice finally that we can also look at the convergence statistics, \\(\\hat{R}\\) (Rhat, the Potential Scale Reduction, also known as “Gelman-Rubin Statistic”) and the effective sample size \\(n_{\\text{eff}}\\) (n.eff). The former has a value of 1 for all the nodes — again, given the fact that the model is conjugate, this is not surprising. The model does not present problems in terms of autocorrelation. The following graphs show the autocorrelation plot for the monitored nodes.    The way in which we need to interpret these is to look at the level of autocorrelation across different lags. These indicate how much correlation exists across consecutive, lagged simulations in the MCMC procedure. It is expected that consecutive iterations be relatively correlated (if you recall how the Gibbs sampling is constructed, you should realise that the \\(s-\\)th simulated value for the \\(p-\\)th parameter \\(\\theta_p^{(s)}\\) depends on the distribution at the \\((s-1)-\\)th run for all the other parameters). But it is also expected that this level of correlation should fade away as you consider simulations that are further apart. So if the autocorrelation graph looks like the ones above, you can say that actually the simulations are close to a series of independent values because the autocorrelation is very low, in fact at low lags as well.\nIn our case, we that the model converges (because of conjugacy), so it is not surprising to see these graphs. Numerically, when the effective sample size is “close enough” the the nominal sample size (i.e. the number of iterations you use to draw your inference), then autocorrelation is not a problem. In this case, we have considered 2 chains, each with 5000 iterations (so a total of 10000). The value of n.eff is 10000 for y.pred, 10000 for theta and 10000 for P.crit. Clearly, the ratio of effective to nominal sample size is then 1, 1 and 1, which clearly indicate no issues.\n ","date":1655733600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655733600,"objectID":"1b697c8627310f515c7014d016c524d4","permalink":"/practical/02_mcmc/solutions/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/02_mcmc/solutions/","section":"practical","summary":"Understanding Gibbs sampling Most of this exercise is actually fairly easy as it is performed directly in Excel. In fact, the model does not need MCMC to be analysed and we could obtain analytic estimates for the two variables \\((y_1,y_2)\\).","tags":[],"title":"Practical 2. Markov Chain Monte Carlo - SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" Description In the wake of the COVID-19 pandemic, several biotechnology and pharmaceutical companies have started collaborating on the development of a vaccine, in an unprecedented effort to deliver vital innovation in a short amount of time. The first vaccine to be approved by the FDA in December 2020 was the one developed jointly by German biotech company BioNTech and US pharmaceutical giant Pfizer. The development of the vaccine was based on a Phase I/II/III multicenter, placebo-controlled trial to evaluate the safety, tolerability, immunogenicity and efficacy of the candidate vaccine.\nThe Phase II/III study was designed using a Bayesian approach based on the following setup. There are two arms: one is randomised to receive two doses of a placebo, while the active arm receives two doses of the candidate vaccine. The data relevant for the efficacy analysis are the number of individuals who in each arm are confirmed COVID-19 cases, over the total number of individuals randomised to the specific arm. This can be formalised as \\(y_{\\rm plac}\\sim \\mbox{Bin}(\\pi_{\\rm plac}, n_{\\rm plac})\\) and \\(y_{\\rm vac}\\sim \\mbox{Bin}(\\pi_{\\rm vac}, n_{\\rm vac})\\), respectively. The actual measure of vaccine efficacy is defined as \\(\\mbox{VE}=\\displaystyle\\left(1-\\frac{\\pi_{\\rm vac}}{\\pi_{\\rm plac}}\\right)\\), assuming a 1:1 allocation ratio in the two arms and thus \\(n_{\\rm plac}=n_{\\rm vac}=N\\).\nHowever, the analysis is based on a reformulation of the problem: the experimenters actually model \\(y=y_{\\rm vac}=\\) number of COVID-19 cases among the individuals in the treatment arm over \\(n=(y_{\\rm vac}+y_{\\rm plac})=\\) the total number of COVID-19 cases in the two arms. This is modelled as \\(y \\sim \\mbox{Bin}(\\theta,n)\\), where \\[\\begin{eqnarray} \\theta\u0026amp; = \u0026amp; \\displaystyle\\frac{\\pi_{\\rm vac}}{\\pi_{\\rm vac}+\\pi_{\\rm plac}} \\nonumber \\\\ \u0026amp; = \u0026amp; \\frac{1-\\mbox{VE}}{2-\\mbox{VE}}. \\end{eqnarray}\\]\nThe reason for this seemingly overly-complex (and obscure) setup is that it allows the experimenters to only specify one observational model and, crucially a single prior distribution for the parameter \\(\\theta\\) — of course, once \\(p(\\theta\\mid {\\rm data})\\) is available, using the equation above, it is straightforward (e.g. through Monte Carlo simulation) to obtain directly the posterior distribution for the main parameter of interest \\(p(\\mbox{VE}\\mid {\\rm data})\\).\n Sample size calculation The determination of the sample size calculation is based on a simulation approach. The experimenter looked to determine a sample size large enough to be able to provide a probability exceeding 90% to conclude that \\(\\mbox{VE}\u0026gt;30\\%\\) with a “high probability” (we note here that the study protocol is vague on the actual threshold selected).\nThe simulation excercise proceeds in two steps. Firstly, the experimenters make assumptions about some of the features of the “data generating process”. For instance, in the study protocol, they stipulate a “true” vaccine efficacy \\(\\widehat{\\mbox{VE}} =0.6\\) (i.e. a reduction by 40% in the infection rate in the vaccine arm, in comparison to the placebo population). This implies that the “true” proportion of COVID-19 cases in the vaccine arm over the total of cases is \\[\\hat\\theta=\\displaystyle\\frac{1-\\widehat{\\mbox{VE}}}{2-\\widehat{\\mbox{VE}}}=0.2857.\\]\nUsing these assumption, we can simulate a large number \\(S\\) (say, 100,000) of potential trial data from the alleged generating process \\(y^{(s)}\\sim \\mbox{Bin}(\\hat\\theta,n)\\), where the superscript \\((s)\\) indicates the \\(s-\\)th simulated dataset and given a fixed value of the overall number of cases \\(n\\). Typically, we repeat the simulation for a grid of possible values of \\(n\\) (e.g. \\(n=[10,20,30,40,50,60,70,\\ldots]\\)).\nOnce the hypothetical trial data have been generated, the second step consists in analysing them according to the statistical analysis plan defined in the protocol. In this case, the full model specification is required, which involves defining a prior distribution for \\(\\theta\\).\nFor simplicity, the experimenters set a minimally informative Beta prior \\(\\theta \\sim \\mbox{Beta}(\\alpha_0,\\beta_0)\\), where the parameters \\(\\alpha_0\\) and \\(\\beta_0\\) are selected to express the limited amount of information available a priori (recall that \\(\\theta\\) represents the proportion of COVID-19 cases occurred in the vaccine group). We know that \\(\\mbox{E}[\\theta]=\\displaystyle\\frac{\\alpha_0}{\\alpha_0+\\beta_0}\\). If we fix \\(\\beta_0=1\\), then we can solve for \\(\\alpha_0\\) so that the prior mean for \\(\\theta\\) is equal to some pre-specified value — in particular, the experimenters had chosen a threshold of \\(\\mbox{VE}=30\\%\\) as the minimum level of efficacy they were prepared to entertain. This value can be mapped to the scale of \\(\\theta\\) as \\(\\displaystyle \\frac{1-0.3}{2-0.3}=0.4117\\) and thus solving \\[\\begin{eqnarray*} \\frac{\\alpha_0}{\\alpha_0+1} = 0.4117 \\end{eqnarray*}\\] gives \\(\\alpha_0=0.700102\\).\nBecause of conjugacy, for each simulated dataset we can easily update the Beta\\((0.700102,1)\\) prior to a Beta\\((\\alpha_1,\\beta_1)\\) posterior distribution for \\(\\theta\\), where \\(\\alpha_1=0.700102+y^{(s)}\\) and \\(\\beta_1=1+n-y^{(s)}\\). Moreover, for each simulation \\(s\\), we can compute analytically any tail-area probability from \\(p(\\theta\\mid{\\rm data})\\). Once again, we are really interested in \\(\\Pr(\\mbox{VE}\u0026gt;0.3\\mid{\\rm data})\\), but using the deterministic relationship linking VE to \\(\\theta\\), we can re-express this as \\[\\begin{eqnarray*} \\Pr(\\mbox{VE}\u0026gt;0.3 \\mid{\\rm data}) \u0026amp; = \u0026amp; \\Pr\\left(\\frac{1-2\\theta}{1-\\theta}\u0026gt;0.3\\mid{\\rm data}\\right) \\\\ \u0026amp; = \u0026amp; \\Pr(\\theta\u0026lt;0.4117 \\mid {\\rm data}), \\end{eqnarray*}\\] which can be computed for each simulation \\(s\\). This produces a large number of simulations that can be used to determine the “power” for a given sample size, as the proportion of times in which this computed probability exceeds a set threshold (i.e. is “large enough” in the phrasing of the study protocol).\nThe experimenters compute \\(n=164\\) as the optimal number of total COVID-19 cases that are necessary to be able to ascertain that \\(\\mbox{VE}\u0026gt;30\\%\\) with a large probability.\nThen, it is necessary to determine the overall sample size (i.e. the total number of individuals to be recruited in the study) so that a total of 164 cases is likely to be observed within the required time frame. Once again, it is necessary to make some assumption about the data generating process; specifically the experimenters consider a 1.3% illness rate per year in the placebo group. Because the study aims at accruing 164 cases within 6 months, this essentially amounts to assuming that \\(\\pi_{\\rm plac} \\approx 0.013/2\\) and thus \\(\\pi_{\\rm vac} \\approx (\\pi_{\\rm plac}\\times 0.4)/2\\) (recall that we are assuming a 60% vaccine efficacy, or that \\(\\pi_{\\rm vac}=0.4\\times \\pi_{\\rm plac}\\)).\nWe can once again resort to simulations to estimate what sample size in each arm \\(N\\) is necessary so that we can expect \\(y_{\\rm vac}+y_{\\rm plac}\\geq 164\\) — this returns an optimal sample size of \\(N=17\\,600\\) per group. Finally, considering an attrition rate of 20% (indicating that such proportion of individuals would not generate an evaluable outcome could be observed), the experimenters inflate the sample size to obtain \\(N^*=\\displaystyle\\frac{N}{0.8}=21\\,999\\) per group (or a total of 43,998 individuals).\n Data analysis At the end of the actual study, there were \\(y_{\\rm vac}=8\\) and \\(y_{\\rm plac}=162\\) confirmed COVID-19 cases in the vaccine and the placebo group, respectively. These imply that \\(y=8\\) and \\(n=(8+162)=170\\). However, there is a slight extra complication: the setup describe above implies the assumption of 1:1 allocation (i.e. \\(n_{\\rm plac}=n_{\\rm vac}\\)) — this is crucial to ensure the simple deterministic relationship between VE and \\(\\theta\\). In actual fact, the placebo group had a slightly higher number of individuals, by the time of the data analysis (there were 17,411 and 17,511 individuals with valid data in the vaccine and placebo group, respectively). Thus, we need to rescale the observed data, which can be done by considering \\(y_{\\rm vac}^*=y_{\\rm vac}\\displaystyle\\frac{17\\,461}{17\\,411}=8.02297\\) and \\(y_{\\rm plac}^*=y_{\\rm plac}\\displaystyle\\frac{17\\,461}{17\\,511}=161.53743\\), where \\(17\\,461 = \\displaystyle\\frac{17\\,411+17\\,511}{2}\\) and thus \\(y^*=8.02297\\) and \\(n^*=169.5604\\).\nThese data can be used to update the prior distribution Beta\\((0.700102,1)\\) into a Beta\\((8.723072,162.5374)\\) posterior for \\(\\theta\\). Finally, we can rescale this posterior to determine the relevant posterior distribution \\(p(\\mbox{VE}\\mid {\\rm data})\\), for instance using Monte Carlo to simulate a large number \\(S\\) of values \\(\\theta^{(s)}\\sim \\mbox{Beta}(8.723072,162.5374)\\) and then computing \\(\\mbox{VE}^{(s)}=\\displaystyle\\frac{1-2\\theta^{(s)}}{1-\\theta^{(s)}}\\) and then use these to characterise the uncertainty around the vaccine efficacy.\nIn this case, the posterior distributions \\(p(\\theta\\mid {\\rm data})\\) and \\(p(\\mbox{VE}\\mid {\\rm data})\\) can be visualised below.\nalpha.0=0.700102 beta.0=1 y=c(8,162) n=c(17411,17511) # So needs to reproportion as if the treatment arms had the same sample size (to be in line with model assumptions!) y=y*mean(n)/n # Now can update the Beta prior with the observed data alpha.1=alpha.0+y[1] beta.1=beta.0+y[2] theta=rbeta(100000,alpha.1,beta.1) ve=(1-2*theta)/(1-theta) We can also use the resulting samples from the posterior distribution to estimate the 95% interval of \\([0.9034; 0.9761]\\) for the vaccine efficacy, indicating that we expect the vaccine to perform extremely well.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"726b6aab35f6f7accb550a08a6a4e753","permalink":"/practical/02_mcmc/covid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/02_mcmc/covid/","section":"practical","summary":"Description In the wake of the COVID-19 pandemic, several biotechnology and pharmaceutical companies have started collaborating on the development of a vaccine, in an unprecedented effort to deliver vital innovation in a short amount of time.","tags":[],"title":"Covid vaccine: Bayesian modelling","type":"book"},{"authors":null,"categories":[],"content":" Introduction This short tutorial will guide you through the example of Gibbs sampling shown in class.\nAs a quick reminder, the example does not really require Gibbs sampling or any form of MCMC to estimate the joint posterior distribution for the parameters. However, because of its specific assumptions it is very helpful because essentially we can determine analytically the full conditional distributions for each parameter (details later). This means that we can directly and repeatedly sample from them (which is a pre-requisite of Gibbs sampling and what BUGS does).\nThis document also includes the R code used to obtain the output.\n Set up As discussed in class, we assume a set up such as the following.\n\\[\\begin{align} y_i \u0026amp; \\stackrel{iid}{\\sim} \\mbox{Normal}(\\mu,\\sigma^2),\\qquad \\mbox{with } i=1,\\ldots,n \\\\ \\mu \u0026amp; \\sim \\mbox{Normal}(\\mu_0,\\sigma^2_0) \\\\ \\tau = \\frac{1}{\\sigma^2} \u0026amp; \\sim \\mbox{Gamma} (\\alpha_0,\\beta_0) \\end{align}\\] Under these assumptions we can prove analyticaly that the full conditional distributions for the two parameters \\(\\mu\\) and \\(\\tau\\) are \\[\\mu\\mid \\sigma^2,\\boldsymbol{y} \\sim \\mbox{Normal}(\\mu_1,\\sigma^2_1) \\qquad \\mbox{with: } \\mu_1=\\sigma^2_1\\left( \\frac{\\mu_0}{\\sigma^2_0} + \\frac{n\\bar{y}}{\\sigma^2}\\right) \\qquad \\mbox{and: } \\sigma^2_1=\\left(\\frac{1}{\\sigma^2_0}+\\frac{n}{\\sigma^2}\\right)^{-1}\\] \\[\\tau\\mid\\mu,\\boldsymbol{y} \\sim \\mbox{Gamma}(\\alpha_1,\\beta_1) \\qquad \\mbox{with: }\\,\\alpha_1=\\alpha_0+\\frac{n}{2}\\quad \\mbox{and: } \\quad \\beta_1 = \\beta_0 + \\frac{1}{2}\\sum_{i=1}^n (y_i-\\mu)^2.\\]\nNotice that the full conditionals are not the target distributions for Bayesian inference. What we really want is the marginal posterior distributions \\(p(\\mu \\mid \\boldsymbol{y} )\\) and \\(p(\\tau \\mid \\boldsymbol{y})\\), which can be simply obtained from the marginal joint posterior \\(p(\\mu, \\tau \\mid \\boldsymbol{y} )\\).1\nHowever, in this case, for each of the two parameters, conditionally on (i.e. given) the other, the posterior is in the same family of the prior — the posterior for \\(\\mu\\) is still a Normal and the posterior for \\(\\tau\\) is still a Gamma; what changes is the value of the “hyper-parameters” (i.e. the parameters of these distributions), which move from (\\(\\mu_0\\), \\(\\sigma_0^2)\\) to \\((\\mu_1 , \\sigma_1^2 )\\) and from \\((\\alpha_0, \\beta_0 )\\) to \\((\\alpha_1 , \\beta_1)\\). For this reason, this model is called “semi-conjugated”.\nThese relationships clarify that the (posterior) distribution of the mean \\(\\mu\\) depends (among other things) on the variance \\(\\sigma^2 = 1/\\tau\\) as well as that the posterior for the precision \\(\\tau\\) depends (among other things) on the mean \\(\\mu\\). Crucially, in this example, this dependence is known in closed form. A very useful implication of this set up is that it means that we can directly determine not just the distributional form of the posteriors (Normal for the mean and Gamma for the precision), but also the numerical value of the “hyper-parameters” (\\(\\mu_1, \\sigma_1^2 , \\alpha_1,\\beta_1 )\\).\nWhat do we really need to do? The point of this exercise is to create a routine that can simulate sequentially from these distributions, with the aim of obtaining a sample from the joint posterior distribution \\(p(\\mu, \\tau \\mid \\boldsymbol{y})\\). So, once the data y have been observed and we have fixed the values for the parameters of the prior distributions (\\(\\mu_0, \\sigma_0^2,\\alpha_0,\\beta_0)\\), we can use, for instance R, to simulate from the resulting full conditionals.\nAt each iteration, we will update sequentially the values of the two parameters: first we update \\(\\mu\\) by simulating from its full conditional, then we set the value of \\(\\mu\\) to the one we have just simulated and update the value of \\(\\tau\\) by randomly drawing from its full conditional. And we repeat this process “until convergence”.\n  Running the example Data and fixed quantities Suppose we have observed a sample of \\(n = 30\\) data points, for example, in R you may input the data onto your workspace using the following command.\n# Vector of observed data y = c(1.2697,7.7637,2.2532,3.4557,4.1776,6.4320,-3.6623,7.7567,5.9032, 7.2671,-2.3447,8.0160,3.5013,2.8495,0.6467,3.2371,5.8573,-3.3749, 4.1507,4.3092,11.7327,2.6174,9.4942,-2.7639,-1.5859,3.6986,2.4544, -0.3294,0.2329,5.2846) Assume also that you want to set the value of the parameters for the priors as \\(\\mu_0 = 0\\), \\(\\sigma_0^2 = 10000\\) and \\(\\alpha_0 = \\beta_0 = 0.1\\). In R we could define these using the following commands.\n# \u0026quot;Hyper-parameters\u0026quot; (ie parameters for the prior distributions) mu_0 = 0 sigma2_0 = 10000 alpha_0 = 0.01 beta_0 = 0.01 We may also want to define some “utility” variables, that can be used later on, for instance the sample size and observed sample mean, which we can input in R as in the following code.\n# Sample size and sample mean of the data n = length(y) ybar = mean(y) The actual values of these variables are now stored in the respective “objects” and can be accessed at any point, for instance the commands:\nn ## [1] 30 ybar ## [1] 3.343347 will output the sample size and mean.\n Initial values As seen in the lecture, in order to run the Gibbs sampling algorithm, we need to initialise the Markov chains. This essentially means telling the computer what values should be used at iteration 0 of the process for anything that is a) associated with a probability distribution (i.e. it is not known with absolute certainty); and b) has not been observed.\nWe can do this in R using the following code.\n# Sets the \u0026quot;seed\u0026quot; (for reproducibility). With this command, you will # *always* get the exact same output set.seed(13) # Initialises the parameters mu = tau = numeric() sigma2 = 1/tau mu[1] = rnorm(1,0,3) tau[1] = runif(1,0,3) sigma2[1] = 1/tau[1] First, we set the “random seed” (in this case to the value 13), which ensures replicability of the results. If you run this code on any machine, you will always and invariably obtain the same results.\nThen, we define the parameters mu and tau to be vectors, which in R we do by using the R built-in command numeric(). Basically, the command mu = tau = numeric() instructs R to expect these two objects to be vectors of (as yet) unspecified length (if you used the command x = numeric(5) we would define a vector of length 5).\nFinally, we set the first value of mu and tau to be randomly generated, respectively from a Normal(mean = 0, sd = 3) and a Uniform(0, 3). R has built-in commands to draw (pseudo-)random numbers — typically these are constructed using the prefix r (for “random”) and a string of text describing the distribution (e.g. norm for “Normal”, unif for “Uniform”, bin for “Binomial”, etc.). The first argument (input) to a call to a rxxxx(...) command is the number of values you want to simulate. So, for instance, rnorm(1000,0,6) instructs R to simulate 1000 values from a Normal distribution with mean 0 and standard deviation 6 — notice that, unlike BUGS, R parameterises the Normal in terms of mean and sd (instead of the precision). You can check the values that have been selected to initialise your Markov chain by simply typing the name of the variables (or some suitable function thereof).\nYou can check the values that have been selected to initialise your Markov chain by simply typing the name of the variables (or some suitable function thereof).\nmu ## [1] 1.662981 sqrt(sigma2) ## [1] 0.9249339  Running the Gibbs sampling Generally speaking, the actual Gibbs sampling is really simple (if the full conditionals are known analytically!) and reduces to code such as the following.\n# Sets the number of iterations (nsim) nsim = 1000 # Loops over to sequentially update the parameters for (i in 2:nsim) { # 1. Updates the sd of the full conditional for mu sigma_1 = sqrt(1/(1/sigma2_0 + n/sigma2[i-1])) # 2. Updates the mean of the full conditional for mu mu_1 = (mu_0/sigma2_0 + n*ybar/sigma2[i-1])*sigma_1^2 # 3. Samples from the updated full conditional for mu mu[i] = rnorm(1,mu_1,sigma_1) # 4. Updates the 1st parameter of the full conditional for tau alpha_1 = alpha_0+n/2 # 5. Updates the 2nd parameter of the full conditional for tau beta_1 = beta_0 + sum((y-mu[i])^2)/2 # 6. Samples from the updated full conditional for tau tau[i] = rgamma(1,alpha_1,beta_1) # 7. Re-scales the sampled value on the variance scale sigma2[i] = 1/tau[i] } If you check the code above, you should be able to see that it matches perfectly the mathematical expressions defined for the (updated) parameters of the full posterior distributions. The loop goes from 2 to nsim — the first value of the vectors mu, tau and sigma2 are filled at initialisation.2\nNote that the code above produces simulations from the full conditional of the precision \\(\\tau\\) , which are then rescaled to produce a vector of simulations from the joint posterior distribution for the variance \\(\\sigma^2\\) . It is of course very easy to also rescale these further to obtain a sample of values from the posterior of the standard deviation \\(\\sigma\\), for example using the following code.\nsigma = sqrt(sigma2) Once this code has been executed in R your output is made by vectors that, if the procedure has worked (i.e. it has converged), are drawn, at least with a very good degree of approximation, from the joint posterior distribution of the parameters.\nYou can also visualise a traceplot — in this particular case, convergence is not an issue (as the model is “semi-conjugated”) and even with a single chain, you can get a sense that all the traceplots are “fat, hairy caterpillars”, which indicates all is well.\nA simple code needed to produce these two plots is the following.\n# Histograms from the posterior distributions hist(mu) hist(sigma) # Traceplots plot(mu,t=\u0026quot;l\u0026quot;,bty=\u0026quot;l\u0026quot;) plot(sigma,t=\u0026quot;l\u0026quot;,bty=\u0026quot;l\u0026quot;) Once again, notice how powerful MCMC is: technically, you may not model directly a (highly non-linear) function of the main parameters; for example, all the computation is made in terms of the precision tau, although you may be much more interested in learning the standard deviation sigma. MCMC lets you obtain all the relevant informa- tion on the latter by simply creating simulations from the posterior distributions using the relevant inverse function sigma = 1/sqrt(tau).\nWe can also depict the trajectories of the MCMC samples in the first 10 iterations of the proecess. The arrows follow the moves of the Markov chain during the updates of the parameters, from the initial\n   The main intuition here is to do with the basic properties of conditional probabilities. Recall that given two events \\(A\\) and \\(B\\), then by definition \\(\\Pr(A \\mid B) = \\frac{\\Pr(A,B)}{\\Pr(B)}\\). Now we can extend this to the case of three events \\(A\\), \\(B\\) and \\(C\\), by adding \\(C\\) to the conditioning set (i.e. to the right of the “\\(\\mid\\)” symbol throughout the equation) and get \\(\\Pr(A \\mid B, C ) = \\frac{\\Pr(A,B\\mid C)}{Pr(B\\mid C)}\\). If we replace \\(\\mu\\) for \\(A\\), \\(\\sigma^2\\) for \\(B\\) ,\\(\\boldsymbol{y}\\) for \\(C\\) and a probability distribution \\(p(\\cdot)\\) for the probability associated with an event \\(\\Pr(\\cdot)\\), we can write \\(p(\\mu \\mid \\sigma^2, \\boldsymbol{y} ) = \\frac{p(\\mu,\\sigma^2\\mid \\boldsymbol{y})}{p(\\sigma^2\\mid \\boldsymbol{y})}\\propto p(\\mu, \\sigma^2 \\mid \\boldsymbol{y} )\\). Similarly, \\(p(\\sigma^2 \\mid \\mu, \\boldsymbol{y}) = \\frac{p(\\mu,\\sigma^2\\mid\\boldsymbol{y})}{p(\\mu\\mid\\boldsymbol{y})} \\propto p(\\mu,\\sigma^2\\mid\\boldsymbol{y})\\). We can then see that each full conditional is proportional to the target distribution (i.e. the joint posterior of all the parameters). Thus, sampling repeatedly from all the full conditionals essentially gives us something that is, broadly speaking, proportional to our target joint posterior distribution. Formal theorems ensure that if we do this long enough, we are guaranteed to actually approximate the target to an arbitrary degree of precision.↩︎\n NB: there is a slight confusion in the terminology: usually, we refer the initialisation of the process as iteration 0. However, R does not let you index a vector with the number 0, i.e. the first element of a vector is indexed by the number 1. Thus, what in the Lecture 2 was indicated as \\(\\mu^{(0)}\\) is actually mu[1] in the R code. Similarly, the updated value for \\(\\mu\\) at iteration 2 is indicated as \\(\\mu^{(2)}\\) in the slides, but as mu[3] in the R code.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0135d0e0d165c625493de072b2d547f1","permalink":"/practical/02_mcmc/tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/practical/02_mcmc/tutorial/","section":"practical","summary":"Introduction This short tutorial will guide you through the example of Gibbs sampling shown in class.\nAs a quick reminder, the example does not really require Gibbs sampling or any form of MCMC to estimate the joint posterior distribution for the parameters.","tags":[],"title":"How do MCMC and Gibbs sampling really work?","type":"book"},{"authors":null,"categories":[],"content":" Introduction to R and cost-effectiveness analysis using BCEA — SOLUTIONS MCMC in R/BUGS The model file is fairly simple in this case and it is coded as in the following\n# Model for Laplace\u0026#39;s analysis of birth\u0026#39;s data model { y ~ dbin(theta,n) theta ~ dunif(0,1) } which you can save on a file ModelLaplace.txt. Of course, the naming convention is completely up to you and you can use any name or file extension, for that matter, e.g. something like model.bug is perfectly acceptable to BUGS, as long as all is consistent in your call from R\nThe next step consists into inputting the data in the R workspace. Assuming you have opened R, you can point it in the “working” directory, i.e. the folder in which you have stored the relevant files, say something like C:\\gianluca if you are under MS Windows. R uses Unix notation and thus paths to folder are characterised by a slash (not a forward slash, as in MS Windows). Thus you can point R to the relevant directory using the setwd command, for example as in the following.\nAn alternative way of specifying a path in R is to use “double forward slash”, as in the following\nTo R , both commands have the same meaning.\nNow we can start inputting the data in the workspace. Because the example is so simple, we can simply write down the values we want to associated with each of the relevant variables, for example as in the following commands.\ny=241945 n=493527 data=list(y=y,n=n) The first two commands define new variables y and n and assign them the relevant values. The third line creates a new R object — that is a list in which we store y and n. Lists are very helpful because they act as boxes in which you can put variables of different nature (scalars, matrices, strings, numbers, etc.). The way R defines variables and lists in particular is rather straightforward: notice however that you can assign names to elements of a list — we do this here by using the option y=y,n=n.\nYou can inspect the list by using various R commands, for example as in the following.\nnames(data) [1] \u0026quot;y\u0026quot; \u0026quot;n\u0026quot; data$y [1] 241945 The command names returns the names of the objects included inside the data list, while the “$” operator allows us to access objects contained inside other objects. So the command data$y looks inside the object data and returns the value of the object y.\nThe next step we need to follow before we can run our model using BUGSis to set up the environment and main variables for the MCMC. We do this using the R commands below.\nfilein=\u0026quot;ModelLaplace.txt\u0026quot; params=\u0026quot;theta\u0026quot; inits_det=list(list(theta=.1),list(theta=.9)) inits_ran=function(){list(theta=runif(1))} The first one creates a string variable with the path to the model file. In this case, we assume that the .txt file is stored in the working directory and so all that is necessary is a pointer to the file name. If the .txt were stored somewhere, e.g. in the folder C:\\gianluca\\myfiles, then we would need to specify the full path to the file, e.g. filein=C:/gianluca/myfiles/ModelLaplace.txt. Of course, there is nothing special about the name filein — and you can use any name you like, provided you are then consistent in the call to BUGS.\nThe second command defines the parameters to be monitored. In this case, the model is so simple that it only has one parameter, so we simply define a string variable params. If we had more than one parameter, we would need to create a vector of names — in R we can do this by using the “collection” operator, for example as in the following.\nparams=c(\u0026quot;theta1\u0026quot;,\u0026quot;theta2\u0026quot;,\u0026quot;theta3\u0026quot;) The third and fourth commands show two different ways of creating initial values to pass to BUGS. The first one uses deterministic initial values, in the sense that you are passing specific values to R , which it in turn will send to BUGS. If you want to do this, you need to create a “list of lists”. In other words, first you define a variable (in this case inits_det) as a list. Then, you create other lists inside it — each of this list should contain a specific value for each of the variables you need to initialise. The number of lists inside the upper-level list is defined by the number of Markov chain you want to run (see below and the lecture slides). You can explore the list you have just created by simply calling its name.\ninits_det [[1]] [[1]]$theta [1] 0.1 [[2]] [[2]]$theta [1] 0.9 This may be a bit daunting at first glance, but once you get your head around it, the way in which R manages its objects is actually very informative and very much linked with its object-oriented programming nature. The output is characterised by a series of “tags”. For example, the tag [[1]] indicates the first element (i.e. the first list included) in the main object (i.e. the variable inits_det). So for example, you can access elements inside an object using the “‘double square bracket’’ and/or the “$” notation, for example as in the following.\ninits_det[[1]] $theta [1] 0.1 inits_det[[1]]$theta [1] 0.1 The lower level tag [1] is used to indicate the variables stored inside the upper-level element of the object (e.g. the value 0.1 that we have assigned to theta in the first list inside inits_det).\nThe fourth command defines “random” initial values. To do so, instead of you passing a specific value, you can create a R function that simulates them from a suitable probability distribution. In this case, we know that theta is defined in our model code to be a probability and so it has to range between 0 and 1. As we have seen in the classes, a Uniform distribution may be a good candidate and thus in this case we set up a little function that creates a list and puts inside it a number generated randomly from a Uniform distribution. In R, built-in functions such as runif have default values. So a call\nrunif(1) [1] 0.1834258 randomly samples 1 value from a Uniform distribution defined in the default range \\([0;1]\\). You can get more information on R functions typing the command help(name_of_the_function) to your R terminal.\nAt this point we are finally ready to call BUGSin background and do the MCMC simulation for us. We can do so using the following code.\nlibrary(R2OpenBUGS) model= bugs(data=data,inits=inits_det,parameters.to.save=params,model.file=filein, n.chains=2,n.iter=10000,n.burnin=4500,n.thin=1,DIC=TRUE)  Firstly we load the package R2OpenBUGS, which allows us to link R to BUGS. Then we define a new object model, which will collect the output of the call to the function bugs. This takes several arguments. In this case, we are using the deterministic initial values and considering 2 chains. We ask BUGSto do a burn-in (cfr. slides as well as BMHE and The BUGS Book) of 4500 iterations and then simulate for another 10000 iterations after that. We do not require thinning (or, equivalently, a thinning of 1, which simply means that all the 10000 simulations after burn-in are stored for our analysis). Finally, we instruct BUGSto compute the DIC (more on this later — do not worry about it for now).\nThe commands\nnames(model)  [1] \u0026quot;n.chains\u0026quot; \u0026quot;n.iter\u0026quot; \u0026quot;n.burnin\u0026quot; \u0026quot;n.thin\u0026quot; [5] \u0026quot;n.keep\u0026quot; \u0026quot;n.sims\u0026quot; \u0026quot;sims.array\u0026quot; \u0026quot;sims.list\u0026quot; [9] \u0026quot;sims.matrix\u0026quot; \u0026quot;summary\u0026quot; \u0026quot;mean\u0026quot; \u0026quot;sd\u0026quot; [13] \u0026quot;median\u0026quot; \u0026quot;root.short\u0026quot; \u0026quot;long.short\u0026quot; \u0026quot;dimension.short\u0026quot; [17] \u0026quot;indexes.short\u0026quot; \u0026quot;last.values\u0026quot; \u0026quot;isDIC\u0026quot; \u0026quot;DICbyR\u0026quot; [21] \u0026quot;pD\u0026quot; \u0026quot;DIC\u0026quot; \u0026quot;model.file\u0026quot;  model$n.iter [1] 10000 will return the list of names for all the elements included in the model object; and access the value of the element n.iter included inside the object model, respectively.\nThe R2OpenBUGS has a “print” method — meaning that we can display the results of our model in a tabular form, simply using the following command (the option digits=3 defines the number of significant figures to be displayed).\nprint(model,digits=3) Inference for Bugs model at \u0026quot;ModelLaplace.txt\u0026quot;, Current: 2 chains, each with 10000 iterations (first 4500 discarded) Cumulative: n.sims = 11000 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff theta 0.490 0.001 0.489 0.49 0.49 0.491 0.492 1.001 11000 deviance 14.562 1.427 13.560 13.66 14.02 14.870 18.740 1.002 3200 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = Dbar-Dhat) pD = 1.002 and DIC = 15.560 DIC is an estimate of expected predictive error (lower deviance is better). This returns a table with summary statistics from the posterior distribution of the nodes we have instructed R and BUGSto monitor. Notice that BUGSwill automatically monitor a node deviance — see Lecture 5. For now, let us not worry about it!\nIn this case, we can easily assess that the estimate for the probability of a girl being born is on average 0.490 and we can approximate a 95% credible interval by considering as lower and upper limits the 2.5% and 97% quantiles of the posterior distribution. In this case, the interval estimate is 0.489 to 0.492. Because the entire interval is below the threshold of 0.5, Laplace concluded in his analysis that it was “morally certain” that a boy birth was more likely than a girl (see BMHE, chapter 2).\nIn any MCMC analysis, convergence is a crucial point. Because the Uniform distribution (that we have used as prior for theta) can be considered as a special case of the Beta distribution, we know from Lecture 3 that actually ours is a case of Binomial-Beta conjugate model. Thus, we do not really have issues with convergence and in fact the BUGSmodel is not even really a MCMC exercise (the simulations are still obtained but all the calculations are done in close-form because of conjugacy). This is reflected in the value of the potential scale reduction factor (or Gelman-Rubin statistic \\(\\hat{R}\\)), described in the column headed Rhat, which show values well below the threshold of 1.1. Similarly, autocorrelation is not an issue as the effective sample size (described in the column headed neff) is exactly identical with the actual sample size. In particular, notice that: we have 2 chains, we run the model for 10000 iterations per chain after a burn-in of 4500 iterations per chain that are discarded. This makes the total number of simulations used for our analysis \\(2\\times (10000-4500)=11000\\). This number is stored also in the object model$n.sims.\nA handy workaround that allows us to avoid using the “$” or “double square bracket” notation is to “attach” an object to the workspace. This means that we effectively bypass the upper-level object and make its elements available directly to R and we can do this by using the command\nattach.bugs(model)  NB: It is possible that this command does not produce the required effect (in newer versions of R2OpenBUGS). An alternative that should work is\nattach.all(model) (i.e. use the function attach.all, still from the package R2OpenBUGS, where the argument in brackets is the name of the BUGS model for which you want to make all the output directly available on your R workspace).   Notice that this operation comes with some risks — every variable that is currently present in the R workspace with the same name as a variable included in the object model will be now overwritten. Nevertheless, we can now use all the elements of model directly; for example we can produce a histogram of the posterior distribution for theta simply using the following command.\nhist(theta) which confirms that, effectively, there is no chance that theta is greated than 0.5.\n  Health economic evaluation in R using BCEA First we load a dataset that has been previously saved. There are several format to which you can save R data or R workspaces — in this case we use the format .RData. We can load a .RData file simply using the built-in R command load.\nload(\u0026quot;Vaccine.RData\u0026quot;) names(he)  [1] \u0026quot;n_sim\u0026quot; \u0026quot;n_comparators\u0026quot; \u0026quot;n_comparisons\u0026quot; \u0026quot;delta_e\u0026quot; [5] \u0026quot;delta_c\u0026quot; \u0026quot;Kmax\u0026quot; \u0026quot;k\u0026quot; \u0026quot;ib\u0026quot; [9] \u0026quot;eib\u0026quot; \u0026quot;kstar\u0026quot; \u0026quot;best\u0026quot; \u0026quot;ref\u0026quot; [13] \u0026quot;comp\u0026quot; \u0026quot;step\u0026quot; \u0026quot;interventions\u0026quot; \u0026quot;delta.e\u0026quot; [17] \u0026quot;delta.c\u0026quot;  The file Vaccine.RData contains the object he, which in turns has several elements in it.\nWe can now load the package BCEA (assuming you have already installed it) and use it to post-process he and produce relevant summaries/analyses.\n The practical uses the current version of BCEA, which is available from CRAN. However, the development version is in the “beta-testing” version and will soon replace the current stable version. As this will be a major update, some of the commands below may break in the future — differences will be minor and changes will be easy to address.   library(BCEA)  Attaching package: \u0026#39;BCEA\u0026#39; The following object is masked from \u0026#39;package:graphics\u0026#39;: contour ICER=mean(he$delta.c)/mean(he$delta.e) ICER [1] 20097.59 The first thing we need to do is to compute the ICER. If you check with your slides for Lecture 3, you will see that \\[\\mbox{ICER} = \\frac{\\mbox{E}[\\Delta_c]}{\\mbox{E}[\\Delta_e]}\\] and so we create a new variable ICER to which we assign as value the ratio of the mean of the element he$delta.c to the mean of the element he$delta.e. Its value is returned to be 20097.59. The only difficulty of this part is to realise how to translate the correct formula for the ICER into R commands (i.e. you need to take the ratio of the means, not the mean of the ratio!) and to access the elements delta.e and delta.c from inside the main object he.\nSecondly, we are asked to compute the Expected Incremental Benefit (EIB) for a value of the willingness to pay threshold \\(k=30000\\). Now, from the Lecture we know that \\[\\mbox{EIB} = k\\mbox{E}[\\Delta_e] - \\mbox{E}[\\Delta_c]\\] and so we can easily compute this using the following R commmands.\nk=30000 EIB=k*mean(he$delta.e)-mean(he$delta.c) EIB [1] 2.48131 Because for this willingness to pay threshold \\(\\mbox{EIB}\u0026gt;0\\), then the new treatment \\(t=1\\) is more cost-effective than the comparator \\(t=0\\).\nNext, we are required to display the cost-effectiveness plane, using the BCEA built-in command ceplane.plot. This gives the following result.\nceplane.plot(he,wtp=10000) The reason why BCEA says “ICER=NULL” in the top-right corner of the graph is because the object he has been modified from the standard BCEA output (effectively, the variable ICER is computed within a BCEA object. But because you were required to compute the ICER yourselves, this has been removed and so ceplane.plot does not know what value to use. If you are bothered by this, you can simply define in R he$ICER=ICER and re-run the ceplane.plot command. Now BCEA will print on the graph the value of the ICER.\nIn any case, because we have selected a different threshold (\\(k=10\\,000\\) instead of \\(k=30,000\\) as before), this time the ICER (the red dot in the graph) is not included in the “sustainability area” (the grey area in the plane). For this reason, we can conclude that at this new threshold, \\(t=0\\) is more cost-effective.\nIf we now execute the following command\neib.plot(he, plot.cri=FALSE) we obtain a plot of the EIB for different values of the willingness to pay. The interpretation of this graph is that for willingness to pay up to about 20100, then \\(t=0\\) is the most cost-effective treatment (because EIB\\(\u0026lt;0\\)). After that, \\(t=1\\) becomes more cost-effective and EIB\\(\u0026gt;0\\) indicating that the new treatment has a greater utility than the comparator.\nFinally, we can use BCEA to show a contour plot of the economic results, by using the following command.\ncontour(he) The probability that the new intervention (vaccination) is dominated by the reference (status quo) is estimated by the proportion if points in the cost-effectiveness plane that lie in the NW quadrant. This is reported by the graph as 0.169.\n ","date":1655742600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655742600,"objectID":"d33c5529730d40ed1b82f64a7d2337fe","permalink":"/practical/03_bcea/solutions/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/03_bcea/solutions/","section":"practical","summary":"Introduction to R and cost-effectiveness analysis using BCEA — SOLUTIONS MCMC in R/BUGS The model file is fairly simple in this case and it is coded as in the following","tags":[],"title":"Practical 3. Introduction to R and cost-effectiveness analysis using BCEA - SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" When dealing with individual level data, regression models are arguably one of the most commonly used tools in statistical analysis. As discussed here (which gives more details and background), the term “regression” was introduced by Francis Galton.\n Historical background Galton was a very controversial figure. He was a polyscientist, who made contributions to Statistics, Psychology, Sociology, Anthropology and many other sciences. He was the half-cousin of Charles Darwin and was inspired by his work on the origin of species to study hereditary traits in humans, including height, which he used to essentially invent regression. He also established and then financed the “Galton Laboratory” at UCL (that is the first incarnation of the UCL Department of Statistical Science), which Karl Pearson (another important and controversial figure) went on to lead. Alas, both Galton and Pearson wer also a major proponent of eugenics (in fact, Galton is credited with the invention of the term) and has thus left a troubling legacy behind them. You can read more about UCL’s own enquiry here.   Galton worked to study hereditary traits. In particular, he collected data on \\(n=898\\) children from 197 families. The original data are stored in the file Galton.txt. The data comprise the height of each child \\(y_i\\), as well as the height of the father \\(X_{1i}\\) and the mother \\(X_{2i}\\), for \\(i=1,\\ldots,n\\), all measured in inches.\nGalton’s objective was to find out whether there was some consistent relationship between the outcome and the predictor and, if so, to quantify the strength of this relationship. His original analysis was based on least squares fitting. But in general terms, regression can be framed as a statistical model; in its simplest form (which we are using here), we can assume that the sampling variability underlying the observed data can be described by a Normal distribution. This amount to assuming that \\[\\begin{align} y_i \u0026amp; \\sim \\mbox{Normal}(\\mu_i,\\sigma^2) \\\\ \\mu_i \u0026amp; = \\beta_0 + \\beta_1 X_{1i} + \\ldots + \\beta_K X_{Ki}.\\tag{1} \\end{align}\\] for \\(i=1,\\ldots,n\\) (and assuming that there are \\(K\\) “covariates” \\(X_1,\\ldots,X_K\\)). This notation highlights the probabilistic nature of the assumed regression relationship between \\(y\\) and \\(\\boldsymbol{X}\\): we are assuming that the linear predictor of Equation (1) is on average the best estimate of the outcome, given a specific “profile”, i.e. a set of values for the observed covariates. But we do not impose determinism on this relationship: there is a variance, which is determined by the sampling variability in the observed data and expressed by the population parameter \\(\\sigma^2\\).\n An alternative (but entirely equivalent!) way of writing the regression model (which is perhaps more common in Econometrics than it is in Statistics) is to consider\n\\[\\begin{align}y_i = \\beta_0 + \\beta_1 X_{1i} + \\ldots + \\beta_K X_{Ki} + \\varepsilon_i,\\end{align}\\]\nwith \\(\\varepsilon_i \\sim \\mbox{Normal}(0,\\sigma^2)\\).\nThis formulation explicitly describes the assumption mentioned above. The quantity \\(\\varepsilon_i\\) represents some kind of “white noise”, or, in other words, a random error, that is centered on 0 and whose variance basically depends on the population variability.   We can follow the model specification described here. The data can be loaded up in the R workspace using the following code.\n# Load the whole dataset data=read.table(\u0026quot;tutorial-regression/Galton.txt\u0026quot;,header=TRUE) # Creates the covariates matrix. Notice that the first column is made by \u0026#39;1\u0026#39; and the father\u0026#39;s and mother\u0026#39;s heights are \u0026quot;centered\u0026quot; X=cbind(rep(1,nrow(data)),scale(data$Father,scale=F),scale(data$Mother,scale=F)) colnames(X)=c(\u0026quot;Intercept\u0026quot;,\u0026quot;Father\u0026quot;,\u0026quot;Mother\u0026quot;) # Creates the outcome (children\u0026#39;s height) y=data$Height The assumptions about the model for the data and the priors described here can be coded up into the following BUGS model\nmodel { for(i in 1:n) { y[i] ~ dnorm(mu[i],tau) # If you are using R2jags, you can take advantage of the \u0026#39;%*%\u0026#39; operator mu[i] \u0026lt;- X[i,]%*%beta # If you are using R2OpenBUGS, you need to specify the full linear predictor # mu[i] \u0026lt;- X[i,1]*beta[1] + X[i,2]*beta[2] + X[i,3]*beta[3] } for(k in 1:K) { beta[k] ~ dnorm(mu.beta[k],tau.beta[k]) } tau ~ dgamma(a,b) sigma \u0026lt;- pow(tau,-.5) } You can save this model into a .txt file (say, model.txt) and use it to run BUGS or JAGS. Notice that if you’re doing the former, you will need to use the specification of the linear predictor mu[i] that is currently commented out (ie it is prefixed by a “#” in the code above). This is because JAGS implements the operator “%*%”, which is the sum of the products between the covariate \\(X_k\\) and its coefficient \\(\\beta_k\\). So the notation\nmu[i] \u0026lt;- X[i,]%*%beta is equivalent to \\[\\sum_{k=1}^3 X_{ik}\\beta_k\\] (assuming that X is a matrix with \\(n\\) rows and \\(K=3\\) columns and beta is a vector with length \\(K=3\\)).\nYou now need to define the data list, the parameters to be monitored and the other relevant quantities (number of chains, iterations, burn-in — as seen in class). For example, you can use the following code.\n# Defines the data list dataList \u0026lt;- list( y=y, # outcome (children\u0026#39;s heights) X=X, # covariates matrix (including a column of \u0026#39;1\u0026#39; for the intercept) n=nrow(X), # number of data points (= rows of X) K=ncol(X), # number of covariates (= columns of X) # \u0026quot;hyper-parameters\u0026quot; a=0.1,b=0.1,mu.beta=c(65,0,0),tau.beta=c(20,10,10)^-2 ) # MCMC-related variables n.chains=2 # 2 chains n.iter=10000 # 10000 iterations n.burnin=5000 # 5000 to be discarded as burn-in parameters.to.save=c(\u0026quot;beta\u0026quot;,\u0026quot;sigma\u0026quot;) # monitored parameters At this point, you can use R2OpenBUGS or R2jags to run the model. If you’re using the former, the following code would do the job\nlibrary(R2OpenBUGS) model=bugs(data=dataList,model.file=\u0026quot;model.txt\u0026quot;,n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin) while if you want to use R2jags you can simply replace the command library(R2OpenBUGS) with library(R2jags) and call jags(...) instead of bugs(...).\nWith this setup, you should be able to replicate the results shown in the notes.\n You can also use this script, which is annotated with instructions and will allow you to replicate the whole analysis presented in the notes (including the graphs and the post-processing). For simplicity, the BUGS code is also stored here.    NB: This document shows a more detailed introduction to the basics of regression and this also describes the issue related with centering covariates in a regression model.\nThe impact of covariate centering on Bayesian computation is also presented in Chapter 2 of BMHE.   ","date":1655805600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655805600,"objectID":"da0a717906aa9d6a87a51854da282218","permalink":"/practical/04_ild/tutorial-regression/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/04_ild/tutorial-regression/","section":"practical","summary":"When dealing with individual level data, regression models are arguably one of the most commonly used tools in statistical analysis. As discussed here (which gives more details and background), the term “regression” was introduced by Francis Galton.","tags":[],"title":"Linear regression Tutorial","type":"book"},{"authors":null,"categories":[],"content":" This document comments more in details the R script used to perform the whole analysis. Similar results can be obtained using the BUGS interface directly — but as we mentioned in the class, this is usually less efficient and so we focus here on the R script.\nThe first thing to do is to make sure that you are in the correct folder. You can check the location of your R workspace by typing the following command\ngetwd() which would return something like\n[1] \u0026quot;/home/gianluca\u0026quot; — the symbol [1] here indicates the the answer is a vector and the current rows starts with its first (and, in this case, only) element.\nYou can move to different directories by using a command like the following\nsetwd(\u0026quot;/home/gianluca/Mystuff\u0026quot;) which, in this case, would move the workspace in the subfolder Mystuff. In Rstudio you can also use the menus under Session to move across the folders on your computer.\nThe next step is to load in the R workspace the relevant “packages”. In this case, this is done using the following command (that are typeset in red).\nlibrary(R2OpenBUGS) You can of course load any other package you require at any point in your script.\nQuestion 1 We now proceed to load the data on costs, using the following R command (here the hash symbol “#” indicates a comment)\n# Loads the data on costs only into R from the txt file (list originally prepared for BUGS) cost.data=source(\u0026quot;cost-data.txt\u0026quot;)$value The R command source executes into the R workspace the commands included in the file that is used as its argument. In this case, the file cost-data.txt contains a list of values for a set of different variables and the above comment assigns this list to a new R variable named cost.data. Notice here that we also add the suffix $value to the source command. The reason for this is that in this way, R can strip the values for the elements of the list contained in the .txt file from all the “meta-data”” (i.e. external information that is irrelevant to us). For example, compare the following output\ncost.data=source(\u0026quot;cost-data.txt\u0026quot;)$value names(cost.data) [1] \u0026quot;N1\u0026quot; \u0026quot;N2\u0026quot; \u0026quot;cost1\u0026quot; \u0026quot;cost2\u0026quot; with the one obtained by omitting the $value suffix.\ncost.data=source(\u0026quot;cost-data.txt\u0026quot;) names(cost.data) [1] \u0026quot;value\u0026quot; \u0026quot;visible\u0026quot; names(cost.data$value) [1] \u0026quot;N1\u0026quot; \u0026quot;N2\u0026quot; \u0026quot;cost1\u0026quot; \u0026quot;cost2\u0026quot; In the first case, the elements of the object cost.data are the variables that we need to use as data for the BUGSmodel. In the second one, these are actually “hidden” inside the object values that is stored inside the object cost.data.\nWe are now ready to start setting everything up to run BUGSremotely from our R session. The first things to do are to define the relevant inputs to the BUGS function that will do just that.\n# Runs the BUGS model from R model.file=\u0026quot;normal-mod.txt\u0026quot; params \u0026lt;- c(\u0026quot;mu\u0026quot;,\u0026quot;ss\u0026quot;,\u0026quot;ls\u0026quot;,\u0026quot;delta.c\u0026quot;,\u0026quot;dev\u0026quot;,\u0026quot;total.dev\u0026quot;) n.chains=2 n.burnin=1000 n.iter=2000 n.thin=1 debug=FALSE  The first command here defines a string with the path to the file in which we have saved the model code. This will instruct BUGSabout where to read the model assumptions.\n The second line defines the parameters to be monitored and stores them (as strings of names) into the vector params — of course you can use any naming convention you like; so this vector may be called parameters, or x instead.\n The third line defines a numeric variable in which we specify the number of Markov chains we want to run — in this case 2. It is usually a good idea to run more than one chain, because by starting them from different initial values, this helps assessing convergence to the target posterior distributions of all the relevant variables in our model.\n The fourth line defines the number of iterations to be used as “burn-in”, i.e. to get closer to the core of the target posterior distributions. In this case, we are selecting 1000 — there is no reason why 1000 should always be sufficient, so we should assess convergence very carefully (more on this later).\n The fifth line defines the total number of simulations to be run, which will be used to obtain the samples that we will use for the analysis after discarding the burn-in. In this case, we set n.iter=2000, which means that, in total, we will run the MCMC for \\(2\\times(2000)=4000\\) simulations (because we have selected 2 chains). Of these, the first \\(2\\times 1000=2000\\) will be discarded as burn-in, which means that the summary statistics will be computed on a sample of \\(2000\\) simulations.\n The sixth line defines the “thinning” of the chains. In this case, n.thin=1, which means that we are actually storing every single iteration after the burn-in for our analysis. In general, BUGSwill save 1 every n.thin simulations, so using a thinning level above 1 means that some of the simulations will be discarded (and consequently, to have the same overall sample size we will need a longer run of the MCMC). This may help reduce autocorrelation in our results (see the lecture slides and both BMHE and The BUGS Book, for more details).\n Finally, the seventh line defines a logical variable debug. In this case we set it to the value FALSE, which means that BUGSwill be called remotely and we will not see it in action. If we set debug=TRUE, then BUGSwould forcibly take over from R and we would see it opening its windows and spitting out its output. This works under MS Windows only.\n  At this point we are finally ready to call BUGS, which we do using the following command.\nm=bugs(data=cost.data,inits=NULL,model.file=model.file,parameters.to.save=params, n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T,debug=debug) The model is run by BUGSand while this is happening, we lose access to the R session (i.e. you cannot use R to make other calculations while BUGSis running). When BUGSis finished, then R takes over again and if everything has worked, the object m is stored in our workspace. We can inspect it by using the following command.\nnames(m)  [1] \u0026quot;n.chains\u0026quot; \u0026quot;n.iter\u0026quot; \u0026quot;n.burnin\u0026quot; \u0026quot;n.thin\u0026quot; [5] \u0026quot;n.keep\u0026quot; \u0026quot;n.sims\u0026quot; \u0026quot;sims.array\u0026quot; \u0026quot;sims.list\u0026quot; [9] \u0026quot;sims.matrix\u0026quot; \u0026quot;summary\u0026quot; \u0026quot;mean\u0026quot; \u0026quot;sd\u0026quot; [13] \u0026quot;median\u0026quot; \u0026quot;root.short\u0026quot; \u0026quot;long.short\u0026quot; \u0026quot;dimension.short\u0026quot; [17] \u0026quot;indexes.short\u0026quot; \u0026quot;last.values\u0026quot; \u0026quot;isDIC\u0026quot; \u0026quot;DICbyR\u0026quot; [21] \u0026quot;pD\u0026quot; \u0026quot;DIC\u0026quot; \u0026quot;model.file\u0026quot;  There are many variables inside the object m and we can use the “$” operator in R to access them. For example, the command\nm$n.sims [1] 2000 returns the total number of simulations used by BUGSto compute the posterior distributions.\nThe first thing to do once the model has run is to check the summary statistics for the posterior distributions, together with the convergence diagnostics. We can do this using the print function. For example, the command\nprint(m,digits=2) Inference for Bugs model at \u0026quot;normal-mod.txt\u0026quot;, Current: 2 chains, each with 2000 iterations (first 1000 discarded) Cumulative: n.sims = 2000 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff mu[1] 22.72 1.18 20.46 21.90 22.71 23.51 25.10 1 2000 mu[2] 24.53 1.28 22.09 23.69 24.52 25.39 27.12 1 2000 ss[1] 486.18 38.60 418.00 459.65 483.50 509.12 568.61 1 2000 ss[2] 550.47 41.47 477.48 520.80 548.55 576.90 634.80 1 2000 ls[1] 3.09 0.04 3.02 3.06 3.09 3.12 3.17 1 2000 ls[2] 3.15 0.04 3.08 3.13 3.15 3.18 3.23 1 2000 delta.c 1.81 1.74 -1.62 0.63 1.83 3.04 5.06 1 2000 dev[1] 2995.63 2.02 2994.00 2994.00 2995.00 2996.00 3001.00 1 1900 dev[2] 3064.17 2.07 3062.00 3063.00 3064.00 3065.00 3070.00 1 2000 total.dev 6059.77 2.81 6056.00 6058.00 6059.00 6061.00 6067.00 1 2000 deviance 6059.77 2.81 6056.00 6058.00 6059.00 6061.00 6067.00 1 2000 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = Dbar-Dhat) pD = 3.90 and DIC = 6064.00 DIC is an estimate of expected predictive error (lower deviance is better). shows the summary table for the nodes (variables) we have chosen to monitor. The optional input digits=2 instructs R to show the results using 2 significant figures. The table reports the mean, standard deviation and selected quantiles of the posterior distributions. We can typically use the 2.5% and the 97.5% quantiles to approximate a 95% credible interval. In addition, the table reports the values for \\(\\hat{R}\\), the potential scale reduction (or Gelman-Rubin statistic) and the effective sample size (n.eff) — see the lecture slides and both BMHE and The BUGS Book, for more details.\nGiven this analysis, the mean cost is 22.72 for the control arm and 24.53 for the intervention arm. The mean difference in costs is 1.81 — recall that costs are entered in BUGS\\(\\times 1000\\). We can also manipulate the simulations to produce further analyses. For example, we can use the code\nplot( m$sims.list$mu[,1], m$sims.list$mu[,2], xlab=\u0026quot;Population average cost in arm 1 (x 1000)\u0026quot;, ylab=\u0026quot;Population average cost in arm 2 (x 1000)\u0026quot;, pch=20, cex=.6, main=\u0026quot;Joint distribution of mean costs\u0026quot; ) to display (in the graph below) the posterior joint distribution of the population average costs in the two arms (the R script provided for the practical provides also some description of the graphical parameters used in this call).\nNotice again the use of the “$” operator to access elements of the object m. In this case, the element sims.list is a list containing all the simulated values for all the monitored nodes. You can actually inspect them with the following code.\nnames(m$sims.list) [1] \u0026quot;mu\u0026quot; \u0026quot;ss\u0026quot; \u0026quot;ls\u0026quot; \u0026quot;delta.c\u0026quot; \u0026quot;dev\u0026quot; \u0026quot;total.dev\u0026quot; [7] \u0026quot;deviance\u0026quot;  head(m$sims.list$mu[,1]) [1] 21.77 23.83 20.31 22.58 23.79 23.76 The R command head shows by default the first 6 values of a variable.\nNotice here the interesting fact that BUGSeffectively adds a dimension to each node. So the node mu is defined in the model code as a vector with 2 values (one for arm 1 and the other for arm 2). But because when processed by BUGSwe record for each of these two elements n.sims simulations, then the resulting object turns into a matrix with n.sims = 2000 rows and 2 columns. So, in reference to the code used to generate the plot of the joint distribution of the mean costs, m$sims.list$mu[,1] indicates all the rows and the first column and m$sims.list$mu[,2] indicates all the rows and the second column of the matrix m$mu.\n Question 2 We have already included in the vector params the nodes related to the deviance — these are dev and total.dev, which are coded in the BUGSmodel to compute manually the deviance associated with the underlying Normal model. In practice you do not need to do this and BUGSwill calculate (and monitor) the deviance automatically — assuming that there is at least one observed variable (you can think of why this is!).\nGoing back to the summary statistics table, we can see that the overall model deviance is, on average, 6059.77. This is the same value, whether computed manually (in the node total.dev) or automatically (in the node deviance).\n Question 3 The first thing to do now is to load the new dataset, which includes data on costs as well as utilities. Then we can setup our call to BUGSpointing to the correct model file. Finally, because this new model (based on Gamma-Gamma structure — see the lecture slides) is more complex, we need to provide BUGSwith a set of suitable initial values (identified using a trial-and-error procedure). We do this using the following R commands.\n# Loads the data on costs only into R from the txt file (list originally prepared for BUGS) cost.utility=source(\u0026quot;cost-util-data.txt\u0026quot;)$value # Specifies the new model file model.file=\u0026quot;cgeg-mod.txt\u0026quot; # Loads the 3 sets of initial values from the .txt files inits1=source(\u0026quot;cgeg-inits1.txt\u0026quot;)$value inits2=source(\u0026quot;cgeg-inits2.txt\u0026quot;)$value inits3=source(\u0026quot;cgeg-inits3.txt\u0026quot;)$value # And combines them into a single list inits=list(inits1,inits2,inits3) # Re-defines the list of parameters to be monitored params=c(\u0026quot;mu.c\u0026quot;,\u0026quot;mu.e\u0026quot;,\u0026quot;delta.c\u0026quot;,\u0026quot;delta.e\u0026quot;,\u0026quot;shape.c\u0026quot;,\u0026quot;shape.e\u0026quot;,\u0026quot;beta\u0026quot;,\u0026quot;INB\u0026quot;,\u0026quot;CEAC\u0026quot;) # Re-defines the burn-in, number of simulations and number of chains n.burnin=1000 n.iter=4000 # NB: this adds 3000 simulations to the 1000 of burnin n.chains=3 Notice that we need to be consistent in the definition of the number of chains and the set up of the inits. So if we select 3 chains, then inits needs to be a list comprising of 3 lists (as in this case) — failure to do this will result in R complaining and stopping with an error message.\nAfter that, we are ready to call BUGSand run the new model, which we do using the following command.\nm2=bugs(data=cost.utility,inits=inits,model.file=model.file,parameters.to.save=params, n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T,debug=debug) The results can be again printed in the form of a summary table.\nprint(m2,digits=2) Inference for Bugs model at \u0026quot;cgeg-mod.txt\u0026quot;, Current: 3 chains, each with 4000 iterations (first 1000 discarded) Cumulative: n.sims = 9000 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff mu.c[1] 23.47 1.99 20.02 22.09 23.31 24.69 27.83 1 700 mu.c[2] 25.99 2.02 22.47 24.57 25.84 27.24 30.38 1 3000 mu.e[1] 74.70 7.13 61.77 69.81 74.28 79.17 90.15 1 1300 mu.e[2] 81.05 8.14 66.62 75.26 80.63 86.45 97.78 1 6100 delta.c 2.52 2.80 -3.02 0.66 2.55 4.38 7.94 1 660 delta.e -6.34 10.74 -27.39 -13.84 -6.27 1.02 14.81 1 1800 shape.c[1] 1.18 0.09 1.01 1.12 1.18 1.24 1.36 1 3300 shape.c[2] 1.67 0.13 1.43 1.58 1.67 1.76 1.95 1 9000 shape.e[1] 0.41 0.03 0.35 0.39 0.41 0.43 0.46 1 9000 shape.e[2] 0.37 0.03 0.32 0.36 0.37 0.39 0.43 1 9000 beta[1] 0.16 0.02 0.12 0.14 0.16 0.18 0.21 1 1100 beta[2] 0.17 0.02 0.13 0.15 0.17 0.18 0.21 1 3100 INB[1] -2.52 2.80 -7.94 -4.38 -2.55 -0.66 3.02 1 660 INB[2] -3.15 3.57 -10.17 -5.55 -3.18 -0.74 3.81 1 730 INB[3] -3.79 4.46 -12.59 -6.81 -3.82 -0.74 4.87 1 810 INB[4] -4.42 5.43 -14.98 -8.15 -4.41 -0.68 6.01 1 900 INB[5] -5.06 6.42 -17.57 -9.50 -5.05 -0.62 7.23 1 970 INB[6] -5.69 7.44 -20.19 -10.84 -5.68 -0.61 8.70 1 1000 INB[7] -6.33 8.47 -23.01 -12.18 -6.36 -0.54 10.07 1 1100 INB[8] -6.96 9.51 -25.67 -13.52 -6.93 -0.46 11.42 1 1100 INB[9] -7.59 10.56 -28.38 -14.90 -7.60 -0.39 12.83 1 1200 INB[10] -8.23 11.61 -31.06 -16.23 -8.20 -0.27 14.26 1 1200 INB[11] -8.86 12.67 -33.72 -17.60 -8.83 -0.18 15.60 1 1300 CEAC[1] 0.18 0.39 0.00 0.00 0.00 0.00 1.00 1 680 CEAC[2] 0.19 0.39 0.00 0.00 0.00 0.00 1.00 1 810 CEAC[3] 0.20 0.40 0.00 0.00 0.00 0.00 1.00 1 910 CEAC[4] 0.21 0.41 0.00 0.00 0.00 0.00 1.00 1 1200 CEAC[5] 0.22 0.41 0.00 0.00 0.00 0.00 1.00 1 1300 CEAC[6] 0.23 0.42 0.00 0.00 0.00 0.00 1.00 1 2200 CEAC[7] 0.23 0.42 0.00 0.00 0.00 0.00 1.00 1 2800 CEAC[8] 0.23 0.42 0.00 0.00 0.00 0.00 1.00 1 2600 CEAC[9] 0.24 0.43 0.00 0.00 0.00 0.00 1.00 1 2000 CEAC[10] 0.24 0.43 0.00 0.00 0.00 0.00 1.00 1 2200 CEAC[11] 0.24 0.43 0.00 0.00 0.00 0.00 1.00 1 2300 deviance 9756.35 4.44 9750.00 9753.00 9756.00 9759.00 9766.00 1 2600 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = Dbar-Dhat) pD = 9.93 and DIC = 9766.00 DIC is an estimate of expected predictive error (lower deviance is better). As is possible to see, all values for \\(\\hat{R}\\) are essentially 1, indicating that the model has converged. Autocorrelation is somewhat large, as confirmed by the fact that the effective sample size is relatively small in comparison to the actual sample size (9000 simulations) for many if not all the nodes. In real-world analyses, this would grant further analysis — for example by inspecting the traceplots of the nodes and possibly running the model for longer or using thinning (see BMHE, chapter 4 for R code to create traceplots).\nThe next part consists in manipulating the object m2 in order to obtain more advanced analyses. Notice that in fact, we can use BCEA to do all these, but for the sake of practice, we use our own R code, in this case.\nThe first thing to do is a cost-effectiveness plot. Recall that this is obtained by plotting the joint distribution of \\(\\Delta_e,\\Delta_c\\). Thus, it is sufficient to access the nodes delta.e and delta.c inside the object m2 and the R function plot, as shown below.\nplot(m2$sims.list$delta.e,m2$sims.list$delta.c,pch=20,cex=.8,xlab=\u0026quot;Effectiveness differential\u0026quot;, ylab=\u0026quot;Cost differential\u0026quot;,main=\u0026quot;Cost-effectiveness plane\u0026quot;) abline(v=0,lwd=2,col=\u0026quot;gray\u0026quot;) abline(h=0,lwd=2,col=\u0026quot;gray\u0026quot;) The command abline can be used to add a line to the graph. The input v=0 indicates a vertical line at 0, while the input h=0 specifies a horizontal line at 0. The extra parameters lwd and col specify the width of the line and the color used, respectively (you can use help(plot) and help(colours) in your R terminal to get more information).\nWe can use this graph to assess the uncertainty around the overall cost-effectiveness analysis; for example, we can visually assess the proportion of points lying in each of the four quadrants — for instance, the vast majority seems to be in the NW quadrant, where the new intervention is more expensive and less effective.\nNext we turn to the analysis of the Incremental Net Benefit (INB). We are asked to assess its value for a willingness to pay of \\(k=\\)£500. So, using the code provided in the model file, we can use the following R commands to identify the corresponding index.\nK=numeric() K.space=0.1 for (j in 1:11) { K[j]=(j-1)*K.space } idx=which(K==0.5) Firstly, we recreate in the R workspace the vector of possible willingness to pay thresholds, K. Unlike BUGS, R requires us to define any non-scalar quantity before we can use it, e.g. inside a loop. We can do this using the command K=numeric(), which effectively creates a new variable K and tells R to expect a numeric vector (which can also be length 1, i.e. be a scalar). Next, we set the step of 0.1 (=£100) in the variable K.space and use it to fill in the vector K. Notice that in R we create loops using the for function, which takes as arguments\n the index (in this case j);\n the starting point (in this case 1);\n the ending point (in this case 11).\n  The R notation is fairly straightforward as basically instructs the computer to move j “in 1 to 11” — this means that R will repeat the commands specified inside the loop (delimited by the two curly brackets “{” and “}”) upon varying the index j from 1 to 2, 3, …, 11.\nThe resulting vector is\nK  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 and the variable idx is computed as the element of K that is exactly equal to 0.5, as defined in the function which above. Notice that in logical functions (e.g. if, while, which), R requires that the equality condition is represented by “==” (notice the double sign of equality).\nAt this point, we can use the R built-in functions mean and quantile to estimate the average and 95% interval for the INB at \\(k=\\mbox{\\pounds}500\\). We can do this using the following commands.\nmean(m2$sims.list$INB[,idx]) [1] -5.691401 quantile(m2$sims.list$INB[,idx],.025)  2.5% -20.19075  quantile(m2$sims.list$INB[,idx],.975)  97.5% 8.70215  Notice again that INB is defined as a vector in the BUGSmodel (with one value for each value of K). This means that resulting object from the BUGSsimulations is turned into a matrix (increased by one dimension) and what we need is to access all the rows of the idx-th column (which is associated with \\(k=\\mbox{\\pounds}500\\)).\nWe can manipulate the simulation further to obtain an estimate of the probability that the INB is positive at this threshold. This can be easily obtained by computing the proportion of simulations for which this is true, which we do using the following command.\nsum(m2$sims.list$INB[,idx]\u0026gt;0)/m2$n.sims [1] 0.227 The logical expression m2$sims.list$INB[,idx]\u0026gt;0 checks whether each of the m2$n.sims simulations in the idx-th column of the matrix m2$sims.list$INB is positive. If so, it returns a 1, while if not, it will return a 0. Thus to sum over all these 1s and 0s and then divide by the total number of simulations, will provide an estimate of the probability that INB is positive.\nFinally, we can compute and plot the Cost-Effectiveness Acceptability Curve (CEAC), using the following code.\nCEAC=numeric() for (i in 1:length(K)) { CEAC[i]=mean(m2$sims.list$CEAC[,i]) } plot(K,CEAC,xlab=\u0026quot;Willingness to pay (x 1000)\u0026quot;,ylab=\u0026quot;Cost-effectiveness acceptability curve\u0026quot;, main=\u0026quot;\u0026quot;,t=\u0026quot;l\u0026quot;) Firstly, recall that the CEAC is in fact the probability that INB is positive, for each selected value of the willingness to pay. So we define a vector CEAC, in which we want to store the best estimate (i.e. the mean of the posterior distribution) from our model. To do this, we create a for loop. Notice that this time, we are being a bit clever and instead of specifying the ending point of the loop, we let R compute it itself by defining it equal to length(K) — that is, obviously, the length of the vector K.\nOnce the vector CEAC has been filled in, we can simply plot it agains the values of the willingness to pay using the plot function. Notice that, by default, plot uses open dots to display the selected values. To overwrite this behaviour, we need to specify the option t=l, which instructs R to use a line (curve) instead.\nWe can link the CEAC with the Cost-Effectiveness plane by noticing that the former is essentially the proportion of “futures” (i.e. simulated points in the latter) that lie below the line \\(\\Delta_e=k\\Delta_c\\), for a given willingness to pay threshold, \\(k\\).\nFor example, the graph above plots the line \\(\\Delta_e=0.4\\Delta_c\\), which implies we’re considering \\(k=0.4\\) (recall that the costs are scaled by £1000, so in fact, we mean \\(k=\\)£400!). As is possible to see, most of the points lie above the willingness to pay. In other words, it is much more likely that the intervention \\(t=1\\) is not cost-effective. The proportion of points below the line (BMHE refers to this as the “sustainability area”) is below 0.5 and it indicates, for that given willingness to pay, the CEAC.\nIn reality, we do not really need to perform the economic evaluation “by hand”, ie programming the code above to compute the various health economic summaries and graphical representations. This is the point of BCEA!… In this case, the output of the BUGS model does contain the population average measures of costs and effectiveness — these are the parameters mu.c and mu.e. We can simply pass these as “input” to bcea and obtain all the necessary and relevant economic summaries and plots. For instance, we can use the following code to create two matrices e and c, which we can then feed to BCEA as input.\n# Defines the population average \u0026quot;effectiveness measures\u0026quot; (NB: days in hospital are bad so take -e!) e = -m2$sims.list$mu.e # Defines the population average \u0026quot;cost measures\u0026quot; c = m2$sims.list$mu.c # Calls BCEA library(BCEA) # Runs the function `bcea` to obtain the health economics summary he=bcea( # Defines the inputs e,c, # Labels for the two intervention groups interventions=c(\u0026quot;Standard case management\u0026quot;,\u0026quot;Intensive case management\u0026quot;), # Defines the \u0026quot;reference\u0026quot; interventions (the one that is evaluated) ref=2, # Selects the maximum value for the willingness to pay threshold Kmax=1000 ) # Now can summarise the decision problem summary(he) NB: k (wtp) is defined in the interval [0 - 1000] Cost-effectiveness analysis summary Reference intervention: Intensive case management Comparator intervention: Standard case management Standard case management dominates for all k in [0 - 1000] Analysis for willingness to pay parameter k = 1000 Expected utility Standard case management -74728 Intensive case management -81073 EIB CEAC ICER Intensive case management vs Standard case management -6345.2 0.28222 -0.3973 Optimal intervention (max expected utility) for k = 1000: Standard case management EVPI 1834.8 # Or plot the results ceplane.plot(he)  \n Running the model using R2jags In general, there aren’t many differences in running a model using JAGS or BUGS. In this particular case, however, a “vanilla” run of the code in the file IPD_analysis.R may give some interesting inconsistency.\n You do NOT need to run the model using R2jags — this is just so you are aware of the potential issues. Or, of course, in case you are running JAGS and have encountered this first hand!   The script basically is identical — the only differences are that\n You load the package R2jags instead of R2OpenBUGS; You call the function jags instead of the function bugs. Specifically, the call to jags should not include the option debug, which is specific to R2OpenBUGS.  However, if you run the script and call\nmodel.file=\u0026quot;normal-mod.txt\u0026quot; params \u0026lt;- c(\u0026quot;mu\u0026quot;,\u0026quot;ss\u0026quot;,\u0026quot;ls\u0026quot;,\u0026quot;delta.c\u0026quot;,\u0026quot;dev\u0026quot;,\u0026quot;total.dev\u0026quot;) n.chains=2 n.burnin=1000 n.iter=2000 n.thin=1 m=jags(data=cost.data,inits=NULL,model.file=model.file,parameters.to.save=params, n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T) interestingly the summary table looks like this.\nprint(m,digits=2) Inference for Bugs model at \u0026quot;normal-mod.txt\u0026quot;, fit using jags, 2 chains, each with 2000 iterations (first 1000 discarded) n.sims = 2000 iterations saved mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff delta.c 1.63 9.35 -16.96 -4.54 1.86 7.93 19.73 1.01 2000 dev[1] 3583.82 378.94 2994.04 3200.21 3815.87 3925.86 3934.52 4.76 2 dev[2] 3891.97 67.47 3749.52 3836.38 3928.69 3942.13 3972.89 3.48 3 ls[1] 4.36 0.71 3.07 3.77 4.81 4.98 4.99 3.77 2 ls[2] 4.87 0.10 4.65 4.79 4.93 4.95 5.00 3.46 3 mu[1] 22.70 5.93 10.01 19.78 22.66 25.39 36.14 1.22 110 mu[2] 24.33 7.23 10.15 19.65 24.39 29.46 38.22 1.09 390 ss[1] 12042.81 9274.28 463.50 1888.71 15634.44 21181.73 21746.46 4.19 2 ss[2] 17410.39 3360.54 10968.97 14388.13 19129.83 19906.17 21902.75 3.50 3 total.dev 7475.79 339.34 6924.63 7142.32 7658.96 7760.59 7862.98 3.87 2 deviance 7475.79 339.34 6924.63 7142.32 7658.96 7760.59 7862.98 3.87 2 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = var(deviance)/2) pD = 16814.4 and DIC = 24290.2 DIC is an estimate of expected predictive error (lower deviance is better). The exact same model, with the exact same data, shows different results with evidence of very poor mixing in the chains (high values of Rhat and very low values for n.eff)!\nThe reason for this is in the different way in which OpenBUGS and JAGS handle the generation of initial values: the former uses a random draw from the prior distribution, while the latter uses values that are restricted to be in the proximity of the mean of the prior distribution1.\nIn this case, R2OpenBUGS does a better job at selecting initial values that are closer to the main mass in the posterior distribution, which means that the process converges much more easily and quickly. The process can be “saved” either by running the chains for longer, or by selecting “better” initial values. For instance, running the code\nm=jags(data=cost.data,inits=NULL,model.file=model.file,parameters.to.save=params, n.chains=n.chains,n.iter=10000,n.burnin=9000,n.thin=n.thin,DIC=T) (which creates 10000 simulations, discards the first 9000 and thus saves 1000 per chain — or 2000 in total). The summary statistics look much better now:\nprint(m,digits=2) Inference for Bugs model at \u0026quot;normal-mod.txt\u0026quot;, fit using jags, 2 chains, each with 10000 iterations (first 9000 discarded) n.sims = 2000 iterations saved mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff delta.c 1.90 1.74 -1.53 0.71 1.91 3.03 5.30 1.00 570 dev[1] 2995.58 2.02 2993.68 2994.20 2994.94 2996.28 3000.71 1.01 2000 dev[2] 3064.08 1.86 3062.30 3062.79 3063.55 3064.69 3068.71 1.01 1100 ls[1] 3.09 0.04 3.02 3.07 3.09 3.12 3.17 1.00 2000 ls[2] 3.15 0.04 3.08 3.13 3.15 3.18 3.22 1.01 1500 mu[1] 22.68 1.19 20.34 21.92 22.68 23.48 24.99 1.00 870 mu[2] 24.58 1.25 22.15 23.75 24.57 25.42 27.02 1.00 1200 ss[1] 487.75 37.82 418.52 461.10 485.74 511.92 569.74 1.00 2000 ss[2] 549.30 40.44 476.95 520.62 547.32 577.18 632.35 1.01 1600 total.dev 6059.66 2.75 6056.41 6057.67 6058.93 6060.96 6066.85 1.00 2000 deviance 6059.66 2.75 6056.41 6057.67 6058.93 6060.96 6066.85 1.00 2000 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = var(deviance)/2) pD = 3.8 and DIC = 6063.5 DIC is an estimate of expected predictive error (lower deviance is better). All the Rhat statistics are below 1.1 and the n.eff values are much closer, in general, to the “nominal” sample size n.sims=2000. Even better, we can pass “reasonable” initial values and ensure even better and quicker convergence. For instance, we could use the files normal-initis1.txt and normal-inits2.txt, which can be loaded into the workspace and stored in a list using the following code.\ninits=list(source(\u0026quot;normal-inits1.txt\u0026quot;)$value,source(\u0026quot;normal-inits2.txt\u0026quot;)$value) inits ## [[1]] ## [[1]]$mu ## [1] 0.316036 1.282370 ## ## [[1]]$ls ## [1] 0.437099 0.493518 ## ## ## [[2]] ## [[2]]$mu ## [1] 1.73574 1.31659 ## ## [[2]]$ls ## [1] -1.96202 -1.06980 We can run the original model by specifying these\nm=jags(data=cost.data,inits=inits,model.file=model.file,parameters.to.save=params, n.chains=n.chains,n.iter=n.iter,n.burnin=n.burnin,n.thin=n.thin,DIC=T) and see that convergence is not an issue, even with simply n.iter= 2000 total iterations and n.burnin= 1000 discarded as burn-in.\nprint(m,digits=2) Inference for Bugs model at \u0026quot;normal-mod.txt\u0026quot;, fit using jags, 2 chains, each with 2000 iterations (first 1000 discarded) n.sims = 2000 iterations saved mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff delta.c 1.92 1.73 -1.39 0.75 1.94 3.07 5.33 1.00 2000 dev[1] 2995.61 2.03 2993.67 2994.16 2994.98 2996.31 3001.05 1.00 2000 dev[2] 3064.19 1.84 3062.30 3062.85 3063.58 3065.00 3068.91 1.00 1400 ls[1] 3.09 0.04 3.02 3.07 3.09 3.12 3.17 1.01 210 ls[2] 3.15 0.04 3.08 3.13 3.15 3.18 3.23 1.00 810 mu[1] 22.65 1.20 20.22 21.85 22.63 23.43 25.01 1.00 1100 mu[2] 24.57 1.26 22.12 23.73 24.60 25.43 27.06 1.00 2000 ss[1] 486.70 37.71 418.45 461.68 484.59 510.23 568.02 1.01 210 ss[2] 551.06 42.28 473.12 520.73 549.09 578.76 641.82 1.00 820 total.dev 6059.79 2.67 6056.44 6057.71 6059.21 6061.27 6066.09 1.00 1400 deviance 6059.79 2.67 6056.44 6057.71 6059.21 6061.27 6066.09 1.00 1400 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = var(deviance)/2) pD = 3.6 and DIC = 6063.3 DIC is an estimate of expected predictive error (lower deviance is better).   The JAGS manual states on page 16 that “initial values are not supplied by the user, then each parameter chooses its own initial value based on the values of its parents. The initial value is chosen to be a ‘typical value’ from the prior distribution. The exact meaning of ‘typical value’ depends on the distribution of the stochastic node, but is usually the mean, median, or mode”↩︎\n   ","date":1655805600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655805600,"objectID":"29de051086a250951138b573887259ab","permalink":"/practical/04_ild/solutions/","publishdate":"2022-06-21T10:00:00Z","relpermalink":"/practical/04_ild/solutions/","section":"practical","summary":"This document comments more in details the R script used to perform the whole analysis. Similar results can be obtained using the BUGS interface directly — but as we mentioned in the class, this is usually less efficient and so we focus here on the R script.","tags":[],"title":"Practical 4. Cost-effectiveness analysis with individual level data — SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" Decision modelling Once you have checked the model code in the file EvSynth.txt, you can concentrate on the R script. The first part is fairly simple.\n# Influenza example --- source: Cooper et al (2004); Baio (2012) # Sets up the working directory to the current one working.dir \u0026lt;- getwd() setwd(working.dir) # Defines the data # Number of interventions (t=0: control; t=1: prophylactic use of Neuramidase Inhibitors (NI) T \u0026lt;- 2 # Evidence synthesis on effectiveness of NIs prophylaxis vs placebo r0 \u0026lt;- r1 \u0026lt;- n0 \u0026lt;- n1 \u0026lt;- numeric() # defines observed cases \u0026amp; sample sizes r0 \u0026lt;- c(34,40,9,19,6,34) r1 \u0026lt;- c(11,7,3,3,3,4) n0 \u0026lt;- c(554,423,144,268,251,462) n1 \u0026lt;- c(553,414,144,268,252,493) S \u0026lt;- length(r0) # number of relevant studies # Evidence synthesis on incidence of influenza in healthy adults (under t=0) x \u0026lt;- m \u0026lt;- numeric() # defines observed values for baseline risk x \u0026lt;- c(0,6,5,6,25,18,14,3,27) m \u0026lt;- c(23,241,159,137,519,298,137,24,132) H \u0026lt;- length(x) # Data on costs unit.cost.drug \u0026lt;- 2.4 # unit (daily) cost of NI length.treat \u0026lt;- 6*7 # 6 weeks course of treatment c.gp \u0026lt;- 19 # cost of GP visit to administer prophylactic NI vat \u0026lt;- 1.175 # VAT c.ni \u0026lt;- unit.cost.drug*length.treat*vat # Informative prior on cost of influenza mu.inf \u0026lt;- 16.78 # mean cost of influenza episode sigma.inf \u0026lt;- 2.34 # sd cost of influenza episode tau.inf \u0026lt;- 1/sigma.inf^2 # precision cost of influenza episode The First couple of lines set up the working directory. In previous practicals, we have seen how you can do this by setting a path to the folder you want to use. In this case, we first define a variable working.dir, which we set equal to the current directory (accessed by the R command getwd()). The second line is actually not necessary, strictly speaking, because we already are in the current directory. But this shows, again, how we can use getwd() and setwd() to move across the folders in our computer. Notice that we are using in the script the assign “-\u0026gt;” sign, while in previous practicals we have used the equal “=” sign. For all intents and purposes, R considers them as meaning the same thing.\nThen we start to define the data. Some of the variables we need to define are simple scalars, e.g. T \u0026lt;- 2, the number of interventions. Others are vectors, in which case we need to first define them as numeric(). Notice that you can cascade the -\u0026gt; operator as in r0 \u0026lt;- r1 \u0026lt;- n0 \u0026lt;- n1 \u0026lt;- numeric(), which defines several objects as equal to each other and to an empty vector.\nThe next bit of code defines a R function that we will use to compute the value of the parameters to associate with a logNormal distribution so that the mean and standard deviation on the natural scale are (approximately) equal to some input values.\n# Informative prior on length of influenza episodes ## Compute the value of parameters (mulog,sigmalog) for a logNormal ## distribution to have mean and sd (m,s) lognPar \u0026lt;- function(m,s) { s2 \u0026lt;- s^2 mulog \u0026lt;- log(m) - 0.5 * log(1+s2/m^2) s2log \u0026lt;- log(1+(s2/m^2)) sigmalog \u0026lt;- sqrt(s2log) list(mulog = mulog, sigmalog = sigmalog) } R functions are defined by the keyword function and may or may not have arguments. If you want to specify a function without argument, you can use the following code: myfn \u0026lt;- function(). The commands included between the two curly brackets { and } are those that the function will execute.\nIn this case, we are specifying that the function called lognPar has two inputs (arguments). m is the mean that you want your logNormal distribution to have on the natural scale, while s indicates its intended standard deviation. The function proceeds to first defining the variance s2 by squaring the standard deviation; then it creates a new variable mulog defined as the mean on the log scale (cfr. Lecture 7) in terms of the mean and standard deviation on the natural scale; then it creates s2log, the variance on the log scale, as well as sigmalog, the standard deviation on the log scale. Finally, the variables that we want the function to output are included in a (named) list. When this code is “sourced” (or executed), lognPar becomes available to your R workspace and for example you can use it using a command like the following.\nlognPar(3,2) $mulog [1] 0.9147499 $sigmalog [1] 0.6064031 x \u0026lt;- rlnorm(10000,lognPar(3,2)$mulog,lognPar(3,2)$sigmalog) c(mean(x),sd(x),quantile(x,0.025),quantile(x,0.975))  2.5% 97.5% 2.9924542 2.0223211 0.7648183 8.1689680  which returns the values you should use to model a logNormal distribution so that its mean and standard deviation match your intended values. You can check that all works the way you want by simulating a variable x using the R built-in function rlnorm — in this case we can do 10000 simulations using lognPar(3,2)$mulog as the mean and lognPar(3,2)$sigmalog as the standard deviation. The summary statistics provided above shows that effectively this works perfectly.\nIn fact, we can use the newly created function to complete the definition of the main data for our model, as shown below.\nm.l \u0026lt;- 8.2 # original value in the paper: 8.2 s.l \u0026lt;- sqrt(2) # original value in the paper: sqrt(2) mu.l \u0026lt;- lognPar(m.l,s.l)$mulog # mean time to recovery (log scale) sigma.l \u0026lt;- lognPar(m.l,s.l)$sigmalog # sd time to recovery (log scale) tau.l \u0026lt;- 1/sigma.l^2 # precision time to recovery (log scale) # Parameters of unstructured effects mean.alpha \u0026lt;- 0 sd.alpha \u0026lt;- sqrt(10) prec.alpha \u0026lt;- 1/sd.alpha^2 mean.mu.delta \u0026lt;- 0 sd.mu.delta \u0026lt;- sqrt(10) prec.mu.delta \u0026lt;- 1/sd.mu.delta^2 mean.mu.gamma \u0026lt;- 0 sd.mu.gamma \u0026lt;- 1000 prec.mu.gamma \u0026lt;- 1/sd.mu.gamma^2 All these are fairly simple. The only thing that is perhaps worth noticing is that the BUGS model will use some Normal and logNormal distributions and so we will need to use precisions. For this reason, we create for example the quantity tau.l, which is 1 divided by a variance (i.e. a precision), which we can directly use.\nWe can now call BUGS and run the model.\n# Prepares to launch OpenBUGS library(R2OpenBUGS) # Creates the data list data \u0026lt;- list(S=S,H=H,r0=r0,r1=r1,n0=n0,n1=n1,x=x,m=m,mu.inf=mu.inf,tau.inf=tau.inf, mu.l=mu.l,tau.l=tau.l,mean.alpha=mean.alpha,prec.alpha=prec.alpha, mean.mu.delta=mean.mu.delta,prec.mu.delta=prec.mu.delta, mean.mu.gamma=mean.mu.gamma,prec.mu.gamma=prec.mu.gamma) # Points to the txt file where the OpenBUGS model is saved filein \u0026lt;- \u0026quot;EvSynth.txt\u0026quot; # Defines the parameters list params \u0026lt;- c(\u0026quot;p1\u0026quot;,\u0026quot;p2\u0026quot;,\u0026quot;rho\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;c.inf\u0026quot;,\u0026quot;alpha\u0026quot;,\u0026quot;delta\u0026quot;,\u0026quot;gamma\u0026quot;) # Creates a function to draw random initial values inits \u0026lt;- function(){ list(alpha=rnorm(S,0,1),delta=rnorm(S,0,1),mu.delta=rnorm(1), sigma.delta=runif(1),gamma=rnorm(H,0,1),mu.gamma=rnorm(1), sigma.gamma=runif(1),c.inf=rnorm(1)) } # Sets the number of iterations, burnin and thinning n.iter \u0026lt;- 10000 n.burnin \u0026lt;- 9500 n.thin \u0026lt;- 20 # Finally calls OpenBUGS to do the MCMC run and saves results to the object \u0026quot;es\u0026quot; es \u0026lt;- bugs(data=data,inits=inits,parameters.to.save=params,model.file=filein, n.chains=2, n.iter, n.burnin, n.thin, DIC=TRUE, working.directory=working.dir) Most of these commands should be fairly familiar by now. We first load R2OpenBUGS, then include all the relevant data in a list and define the path to the file where the model is saved (here we assume that the file is in the working directory) and then define the parameters to monitor.\nWhen it comes to defining the inital values, we use this time a bespoke function that we create to simulate suitable values for the variables we want to initialise. For example, the BUGS model defines\n... # Evidence synthesis for efffectiveness of NIs for (s in 1:S) { r0[s] ~ dbin(pi0[s],n0[s]) ... delta[s] ~ dnorm(mu.delta,tau.delta) alpha[s] ~ dnorm(mean.alpha,prec.alpha) } ... which implies that the node alpha is a vector with length S. Thus, when we initialise it, we need to provide R and BUGS with S values. We do this in our inits function by defining alpha=rnorm(S,0,1) — this creates S random draws from a Uniform(0,1) distribution.\nIn fact, we are not initialising all the unobserved nodes associated with a probability distribution: if you look at the BUGS model code, you will notice that the nodes phi and l are also of this kind and so, technically, they need initialisation. If we do not do anything, BUGS will take care of it itself. But we can simply add them by simply modifying the inits function as following.\n# Creates a function to draw random initial values inits \u0026lt;- function(){ list(alpha=rnorm(S,0,1),delta=rnorm(S,0,1),mu.delta=rnorm(1), sigma.delta=runif(1),gamma=rnorm(H,0,1),mu.gamma=rnorm(1), sigma.gamma=runif(1),c.inf=rnorm(1) # Now add the new variables , # Make sure you include a \u0026#39;comma\u0026#39; between variables! l=runif(1),phi=rnorm(1) ) } Notice that we need to separate the variables included in the list using commas. You can see what this function does by simply calling it, e.g. inits().\nWe instruct R and BUGS to run this model for 2 chains — notice we hard-code this in the call to the function bugs. You can always do this, although it is not best practice (and you are probably better off by creating suitable variables and then referring to them as inputs to functions). We select a burn-in of 9500 iterations and then do a further 10000 iterations, which we thin by 20. This means we run the model for a total of \\(2\\times (9500+10000)=39000\\) iterations and then because we throw away the first \\(2\\times 9500\\) and we only store 1 in 20 of the remaining, the final analysis is based on \\(1000\\) iterations.\nOnce BUGS has finished, we regain control of the R session and we can print the output.\n# Displays the summary statistics print(es,digits=2) Inference for Bugs model at \u0026quot;EvSynth.txt\u0026quot;, Current: 2 chains, each with 10000 iterations (first 9500 discarded), n.thin = 20 Cumulative: n.sims = 1000 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff p1 0.02 0.01 0.00 0.01 0.02 0.03 0.06 1.01 290 p2 0.00 0.01 0.00 0.00 0.00 0.01 0.01 1.01 240 rho 0.22 0.18 0.11 0.17 0.21 0.24 0.39 1.00 1000 l 8.15 1.41 5.74 7.22 8.01 8.97 11.18 1.00 550 c.inf 16.90 2.33 12.63 15.18 16.90 18.45 21.66 1.00 520 alpha[1] -2.68 0.17 -3.06 -2.79 -2.67 -2.57 -2.39 1.00 1000 alpha[2] -2.29 0.16 -2.63 -2.40 -2.29 -2.19 -1.98 1.00 590 alpha[3] -2.65 0.32 -3.29 -2.86 -2.65 -2.44 -2.07 1.00 560 alpha[4] -2.63 0.23 -3.09 -2.79 -2.63 -2.47 -2.21 1.00 1000 alpha[5] -3.57 0.38 -4.34 -3.82 -3.55 -3.29 -2.91 1.01 290 alpha[6] -2.62 0.18 -2.96 -2.73 -2.62 -2.50 -2.28 1.00 430 delta[1] -1.41 0.30 -1.95 -1.63 -1.43 -1.22 -0.78 1.00 1000 delta[2] -1.69 0.33 -2.40 -1.89 -1.66 -1.47 -1.13 1.00 710 delta[3] -1.50 0.43 -2.36 -1.75 -1.52 -1.25 -0.54 1.01 540 delta[4] -1.71 0.41 -2.68 -1.93 -1.66 -1.44 -1.03 1.02 140 delta[5] -1.40 0.48 -2.22 -1.70 -1.45 -1.15 -0.37 1.00 1000 delta[6] -1.89 0.43 -2.94 -2.13 -1.81 -1.58 -1.27 1.01 260 gamma[1] -3.12 0.87 -4.89 -3.70 -3.05 -2.52 -1.63 1.01 290 gamma[2] -4.59 0.62 -5.95 -4.94 -4.53 -4.17 -3.53 1.00 610 gamma[3] -4.98 0.87 -7.11 -5.46 -4.87 -4.37 -3.63 1.01 260 gamma[4] -3.23 0.43 -4.14 -3.51 -3.20 -2.95 -2.44 1.00 1000 gamma[5] -5.24 0.57 -6.60 -5.56 -5.19 -4.85 -4.28 1.00 1000 gamma[6] -4.00 0.43 -4.93 -4.26 -3.97 -3.71 -3.19 1.01 240 gamma[7] -4.77 0.82 -6.55 -5.25 -4.70 -4.19 -3.42 1.00 1000 gamma[8] -1.50 0.55 -2.66 -1.84 -1.46 -1.14 -0.51 1.01 330 gamma[9] -4.68 0.81 -6.43 -5.15 -4.58 -4.13 -3.40 1.00 330 deviance 93.98 5.87 84.56 89.65 93.39 97.90 107.00 1.00 1000 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = Dbar-Dhat) pD = 16.84 and DIC = 110.80 DIC is an estimate of expected predictive error (lower deviance is better). All seems reasonable — the value for \\(\\hat{R}\\) is below the 1.1 threshold for all the monitored nodes and the effective sample size is reasonably large (and close to the nominal value n.sims=1000). You can play around with reducing the level of thinning (to increase the sample size) or increasing the total number of iterations to see how the results are affected, but in general terms, the model seems to have reached convergence.\nOne graphical way of confirming this is to create “traceplots” of the simulations, which you can do using the following code.\n# Convergence check through traceplots (example for node p1) plot(es$sims.list$p1[1:500],t=\u0026quot;l\u0026quot;,col=\u0026quot;blue\u0026quot;,ylab=\u0026quot;p1\u0026quot;) points(es$sims.list$p1[501:1000],t=\u0026quot;l\u0026quot;,col=\u0026quot;red\u0026quot;) Here we first plot the first half of the simulations (in this case for the node p1). BUGS stores the simulations for all the different chains by stacking one chain after the other and thus we do this by accessing the first 500 values of the simulations for p1, which are stored inside the BUGS object as es$sims.list$p1. The R notation [1:500] instructs R to access the positions 1 to 500 of a vector. The options used in the plot function specify that we want to plot lines (t=l) in blue (col=blue) and use a \\(y-\\)axis label “p1”. Then we add to the existing plot using the R built-in function points. This has a syntax very similar to plot but does not overwrite an existing graph. In this case, we superimpose the elements from position 501 to position 1000 of the vector es$sims.list$p1 (the simulations for the second chain). We use the options t=l and col=red to instruct R to plot red lines.\nAs is possible to see, the traceplot “looks good” — the two chains are well mixed and on top of each other, confirming convergence (which ties up with the analysis of Rhat and n.eff for this particular node). You can try and replicate this analysis for other nodes.\nFinally, we need to perform the full economic analysis. We could programme all the commands we need ourselves, but we can use BCEA to do most of the work for us.\n# Attaches the es object to the R workspace (to use the posteriors for the economic analysis) attach.bugs(es) # Runs economic analysis # cost of treatment c \u0026lt;- e \u0026lt;- matrix(NA,n.sims,T) c[,1] \u0026lt;- (1-p1)*(c.gp) + p1*(c.gp+c.inf) c[,2] \u0026lt;- (1-p2)*(c.gp+c.ni) + p2*(c.gp+c.ni+c.inf) e[,1] \u0026lt;- -l*p1 e[,2] \u0026lt;- -l*p2 library(BCEA)  Attaching package: \u0026#39;BCEA\u0026#39; The following object is masked from \u0026#39;package:graphics\u0026#39;: contour treats \u0026lt;- c(\u0026quot;status quo\u0026quot;,\u0026quot;prophylaxis with NIs\u0026quot;) m \u0026lt;- bcea(e,c,ref=2,treats,Kmax=10000) Notice that in this case we “attach” the BUGS object es to the R workspace. This will allow us to access all the relevant quantites stored inside of it directly by calling their name, i.e. we will not need to use the clunky notation es$sims.list$p1 to access the simulated values for the parameter \\(p_1\\), but we will just need to call p1. This is handy, but, again, we need to be careful because attaching an object will overwrite any other object with the same name that already exists in the R workspace.\nOnce we have done this, we define the suitable economic summaries. We can think of this step as moving from the “Statistical model” to the “Economic model” box (as in here) — cfr. also the slides for Lecture 7. This step is fairly simple – we first create two matrices e and c, which we will then fill with the simulations for the effectiveness and cost variables for the \\(T=2\\) treatments considered. Notice that we define e and c as matrices filled with NA (R notation for “null” or missing values), with n.sims=1000 rows and T=2 columns. The notation c[,1] should be clear by now. With this, we mean to talk all the rows and only the first column of the matrix c. We fill this with suitable values obtained by combining the probabilities of infection and the relevant costs.\nAt this point, we are ready to load BCEA and then call the function bcea, which performs the basic economic analysis for us. Before we do so, we define for convenience a vector of names, to associate with the interventions we are considering. In this case, the index 1 is associated with the status quo (so the first column of c and e contains the simulations for this treatment), while the second is associated with the active intervention.\nFinally, we create an object m in which we store the output of the call to bcea. There are some mandatory and some optional inputs to this function — check help(bcea) as well as BMHE and Bayesian Cost-Effectiveness Analysis with the R package BCEA to see more details. At the very minimum, bcea expects that you pass as arguments the matrices including the simulations for effects and costs, in this order. Thus, unless you specify a name for the arguments, R will assume you are following the default. For example, the function bcea uses the following arguments in exactly this order.\n e = a numeric matrix with simulations for the effectiveness variable, for all the treatments considered (must have more than 1 column);\n c = a numeric matrix with simulations for the cost variable, for all the treatments considered (must have more than 1 column);\n ref = a number to identify the treatment to be considered as the “reference”, i.e. the one we are comparing against the other(s). This is an optional argument and unless specified differently, the default is 1, in which case BCEA will assume that the intervention of interest is in the first column of e and c. In this case, however, we specify ref=2;\n interventions = a vector of strings of length equal to the number of columns in e, giving names to the interventions. This is also an optional argument and unless otherwise specified, BCEA will create labels in the form Intervention1, Intervention2, …;\n Kmax = a number specifying the maximum value for the willingness to pay to be considered. The default value is k=50000. The willingness to pay is then approximated on a discrete grid in the interval [0,Kmax]. The grid is equal to the argument wtp — see below — if that parameter is provided, or simply composed of 501 elements if wtp=NULL (the default);\n wtp = an optional vector containing specific values for the willingness to pay grid. If not specified then BCEA will construct a grid of 501 values from 0 to Kmax (see point above). This option is useful when performing intensive computations (e.g. for the EVPPI);\n plot = a logical value (i.e. TRUE or FALSE), indicating whether the function should produce the summary plot or not. The default is set to FALSE.\n   ","date":1655813700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655813700,"objectID":"1655111bd5075f3d1bf66c5351768101","permalink":"/practical/05_ald/solutions/","publishdate":"2022-06-21T12:15:00Z","relpermalink":"/practical/05_ald/solutions/","section":"practical","summary":"Decision modelling Once you have checked the model code in the file EvSynth.txt, you can concentrate on the R script. The first part is fairly simple.\n# Influenza example --- source: Cooper et al (2004); Baio (2012) # Sets up the working directory to the current one working.","tags":[],"title":"Practical 5. Evidence synthesis and decision models — SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":"  2. Individual level data on event history and Markov models In this case, we consider individual level data, e.g. derived from a randomised study, in which patients have been:\nRandomised to either standard of care (\\(t=0\\)) or an innovative immuno-oncologic drug;\n Followed up for a period of time during which their “event history” has been recorded. In particular, we know whether or not the individuals have “progressed” to a more serious stage of the cancer (and in that case, the time since enrollement at which this has been confirmed), as well as whether or not the individual has “died” (again with the exact time of death being recorded).  As shown in class, the data look like this.   Patient  Treatment  Progression?  Death?  Progression time  Death time      1  1  1  0  31.99  32.00    2  1  1  0  30.55  30.60    \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)    10  1  1  1  0.17  0.46    11  1  1  1  1.27  1.57    \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)  \\(\\ldots\\)     The time is recorded in months; from the modelling point of view, this is not a problem; however, for the sake of running the Markov model for a “lifetime horizon”, it may be more efficient to convert the times in, say, years — this means that the values are rescaled (by 12, in this case) and so the overall number of cycles becomes smaller (so, instead of running the Markov model for 120 months, which implies a relatively large computational burden), we can do it for 10 years.\nAs shown in class, this dataset has the advantage of using the nature of the data — for each individual we know whether and when they experience the two events of interest (progression and death). This is not possible when we use digitised data on PFS and OS separately. However to run the analysis, we need to convert the data to a suitable format (often referred to as “multistate”). For example, we can use the tidyverse and re-arrange the data in a very efficient way.\n# \u0026#39;tidyverse\u0026#39; code to re-arrange the data in \u0026quot;multistate\u0026quot; format msmdata= # Transition Pre to Post data %\u0026gt;% mutate( # use the original data and start making changes to them id=patid, # patient ID from=1, # starting state (1=\u0026quot;Pre-progression\u0026quot;) to=2, # arriving state (2=\u0026quot;Progressed\u0026quot;) trans=1, # transition ID (1=\u0026quot;Pre-progression -\u0026gt; Progressed\u0026quot;) Tstart=0, # entry time Tstop=prog_t, # exit time (time at which the event happens) time=Tstop-Tstart, # observed time status=case_when( # event indicator prog==1~1, # if progressed then 1 TRUE~0 # otherwise 0 (censored for progression) ), treat=treat # treatment arm ) %\u0026gt;% select(id,from,to,trans,Tstart,Tstop,time,status,treat) %\u0026gt;% # selects only the relevant columns (for simplicity) bind_rows( # stack these new rows below those selected above # Transition Pre to Death data %\u0026gt;% mutate( # use the original data and start making changes to them id=patid, # patient ID from=1, # starting state (1=\u0026quot;Pre-progression\u0026quot;) to=3, # arriving state (3=\u0026quot;Death\u0026quot;) trans=2, # transition ID (2=\u0026quot;Pre-progression -\u0026gt; Death\u0026quot;) Tstart=0, # entry time Tstop=death_t, # exit time (time at which the event happens) time=Tstop-Tstart, # observed time status=case_when( # event indicator (death==1 \u0026amp; prog_t==death_t)~1, # if death then 1 TRUE~0 # otherwise 0 (censored for death) ), treat=treat # treatment arm ) %\u0026gt;% select(id,from,to,trans,Tstart,Tstop,time,status,treat) # selects only the relevant columns (for simplicity) ) %\u0026gt;% bind_rows( # stack these new rows below those selected above # Transition Post to Death data %\u0026gt;% filter(prog==1) %\u0026gt;% mutate( # use the original data, but **filter only those who have progressed** id=patid, # patient ID from=2, # starting state (2=\u0026quot;Progressed\u0026quot;) to=3, # arriving state (3=\u0026quot;Death\u0026quot;) trans=3, # transition ID (3=\u0026quot;Progressed -\u0026gt; Death\u0026quot;) Tstart=prog_t, # entry time. NB: this time is the time of progression! Tstop=death_t, # exit time (time at which the event happens). NB: this time it\u0026#39;s death! time=Tstop-Tstart, # observed time status=case_when( # event indicator death==1~1, # if death then 1 TRUE~0 # otherwise 0 (censored for death **after progression**) ), treat=treat # treatment arm ) %\u0026gt;% select(id,from,to,trans,Tstart,Tstop,time,status,treat) # selects only the relevant columns (for simplicity) ) %\u0026gt;% arrange(id,trans) # Visualise the data msmdata ## # A tibble: 1,868 × 9 ## id from to trans Tstart Tstop time status treat ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 1 2 1 0 32.0 32.0 1 1 ## 2 1 1 3 2 0 32 32 0 1 ## 3 1 2 3 3 32.0 32 0.00920 0 1 ## 4 2 1 2 1 0 30.6 30.6 1 1 ## 5 2 1 3 2 0 30.6 30.6 0 1 ## 6 2 2 3 3 30.6 30.6 0.0476 0 1 ## 7 3 1 2 1 0 27.9 27.9 1 1 ## 8 3 1 3 2 0 28 28 0 1 ## 9 3 2 3 3 27.9 28 0.0944 0 1 ## 10 4 1 2 1 0 2.88 2.88 1 0 ## # … with 1,858 more rows We can now run three separate survival analyses for each of the three transitions, for instance using survHE (but you don’t have to do this — in case you need it, notice that survHE is installed in the Binder remote server). This step is much more complicated than shown here. For example, we would need to test several parametric models (as discussed in Lecture 6). Then we would need to validate the different models — on the basis of their fit to the observed data, but more importantly in relation to the extrapolation over times that have not been observed.\nIn this case, we can simplify things and assume that, for all three models (PFS, OS and OS after progression, which we indicate respectively as m_12, m_13 and m_23 — the reason for this terminology will be obvious later), we select a Gompertz distribution as the best one. In survHE, the models would be fitted using the following commands.\n# Loads survHE library(survHE) ## Loading required package: flexsurv ## Loading required package: survival ## ## Attaching package: \u0026#39;survHE\u0026#39; ## The following objects are masked _by_ \u0026#39;.GlobalEnv\u0026#39;: ## ## data, msmdata # Runs survival models on the specific subsets to obtain estimate of the various transition probabilities m_12=fit.models(Surv(time,status)~as.factor(treat), # model \u0026#39;formula\u0026#39;: defines the time and censoring indicator and the covariates data=msmdata %\u0026gt;% filter(trans==1), # subsets the msmdata by filtering transition number 1 (pre-progression-\u0026gt;progressed) distr=\u0026quot;gom\u0026quot;, # selects the Gompertz model method=\u0026quot;hmc\u0026quot;, # instructs R to use HMC/Bayesian modelling priors=list(gom=list(a_alpha=1.5,b_alpha=1.5))) # specifies the informative prior m_13=fit.models(Surv(time,status)~as.factor(treat), # model \u0026#39;formula\u0026#39;: defines the time and censoring indicator and the covariates data=msmdata %\u0026gt;% filter(trans==2), # subsets the msmdata by filtering transition number 2 (pre-progression-\u0026gt;death) distr=\u0026quot;gom\u0026quot;, # selects the Gompertz model method=\u0026quot;hmc\u0026quot;, # instructs R to use HMC/Bayesian modelling priors=list(gom=list(a_alpha=1.5,b_alpha=1.5))) # specifies the informative prior m_23=fit.models(Surv(time,status)~as.factor(treat), # model \u0026#39;formula\u0026#39;: defines the time and censoring indicator and the covariates data=msmdata %\u0026gt;% filter(trans==3), # subsets the msmdata by filtering transition number 3 (progressed-\u0026gt;death) distr=\u0026quot;gom\u0026quot;, # selects the Gompertz model method=\u0026quot;hmc\u0026quot;, # instructs R to use HMC/Bayesian modelling priors=list(gom=list(a_alpha=1.5,b_alpha=1.5))) # specifies the informative prior``` When we fit these models, we obtain the estimates for the model parameters (which in the case of the Gompertz are the shape \\(\\alpha\\) and the rate \\(\\mu\\)). These can be used to reconstruct the full survival curves for an arbitrary time interval — for example in the case of the Gompertz model, for any given time \\(t\\), the survival curve is \\[S(t) = 1-\\exp\\left(-\\frac{\\mu}{\\alpha} \\exp(\\alpha t)-1\\right).\\]\nIn survHE the function make.surv can be used to construct nsim simulations of the survival curves for any specific interval of time t=.... Once these are obtained, we can use them to compute the approximated transition probabilities using the formula \\[\\lambda_{s\u0026#39;sj}\\approx 1-\\frac{S_{t+k}}{S_t}\\] for a given pair of times \\(t\\) and \\(t+k\\).\nThe script provided contains a few functions that code up this process. We can load up the functions contained in the script survHE_utils.R; these essentially use dplyr and ggplot2 to manipulate the output of the models using the following steps.\nCompute the survival curves using the make.surv function in survHE\n Use the approximation formula to translate these into the probabilities \\(\\lambda_{12}\\) (transition from Pre-progression to Progressed), \\(\\lambda_{13}\\) (transition from Pre-progressed to Death) and \\(\\lambda_{23}\\) (transition from Progressed to Death).\n Run the specialised function three_state_mm that:\n   first completes the transition matrix by retrieving the remaining transition probabilities (\\(\\lambda_{11}=1-\\lambda_{12}-\\lambda_{13}\\) and \\(\\lambda_{22}=1-\\lambda_{23}\\);\n then move people around according to the Markov matrix algebra.\n  Run the specialised function markov_trace to manipulate the resulting state occupancy tibble and then use ggplot2 to plot the Markov trace.  # Sources the specialised functions source(\u0026quot;survHE_utils.R\u0026quot;) # Then run the Markov model using the specialised function \u0026#39;three_state_mm\u0026#39; mm=three_state_mm( m_12,m_13,m_23, # these are the three objects containing the parameters estimates t=seq(0,130), # specifies that the Markov model needs to be run for discrete times from 0 to 130 (months) nsim=1, # only uses 1 simulation from the distribution of the model parameters (the mean) start=c(1000,0,0) # initial population: 1000 in pre-progression, 0 in progressed and 0 in death ) We can visualise the resulting object\nmm ## $m ## # A tibble: 260 × 11 ## treat t `Pre-progressed` Progressed Death sim_idx lambda_11 lambda_12 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1000 0 0 1 0.993 0.00506 ## 2 1 2 993. 5.06 1.67 1 0.993 0.00527 ## 3 1 3 986. 10.2 3.45 1 0.993 0.00550 ## 4 1 4 979. 15.4 5.33 1 0.993 0.00574 ## 5 1 5 972. 20.7 7.32 1 0.992 0.00599 ## 6 1 6 964. 26.1 9.43 1 0.992 0.00625 ## 7 1 7 957. 31.6 11.7 1 0.992 0.00652 ## 8 1 8 949. 37.2 14.0 1 0.991 0.00680 ## 9 1 9 941. 42.8 16.5 1 0.991 0.00710 ## 10 1 10 932. 48.6 19.1 1 0.991 0.00740 ## # … with 250 more rows, and 3 more variables: lambda_13 \u0026lt;dbl\u0026gt;, lambda_22 \u0026lt;dbl\u0026gt;, ## # lambda_23 \u0026lt;dbl\u0026gt; ## ## $running_time ## Time difference of 0.121031 secs ## ## $base_case ## NULL This includes three elements:\nThe tibble m; this includes the state occupancy as well as the value of the relevant transition probabilities for each time point in the “virtual follow up”.\n The running time; this is a function of the number of simulations used — in this case, we’re only using nsim=1 and so the computation is very fast. Note that in general, running this R is more efficient than any implementation in spreadsheets.\n The user can instruct R to also compute the Markov model for the “base-case” scenario, which is essentially the same as the case with nsim=1. So in this case, the element base_case is not computed and it is set to NULL.  Finally, we can visualise the results using the following code.\nmarkov_trace(mm)  Note on using survHE The code above uses the current CRAN version of survHE, which you can install by using the R command install.packages('survHE') or from the GitHub repository remotes::install_github('giabaio/survHE') – assuming you have installed remotes (check here for more details on remotes).\nsurvHE is a bit of a complicated package – it’s not like most of its functions are difficult, it’s just that it is designed to “wrap up” complex packages doing the survival modelling. In particular, the two Bayesian versions are performed using very structured and heavy R packages (rstan and INLA), which means that its installation can be long. If you’re on the Binder VM, the installation of the whole thing may break it. In that case, you can resort to doing a “cheat” and installing the frequentist module only. You can do this by using the R command remotes::install_github(\"giabaio/survHE\",ref=\"devel\"). Note that in this case we use the extra option ref=\"devel\", which instructs R to install the version contained in the GitHub branch named devel, which contains the code to run the frequentist version of the models, only.\nIn this case, you will need to slightly modify the code above, to remove the reference to the \"hmc\" method. For instance, you need to modify the call to fit.models to the following\nm_12=fit.models(Surv(time,status)~as.factor(treat), # model \u0026#39;formula\u0026#39;: defines the time and censoring indicator and the covariates data=msmdata %\u0026gt;% filter(trans==1), # subsets the msmdata by filtering transition number 1 (pre-progression-\u0026gt;progressed) distr=\u0026quot;gom\u0026quot; # selects the Gompertz model ) (notice the removal to the prior argument too: this is a frequentist model, so there’s no space for priors…).\n ","date":1655910900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655910900,"objectID":"a0d52c4f27efcd354cf3f4d1ccc3fff8","permalink":"/practical/09_mm/solutions/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/09_mm/solutions/","section":"practical","summary":"2. Individual level data on event history and Markov models In this case, we consider individual level data, e.g. derived from a randomised study, in which patients have been:","tags":[],"title":"Practical 9. Markov models — SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" Fixed effects NMA The data from the smoking cessation studies discussed in the lecture are included in the file smoke.Rdata and can be loaded into the R workspace using the load command.\n# Loads the data (assuming they are in the current folder) load(\u0026quot;smoke.Rdata\u0026quot;) We can also use other built-in R commands to inspect the object we have just loaded into our workspace to figure out what is stored in it, for example as in the following.\n# List the objects present in the workspace ls() ## [1] \u0026quot;current_repo\u0026quot; \u0026quot;smoke.list\u0026quot; # What type of object is \u0026#39;smoke.list\u0026#39;? class(smoke.list) ## [1] \u0026quot;list\u0026quot; # What\u0026#39;s in the data list? names(smoke.list) ## [1] \u0026quot;r\u0026quot; \u0026quot;n\u0026quot; \u0026quot;t\u0026quot; \u0026quot;na\u0026quot; \u0026quot;NS\u0026quot; \u0026quot;NT\u0026quot; The command ls() simply lists all the objects currently present in the workspace. In this case, we only have an object smoke.list, which has been created when using the load command above. We can check its “class” (in this case, unsurprisingly, a list) and show the names of the elements contained in it, using the command names.\nWe can ask R to tell us more about these variables; for instance, we can inspect each variable’s “class” (e.g.its nature) using the following helpful R command.\nlapply(smoke.list,class) ## $r ## [1] \u0026quot;matrix\u0026quot; \u0026quot;array\u0026quot; ## ## $n ## [1] \u0026quot;matrix\u0026quot; \u0026quot;array\u0026quot; ## ## $t ## [1] \u0026quot;matrix\u0026quot; \u0026quot;array\u0026quot; ## ## $na ## [1] \u0026quot;integer\u0026quot; ## ## $NS ## [1] \u0026quot;integer\u0026quot; ## ## $NT ## [1] \u0026quot;numeric\u0026quot; The R function lapply can be used to apply a function to the elements of a list (such as smoke.list). In this case, we want R to tell us what class each of the elements of smoke.list belongs to, which is what the command returns — for instance, the object r inside the object smoke.list is a matrix, while the object NS is an integer. We can also visualise each, e.g. by using the following commands\n# Shows the first few elements of the object r included inside the object smoke.list head(smoke.list$r) ## [,1] [,2] [,3] [,4] ## [1,] 79 77 NA NA ## [2,] 18 21 NA NA ## [3,] 8 19 NA NA ## [4,] 75 NA 363 NA ## [5,] 2 NA 9 NA ## [6,] 58 NA 237 NA # Shows the first few elements of the object n included inside the object smoke.list head(smoke.list$n) ## [,1] [,2] [,3] [,4] ## [1,] 702 694 NA NA ## [2,] 671 535 NA NA ## [3,] 116 149 NA NA ## [4,] 731 NA 714 NA ## [5,] 106 NA 205 NA ## [6,] 549 NA 1561 NA More in details, the elements of smoke.list are:\n NS: the total number of studies included in our analysis (24);\n NT: the total number of interventions considered (4);\n na: a vector containing the number of arms included in each of the studies;\n r: a matrix with NS rows and NT columns, containing the number of subjects that in each study and under each treatment arms have been observed to quit smoking;\n n: a matrix with NS rows and NT columns, containing the total number of subjects observed in each study and under each treatment arms;\n t: a matrix with NS rows and 3 columns, identifying the label associated with the treatments included in each of the studies. Notice that there are 3 columns because all studies have at most 3 treatment involved (i.e. all studies compare either 2 or 3 treatments — cfr. the lecture slides). The treatments are labelled as 1 = No intervention; 2 = Self-help; 3 = Individual counselling; 4 = Group counselling.\n  Notice that some of the data will be associated with NA (i.e. “Not Available” or “missing”). In this case, these are not really “missing data”, but rather indicate that that particular column is not relevant. For example, in study 1 there are only two arms (you can confirm this by asking R to tell what na[1] is). The matrix r has values 79 and 77 in the first two columns and then NA in the third and fourth column — this is because study 1 only had data on arm 1 and arm 2. Incidentally, you can check what these arms where by looking at the value of the first row in the matrix t\nsmoke.list$t[1,] ## t1 t2 t3 ## 1 2 NA which tells you that the first arm was treatment 1, the second arm was treatment 2 and the third arm was nothing.\nYou can do a similar check on row (i.e. study) 4, using the following commands.\n# How many arms were used in study 4? smoke.list$na[4] ## [1] 2 # What treatment arms were they? smoke.list$t[4,] ## t1 t2 t3 ## 1 3 NA # Data on the number of subjects quitting smoke for study 4 smoke.list$r[4,] ## [1] 75 NA 363 NA # Data on the total sample size in study 4 smoke.list$n[4,] ## [1] 731 NA 714 NA As we can see, study 4 considered 2 arms (comparing interventions 1 and 3, i.e. No intervention vs Individual counselling) and the number of quitters out of the total number of subjects studied were 75/731 and 363/714, respectively\nNow we are ready to prepare to run the model. First, we consider the “fixed-effect” specification, whose model code is included in the file smokefix_model.txt. One of the complications of this model code is in the use of the “nested index” notation. For example, the code\nfor(s in 1:NS) { # loop over studies for (a in 1:na[s]) { # loop over arms r[s,t[s,a]] ~ dbin(p[s,t[s,a]], n[s,t[s,a]]) ... can be interpreted in this way. Let us consider s = 1, i.e. the first study in our dataset. This consists of na[1] = 2 arms, which means we will have two observed data points in terms of number of subjects who quit smoking. This also means that the index a will run from 1 to na[s] = na[1] = 2. Moreover, t[s,a] is in this case t[1,1] = 1 and t[s,a] = t[1,2] = 2, which in turns means that the above code effectively means that we are using the following modelling assumptions.\nr[1,1] ~ dbin(p[1,1], n[1,1]) r[1,2] ~ dbin(p[1,2], n[1,2]) The use of the nested index notation is a clever shortcut for the full specification of all the cases (for all the studies and for all arms specified in each study) and it is particularly helpful for “non-rectangular” data (i.e. when not all the rows have data on the same number of columns).\nFrom a more substantial point of view, we are modelling the logit of the study- and arm-specific probability of quitting smoking using a linear term made by an overall study-specific mean (mu[s]) and an incremental term (delta[s,t[s,a]]), which accounts for the “treatment effect”.\nlogit(p[s,t[s,a]]) \u0026lt;- mu[s] + delta[s,t[s,a]] Again, we are using the nested index notation in exactly the same way as above. In addition, what characterises this model as a “fixed-effect” specification is the distributional assumption on the incremental effects delta. These are modelled as follows.\n## log odds ratio compared to treatment in arm 1 of study s delta[s,t[s,1]] \u0026lt;- 0 for (a in 2:na[s]) { delta[s,t[s,a]] \u0026lt;- d[t[s,a]] - d[t[s,1]] } Firstly, we set the “baseline” intervention for each study s. In particular, we arbitrarily assume that the first intervention (associated with the index identified by t[s,1]) is the reference one for study s and as such, we (again, arbitrarily) assign it an “extra” effect of 0. Obviously, this means that for the reference treatment, the study- and arm-specific probability of quitting smoking is simply mu[s], because in that case we would be adding to the linear predictor a value for the corresponding delta equal to 0. Any other intervention (from 2 to na[s]) is modelled through the difference between its specific value d[t[s,a]] and the value associated with the study-specific reference intervention, d[t[s,1]].\nLet us make a specific example: consider for example study s = 21. The details are as below.\n# How many arms? smoke.list$na[21] ## [1] 3 # Which arms? smoke.list$t[21,] ## t1 t2 t3 ## 2 3 4 This means that this study compares interventions 2, 3 and 4 (Self-help, Individual and Group counselling) and that, arbitrarily, we assume that the one in the first column of t[21,] is the reference — in this case, that is Self-help. In line with the code above, we then set\ndelta[21,2] \u0026lt;- 0 delta[21,3] \u0026lt;- d[21,3] - d[21,2] delta[21,4] \u0026lt;- d[21,4] - d[21,2] because t[21,1] = 2, t[21,2] = 3 and t[21,3] = 4 (cfr. with the code above describing the general definition of the variables delta[s,t[s,a]]). Again, the nested index notation allows us to use a single double loop to model all the possible cases.\nThe log ORs d are then defined as follows.\nd[1] \u0026lt;- 0 # log odds ratio compared to treatment 1 (e.g. placebo) for (i in 2:NT) { d[i] ~ dnorm(0, 0.0001) } This again sets one reference category, which in this case is associated with intervention 1 (No intervention), by setting the corresponding log OR to 0. Then we assign a vague prior to all the other log ORs (the interventions labelled from 2 to NT = 4), using a Normal with mean equal to 0 and a very small precision.\nThe next bit of the model code constructs the ORs for all potential treatment comparisons.\n## odds ratios for all treatment comparisons for (c in 1:(NT-1)) { for (k in (c+1):NT) { or[c,k] \u0026lt;- exp(d[c] - d[k]) or[k,c] \u0026lt;- 1/or[c,k] } } Again, there is some clever coding to use loops and compactly write down all the possible pairwise comparisons. Notice that the variables d define the log ORs and thus in order to obtain the OR on the natural scale, we need to exponentiate the difference between any two values. The line or[k,c] \u0026lt;- 1/or[c,k] uses the fact that ORs for the comparison between two generic interventions \\(k\\) and \\(c\\) are simply the reciprocal of the ORs for the comparison between \\(c\\) and \\(k\\).\nFinally, the model code has some additional definitions for other useful variables.\n## Log odds of quitting successfully under no intervention (from published data) alpha ~ dnorm(-2.6, 6.925208) # = SD 0.38 ## Absolute probability of quitting successfully under each intervention for (i in 1:NT) { logit(pq[i]) \u0026lt;- alpha + d[i] } ## Life years gained by quitting L ~ dnorm(15, 0.0625) # SD=4 Firstly, we define a model for the absolute probability of quitting smoking under each intervention. We do on the logit scale, by defining the baseline value \\(\\alpha \\sim \\mbox{Normal}(\\mu_\\alpha,\\sigma_\\alpha)\\), to which we add the incremental effect of each treatment. Notice that because we set d[1] = 0, then alpha is equal to logit(pq[1]), which is the absolute “success rate” for No intervention. This is incremented by the value of the log OR for each active treatment (against the reference, i.e. No intervention). Notice also that we use an informative prior distribution to model the parameter \\(\\alpha\\). We have information suggesting that a random smoker who is not undergoing any active treatment has an average chance of quitting smoking of about 7%, ranging between around 2% to 13.8%. We can map this information into a suitable prior on the logit scale by setting \\[\\mu_\\alpha = \\mbox{logit}(0.07) = \\log\\left(\\frac{0.07}{1-0.07}\\right) = \\log\\left(\\frac{0.07}{0.93}\\right) \\approx -2.6.\\] We can also use the fact that \\[\\sigma_\\alpha \\approx \\frac{\\mbox{logit}(0.138)-\\mbox{logit}(0.07)}{1.96} \\approx 0.38\\] — this is reasonable if we assume some sort of symmetry in the interval estimate, whereby the upper extreme is 1.96 standard deviations away from the central point, which implies that \\[\\mbox{logit}(0.07)+ 1.96\\sigma_\\alpha \\approx \\mbox{logit}(0.138).\\] Of course, we need to include in the OpenBUGS model the precision, i.e. \\(1/(0.38)^2=6.925208\\).\nThe model for the life years gained by quitting smoking is constructed in a similar way: our best estimate is a gain of between 7 and 22 extra years, with an average of 15, which we turn into a Normal distribution with mean 15 and standard deviation of 4 (i.e. precision of \\(1/16=0.0625\\)).\nThe model is then run from R.\nlibrary(R2OpenBUGS) ### Initial values inits \u0026lt;- list(list(mu=rep(0,24), d=c(NA,0,0,0)), list(mu=rep(-1,24), d=c(NA,1,1,1))) ### Pilot run with no burn-in, to illustrate convergence using traceplots res0 \u0026lt;- bugs(model=\u0026quot;smokefix_model.txt\u0026quot;, data=smoke.list, inits=inits, parameters=c(\u0026quot;d\u0026quot;), n.chains=2, n.burnin=0, n.iter=10000 ) Here, we first define the initial values creating a suitable list made by two “sub-lists” — this implies we are prepared to run the model using two parallel chains. We initialise the variables mu and d — notice that because the first element of the vector d is in fact fixed at 0, we cannot initialise it. We overcome this issue by creating a vector of initial values, the first of which is set to NA.\nIn addition, this time we run the model with no burn-in, to explore convergence in more details than we’ve done so far. The code below uses the results of the OpenBUGS model as stored in the R object res0 to produce traceplots for the only variable monitored (the vector d). Notice in particular that we can use the object res$sims.array, which (as the name suggests) is an array of dimension Number of iterations stored \\(\\times\\) Number of chains run \\(\\times\\) Number of parameters monitored. In this case, the number of parameters is equal to 4, because there are 3 “active” elements in d (since d[1] is set to 0 and thus is not technically a random variable, in OpenBUGS), plus the model deviance, which OpenBUGS computes by default. You can check this by simply printing the summary statistics for your model\nlibrary(R2OpenBUGS) print(res0,digits=3) Inference for Bugs model at \u0026quot;smokefix_model.txt\u0026quot;, Current: 2 chains, each with 10000 iterations (first 0 discarded) Cumulative: n.sims = 20000 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff d[2] 0.225 0.126 -0.020 0.140 0.227 0.309 0.473 1.001 20000 d[3] 0.765 0.063 0.650 0.727 0.766 0.805 0.879 1.001 20000 d[4] 0.839 0.178 0.499 0.719 0.839 0.959 1.183 1.001 20000 deviance 494.927 16.902 482.300 489.500 494.100 499.300 510.700 1.031 20000 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = Dbar-Dhat) pD = 27.220 and DIC = 522.100 DIC is an estimate of expected predictive error (lower deviance is better). The actual traceplot is produce using the built-in functions plot and points as below.\n# Splits the graphical output into a 2-by-2 panel (side-by-side graphs) par(mfrow=c(2,2)) # First graph plot(res0$sims.array[,1,1],t=\u0026quot;l\u0026quot;,xlab=\u0026quot;Iterations\u0026quot;,ylab=\u0026quot;d[2]\u0026quot;,col=\u0026quot;blue\u0026quot;) points(res0$sims.array[,2,1],t=\u0026quot;l\u0026quot;,col=\u0026quot;red\u0026quot;) # Second graph plot(res0$sims.array[,1,2],t=\u0026quot;l\u0026quot;,xlab=\u0026quot;Iterations\u0026quot;,ylab=\u0026quot;d[3]\u0026quot;,col=\u0026quot;blue\u0026quot;) points(res0$sims.array[,2,2],t=\u0026quot;l\u0026quot;,col=\u0026quot;red\u0026quot;) # Third graph plot(res0$sims.array[,1,3],t=\u0026quot;l\u0026quot;,xlab=\u0026quot;Iterations\u0026quot;,ylab=\u0026quot;d[4]\u0026quot;,col=\u0026quot;blue\u0026quot;) points(res0$sims.array[,2,3],t=\u0026quot;l\u0026quot;,col=\u0026quot;red\u0026quot;) As is possible to see, for all the three important parameters, convergence does not seem an issue and in fact the two chains seem to mix up almost immediately, despite being seen to start from rather different points (cfr. the red and blue lines). Notice that this strategy is not an absolute requirement! We can monitor all the relevant parameters and run the model for a large number of iterations in the first place. But, especially when there are many parameters, this course of action may be beneficial, because we are not stuck waiting for OpenBUGS to finish the simulations for a long time, before we can even assess how the model is working in terms of convergence.\nAt this point, we can monitor all the model parameters (including L and pq) and re-run the model to produce the relevant estimates.\nres \u0026lt;- bugs(model=\u0026quot;smokefix_model.txt\u0026quot;, data=smoke.list, inits=inits, parameters=c(\u0026quot;d\u0026quot;,\u0026quot;L\u0026quot;,\u0026quot;pq\u0026quot;), n.chains=2, n.burnin=1000, n.iter=5000) # Show summary statistics print(res,digits=3) Inference for Bugs model at \u0026quot;smokefix_model.txt\u0026quot;, Current: 2 chains, each with 5000 iterations (first 1000 discarded) Cumulative: n.sims = 8000 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff d[2] 0.224 0.126 -0.023 0.139 0.225 0.309 0.470 1.001 8000 d[3] 0.764 0.058 0.649 0.725 0.765 0.804 0.877 1.004 420 d[4] 0.840 0.176 0.505 0.719 0.838 0.958 1.189 1.001 8000 L 14.895 4.009 6.947 12.250 14.940 17.550 22.750 1.001 7300 pq[1] 0.073 0.026 0.034 0.054 0.069 0.087 0.136 1.001 2700 pq[2] 0.090 0.034 0.040 0.065 0.085 0.108 0.171 1.002 2300 pq[3] 0.143 0.048 0.069 0.108 0.137 0.171 0.254 1.001 7500 pq[4] 0.154 0.055 0.070 0.114 0.146 0.186 0.284 1.001 4900 deviance 494.653 7.175 482.400 489.500 494.100 499.200 510.102 1.001 2700 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = Dbar-Dhat) pD = 26.940 and DIC = 521.600 DIC is an estimate of expected predictive error (lower deviance is better). Again, convergence is clearly reached (cfr. Rhat and n.eff). We could proceed with further analyses as well as with building the cost-effectiveness model, but we defer this to after we’ve run the random effects model (cfr. lecture slides for evidence of heterogeneity in individual studies; we can replicate the analysis monitoring the nodes delta, which are the study- and treatment-specific log ORs).\n Random effects NMA The model code is fairly similar to the one discussed above for the fixed effects NMA. The only difference, really, is in how the study- and treatment-specific log ORs delta are modelled. In this case, we consider a simple specification, characterised by the following code lines\ndelta[s,t[s,a]] ~ dnorm(md[s,t[s,a]],taud[s,t[s,a]]) # random effects means md[s,t[s,a]] \u0026lt;- d[t[s,a]] - d[t[s,1]] ## random effects 1/variance constrained to be the same for every comparison taud[s,t[s,a]] \u0026lt;- tau # model for the standard deviation of the random effects sd ~ dunif(0, 10) # rescaling to the precision tau \u0026lt;- 1/pow(sd, 2) — notice that we can include more complexity, for instance by modelling the precision as dependent on the studies or the treatments, as well as by considering a more structured model accounting for correlation within trials with 3 arms or more (but we do not do this here!).\nThe model is run again using the following code.\nres2 \u0026lt;- bugs(model=\u0026quot;smokere_model.txt\u0026quot;, data=smoke.list, inits=inits, parameters=c(\u0026quot;d\u0026quot;, \u0026quot;sd\u0026quot;, \u0026quot;pq\u0026quot;, \u0026quot;L\u0026quot;), n.chains=2, n.burnin=1000, n.iter=20000 ) print(res2,digits=3) Inference for Bugs model at \u0026quot;smokere_model.txt\u0026quot;, Current: 2 chains, each with 20000 iterations (first 1000 discarded) Cumulative: n.sims = 38000 iterations saved mean sd 2.5% 25% 50% 75% 97.5% Rhat n.eff d[2] 0.519 0.386 -0.230 0.267 0.514 0.766 1.305 1.001 6300 d[3] 0.810 0.232 0.374 0.656 0.805 0.956 1.289 1.001 5900 d[4] 1.169 0.455 0.304 0.867 1.156 1.460 2.098 1.001 38000 sd 0.820 0.183 0.533 0.690 0.795 0.923 1.248 1.001 38000 pq[1] 0.073 0.026 0.034 0.054 0.069 0.088 0.136 1.001 38000 pq[2] 0.122 0.059 0.042 0.080 0.111 0.152 0.269 1.001 6500 pq[3] 0.152 0.057 0.065 0.110 0.143 0.184 0.286 1.001 8400 pq[4] 0.208 0.096 0.071 0.138 0.191 0.262 0.437 1.001 38000 L 14.989 3.993 7.109 12.310 15.000 17.670 22.810 1.001 38000 deviance 281.823 10.073 263.900 274.700 281.200 288.200 303.500 1.001 38000 For each parameter, n.eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor (at convergence, Rhat=1). DIC info (using the rule, pD = Dbar-Dhat) pD = 44.870 and DIC = 326.700 DIC is an estimate of expected predictive error (lower deviance is better). Again, the model seems to do well in terms of convergence, although autocorrelation is possibly more of a concern (check the values of n.eff, which are slightly smaller than the nominal sample size of 38000). This is not uncommon with hierarchical/random effect models.\nIn terms of comparison with the results of the fixed effects version, there is generally an increase in the value of the mean for the log ORs d, coupled with larger uncertainty. On the other hand, the absolute probabilities of quitting pq are rather stable. The estimate for L does not change — but this is not surprising, as this node is modelled independently on the other variables in the code and based on an informative prior, which is not updated by any data. So, simply changing parts of the model that are effectively disconnected by the one in which we model L is not changing our estimates for this node.\nWe can also complete the model to include the cost-effectiveness component. We do this by firstly defining a vector of unit costs associated with each intervention. Here we assume that No intervention does not involve any cost for the public payer, while Self-help, Individual and Group counselling do have some costs. We then define the measures of effectiveness as the overall life years gained (L) weighted by the absolute probability of quitting smoking (pq) for each intervention. We build the variables e and c in the loop over the 4 interventions. Notably, in this case, we do not model the costs (although a variant of this model that does account for this is presented in the the BCEA Book).\n### Cost-effectiveness analysis library(BCEA) ## ## Attaching package: \u0026#39;BCEA\u0026#39; ## The following object is masked from \u0026#39;package:graphics\u0026#39;: ## ## contour unit.cost \u0026lt;- c(0,200,6000,600) ints \u0026lt;- c(\u0026quot;No contact\u0026quot;,\u0026quot;Self help\u0026quot;,\u0026quot;Individual counselling\u0026quot;,\u0026quot;Group counselling\u0026quot;) e \u0026lt;- c \u0026lt;- matrix(NA,res2$n.sims,4) # MCMC sample from distribution of life-years gained by quitting L \u0026lt;- res2$sims.list$L # ...and from distributions of probability of quitting for each of 4 interventions pq \u0026lt;- res2$sims.list$pq for (t in 1:4) { e[,t] \u0026lt;- L*pq[,t] c[,t] \u0026lt;- unit.cost[t] } colnames(e) \u0026lt;- colnames(c) \u0026lt;- ints Finally, we can run BCEA to post-process the model output and produce the relevant summaries, e.g. summaries or the cost-effectiveness plane.\nm \u0026lt;- bcea(e,c,interventions=ints,Kmax=1000,ref=4) summary(m) ## NB: k (wtp) is defined in the interval [0 - 1000] ##  ## ## Cost-effectiveness analysis summary ## ## Reference intervention: Group counselling ## Comparator intervention(s): No contact ## : Self help ## : Individual counselling ## ## Optimal decision: choose No contact for k \u0026lt; 274 ## Self help for 274 \u0026lt;= k \u0026lt; 310 ## Group counselling for k \u0026gt;= 310 ## ## ## Analysis for willingness to pay parameter k = 1000 ## ## Expected utility ## No contact 1096.7 ## Self help 1629.9 ## Individual counselling -3727.5 ## Group counselling 2523.5 ## ## EIB CEAC ICER ## Group counselling vs No contact 1426.85 0.89945 296.03 ## Group counselling vs Self help 893.65 0.78947 309.20 ## Group counselling vs Individual counselling 6251.05 1.00000 -6345.08 ## ## Optimal intervention (max expected utility) for k = 1000: Group counselling ## ## EVPI 97.442 Notice that, because this model involves multiple comparisons, the default output for the plot function in BCEA is not entirely satisfactory. There are ways in which we can modify this default behaviour to improve the look of the pictures (see the help for BCEA as well as the BCEA Book).\nplot(m)  ","date":1655825400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655825400,"objectID":"a2bc3b9df525f8e794afe8136cd0a48e","permalink":"/practical/06_nma/solutions/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/06_nma/solutions/","section":"practical","summary":"Fixed effects NMA The data from the smoking cessation studies discussed in the lecture are included in the file smoke.Rdata and can be loaded into the R workspace using the load command.","tags":[],"title":"Practical 6. Network meta-analysis — SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" The first thing we do is loading and analysing the “base-case” model, which is stored in the object statins_base.Rdata and the “robust” version of the model, stored in statins_HC.Rdata.\nlibrary(BCEA) library(R2OpenBUGS) load(\u0026quot;statins_base.Rdata\u0026quot;) load(\u0026quot;statins_HC.Rdata\u0026quot;) We can use the R function print to visualise the output for the two models, for example as in the following.\nprint(statins_base) The output to this call is a long list of summary statistics — it is also possible to visualise an excerpt by using code such as the following\nhead(statins_base$summary[,c(\u0026quot;mean\u0026quot;,\u0026quot;sd\u0026quot;,\u0026quot;2.5%\u0026quot;,\u0026quot;97.5%\u0026quot;,\u0026quot;Rhat\u0026quot;,\u0026quot;n.eff\u0026quot;)],n=15)  mean sd 2.5% 97.5% Rhat n.eff cost.hosp[1] 238.7010 137.0428 91.68029 482.0066 1.001533 980 cost.hosp[2] 315.6340 168.7579 124.20521 668.4667 1.002554 1000 cost.hosp[3] 523.0695 451.0165 144.02705 1357.9918 1.002820 1000 cost.hosp[4] 424.9861 232.1917 170.08732 877.9484 1.001743 980 cost.hosp[5] 305.1978 172.3249 120.79156 656.3935 1.002747 550 cost.hosp[6] 301.1282 163.4569 121.85935 618.0017 1.001342 1000 cost.stat[1] 480.8821 289.1621 137.50194 1232.7586 1.004167 360 cost.stat[2] 350.0194 201.6103 103.42871 871.5393 1.000252 1000 cost.stat[3] 166.6851 125.6502 34.24849 498.3156 1.000219 1000 cost.stat[4] 305.4061 261.7113 47.69648 1008.5598 1.004566 1000 cost.stat[5] 346.9371 209.6277 103.21293 880.0139 1.006177 400 cost.stat[6] 165.0717 129.7352 35.28509 526.9310 1.000074 1000 cost.tot[1] 719.5831 324.8498 309.13357 1526.9058 1.005610 270 cost.tot[2] 665.6534 265.4862 331.33786 1357.5038 1.001748 1000 cost.tot[3] 689.7546 467.3877 242.54283 1629.7467 1.002190 1000 which produces the first n=15 rows (i.e. parameters) for the whole summary table. In particular, we only select the columns headed as \"mean\", \"sd\", etc. (we do so to exclude additional quantiles that are automatically stored in the object statins_base$summary). We should make sure that the models have all converged and that autocorrelation is not an issue (by e.g. analysing the \\(\\hat{R}\\) and \\(n_{eff}\\) statistics).\nWe can already check the DIC associated with each of the two models, to get some ideas of which one will be given most weight in the structural PSA. We can do so by using the following command.\n# Displays the DIC for the two models c(statins_base$DIC,statins_HC$DIC) [1] 2233.875 2225.988 As is easy to see, the “robust” model is associated with a relatively lower DIC (by over 10 points).\nWe can now move on and use the results from the two Bayesian models as inputs to BCEA. The objects statins_base$sims.list and statins_HC$sims.list contain the simulated values for all the model parameters monitored in list format. We can follow the script and use the code\n# Defines the intervention labels interventions \u0026lt;- c(\u0026quot;Atorvastatin\u0026quot;,\u0026quot;Fluvastatin\u0026quot;,\u0026quot;Lovastatin\u0026quot;,\u0026quot;Pravastatin\u0026quot;, \u0026quot;Rosuvastatin\u0026quot;,\u0026quot;Simvastatin\u0026quot;) # BCEA object with the economic analysis of the \u0026quot;base case\u0026quot; model m1 \u0026lt;- bcea(statins_base$sims.list$effect,statins_base$sims.list$cost.tot, ref=1,interventions=interventions) # BCEA object with the economic analysis of the Half-Cauchy model m2 \u0026lt;- bcea(statins_HC$sims.list$effect,statins_HC$sims.list$cost.tot, ref=1,interventions=interventions) to first define a vector of intervention labels and then apply the function bcea to the suitable variables of effects and costs in the two models.\nThe two objects m1 and m2 can be post-processed as any BCEA objects (e.g. using plot or print methods). But in addition to this, we can also combine them to perform the PSA to structural assumptions. To do so, we need to manipulate the original objects in a suitable way. Firstly, we need to create a list of models, which we can simply do using the following command.\n# Combines the BUGS models models \u0026lt;- list(statins_base,statins_HC) the newly created object contains the information from the two BUGS models. We can also create suitable lists in which we store the relevant variables of effectiveness and costs from the two models, for example using code such as the following.\n# Creates the variables of effectiveness and costs effects \u0026lt;- list(statins_base$sims.list$effect, statins_HC$sims.list$effect) costs \u0026lt;- list(statins_base$sims.list$cost.tot, statins_HC$sims.list$cost.tot) Finally, we can feed these inputs to the BCEA function struct.psa as in the following.\n# Finally uses BCEA to perform the structural PSA to consider the base and HC models m3 \u0026lt;- struct.psa(models,effects,costs,ref=1,interventions=interventions) struct.psa takes as basic arguments three lists, containing the BUGS models, the list of effects and the list of costs simulations and combines them to compute the model weights (based on the DICs).\nThe object m3 is a list, which contains 3 elements:\nlapply(m3,function(x) list(class(x),names(x)))  $he $he[[1]] [1] \u0026quot;bcea\u0026quot; \u0026quot;list\u0026quot; $he[[2]] [1] \u0026quot;n_sim\u0026quot; \u0026quot;n_comparators\u0026quot; \u0026quot;n_comparisons\u0026quot; \u0026quot;delta_e\u0026quot; [5] \u0026quot;delta_c\u0026quot; \u0026quot;ICER\u0026quot; \u0026quot;Kmax\u0026quot; \u0026quot;k\u0026quot; [9] \u0026quot;ceac\u0026quot; \u0026quot;ib\u0026quot; \u0026quot;eib\u0026quot; \u0026quot;kstar\u0026quot; [13] \u0026quot;best\u0026quot; \u0026quot;U\u0026quot; \u0026quot;vi\u0026quot; \u0026quot;Ustar\u0026quot; [17] \u0026quot;ol\u0026quot; \u0026quot;evi\u0026quot; \u0026quot;ref\u0026quot; \u0026quot;comp\u0026quot; [21] \u0026quot;step\u0026quot; \u0026quot;interventions\u0026quot; \u0026quot;e\u0026quot; \u0026quot;c\u0026quot; $w $w[[1]] [1] \u0026quot;numeric\u0026quot; $w[[2]] NULL $DIC $DIC[[1]] [1] \u0026quot;numeric\u0026quot; $DIC[[2]] NULL The R function lapply applies iteratively the function class and names to each element of the object m3.\nAs is possible to see, the first one, named he, is an object in the class BCEA and so it contains the usual elements that such objects do (e.g. n.sim, n.comparators, etc). The elements w and DIC are numeric vectors and include the weights and the value of the DIC associated with each individual models. We can visualise for example the weights by using the command,\nprint(m3$w) [1] 0.01901127 0.98098873 which indicates that the second model (the “robust” Half-Cauchy) is given a weight of over 98%. This is consistent with the fact that its DIC is lower than the one for the base case model, which in turn indicates better fit.\nWe can also use all the methods implemented for BCEA objects to analyse and visualise the output of the model averaging. For example, we can summarise the cost-effectiveness analysis by typing\nsummary(m3$he)  Cost-effectiveness analysis summary Reference intervention: Atorvastatin Comparator intervention(s): Fluvastatin : Lovastatin : Pravastatin : Rosuvastatin : Simvastatin Optimal decision: choose Simvastatin for k \u0026lt; and for k \u0026gt;= Analysis for willingness to pay parameter k = 25000 Expected utility Atorvastatin 22823 Fluvastatin 22435 Lovastatin 21381 Pravastatin 21731 Rosuvastatin 22526 Simvastatin 22738 EIB CEAC ICER Atorvastatin vs Fluvastatin 388.695 0.768 3919.60 Atorvastatin vs Lovastatin 1442.154 0.843 1432.44 Atorvastatin vs Pravastatin 1092.152 0.958 336.72 Atorvastatin vs Rosuvastatin 297.304 0.738 5227.07 Atorvastatin vs Simvastatin 85.002 0.594 19129.49 Optimal intervention (max expected utility) for k = 25000: Atorvastatin EVPI 193.22 and we could plot the cost-effectiveness plane with the following command.\nceplane.plot(m3$he) or generate multiple treatments comparison cost-effectiveness acceptability curves with the following commands.\nm3.multi\u0026lt;-multi.ce(m3$he) ceac.plot(m3.multi) Notice that because in this particular case one of the models is effectively given an almost 100% weight, the model average will resemble it almost identically.\n","date":1655892000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655892000,"objectID":"300078a95ed7b094f1be100614c4d13d","permalink":"/practical/07_structural/solutions/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/07_structural/solutions/","section":"practical","summary":"The first thing we do is loading and analysing the “base-case” model, which is stored in the object statins_base.Rdata and the “robust” version of the model, stored in statins_HC.Rdata.","tags":[],"title":"Practical 7. PSA to structural uncertainty — SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" Instructions The following exercise helps you run the Markov model and the underlying Bayesian model to estimate the transition probabilities in Rand BUGS. You should also look specifically at BMHE and actually run through the calculations to make sure you understand how the process works. In particular, look at the part relative to discounting and how you do that, which is very prevelent in real applied work.\nOpen the file MarkovModel1.txt and inspect the BUGS model code. Make sure you understand the assumptions encoded in the model.\n Follow the script MarkovModel1.R, which will guide you through the necessary steps to create the data, run the MCMC model and then perform the relevant economic analysis from R.\n   Solutions The R script guides you through the process of running the Markov model analysis for the asthma problem seen in the lecture.\nThe graphical representation of the Markov model for the ‘asthma’ problem discussed in section 5.4 of BMHE\n The script first sets up the number of states \\(S=5\\) and the number of time points \\(J=12\\) weeks in the virtual follow up. Also, we load the data, given in the form of matrices with the observed transitions across the states, during the actual follow up in the trial. The code is relatively straightforward — we use the R command matrix to define the data. Notice that the numbers included in the matrix are read by row (as the command byrow=TRUE suggests).\nS = 5 r.0 = (matrix(c( 66,32,0,0,2, 42,752,0,5,20, 0,4,0,1,0, 0,0,0,0,0, 0,0,0,0,156),c(S,S),byrow=TRUE)) r.0  [,1] [,2] [,3] [,4] [,5] [1,] 66 32 0 0 2 [2,] 42 752 0 5 20 [3,] 0 4 0 1 0 [4,] 0 0 0 0 0 [5,] 0 0 0 0 156 You can check that these tie up with the matrix presented in the lecture slides — notice that r.0 indicates the data for the control arm.\nPerhaps more interestingly, the script also defines the prior distribution for the parameters \\(\\boldsymbol\\lambda\\) in terms of a Dirichlet distribution.\nClick to view more details on the Dirichlet distribution The Dirichlet distribution is a multivariate generalisation of the Beta. Specifically, where the Beta$(\\alpha,\\beta)$ is a good model for a single parameter ranging between 0 and 1, the Dirichlet can be used to model $S$ parameters $\\lambda_1,\\ldots,\\lambda_S$ that are all constrained in the range $[0;1]$ and such that $\\sum_{s=1}^S \\lambda_s = 1$. This fits nicely the property that transition probabilites off a given state $s$ are **exhaustive and mutually exclusive**, meaning that all are between 0 and 1 and that they must sum to 1.\nLike the Beta distribution is related to the Binomial sampling distribution, so the Dirichlet is to the multivariate generalisation of the Binomial, i.e. the Multinomial distribution. So, if our situation involves $S$ possible outcomes each of which occurs in $y_1,\\ldots,y_S$ counts out of the total sample size $n$, we can model the sampling distribution as $y_1,\\ldots,y_S \\sim {\\sf Multinomial}(\\boldsymbol\\lambda)$, where $\\boldsymbol\\lambda=(\\lambda_1,\\ldots,\\lambda_S)$ is the vector of probabilities associated with each possible outcome.\nModelling $\\boldsymbol\\lambda \\sim {\\sf Dirichlet}(\\boldsymbol{a})$ for a vector of parameters $\\boldsymbol{a}=(a_1,\\ldots,a_S)$ is the equivalent of the conjugated Beta-Binomial model \u0026mdash; with the added benefit that the posterior distribution is also a Dirichlet: $$\\boldsymbol\\lambda\\mid \\boldsymbol{y}\\sim {\\sf Dirichlet}(a_1+y_1, \\ldots, a_S+y_S).$$\nIn a Dirichlet distribution, the parameters $\\boldsymbol{a}$ have a relatively intuitive interpretation, as they are proportional to the expected probability associated with the various outcomes. So if $a_S$ is very large in comparison to the other parameters $a_1,\\ldots,a_{s-1},a_{s+1},\\ldots,a_S$, then we are encoding the fact that outcome $s$ is much more likely to occur than the others. The scale of the parameters indicates the precision. So for example, a Dirichlet$(1,1,1)$ encodes the assumption that the three possible categories are all equally likely (because the underlying parameters $a_1,a_2,a_3$ are all the same). But in the same way, so would a Dirichlet$(100,100,100)$, or for that matter a Dirichlet$(0.01,0.01,0.01)$. In all cases the three parameters have the same value. The third distribution implies the lowest level of precision, while the second one implies large precision: intuitively, the larger the value, the more prior knowledge we consider.\nThe graph above shows four symplexes: each side of the triangles represent one of the Dirichlet parameters (in this case we assume an underlying variable with three possible categories). When $a_1=a_2=a_3=1$, the mass of points (representing simulations from the underlying Dirichlet distribution) is spread all over the area in the triangle. This is the equivalent of a vague prior where the probability mass is spread all over the range of the variable. Conversely, when $a_1=a_2=a_3=50$, the mass is concentrated in a small, central part of the triangle \u0026mdash; intuitively, this means that the three dimensions carry the same weight (because the parameters have the same value); because that value is large, then the variance of the points is relatively low. When one of the dimensions has a much larger value than the others, the points tend to be pulled towards that area, as happens when we consider a Dirichlet$(2,5,10)$.\n In this case, the “scale” parameter of the Dirichlet distribution is assumed to be \\(\\alpha_0=\\alpha_1=10\\). This assumption implies:\n We effectively assume the same prior distribution in both arms of the trial. \\(\\alpha_0\\) governs the behaviour of the control arm, while \\(\\alpha_1\\) is the parameter defining how the probabilities \\(\\lambda_1,\\ldots,\\lambda_S\\) act in the treatment arm.\n We define alpha.0 = alpha.1 = rep(scale, S); this implies that  scale=10 alpha.0 = alpha.1 = rep(scale,S) alpha.1 [1] 10 10 10 10 10 i.e. that in the prior, each of the \\(S=5\\) categories (STW, UTW, Pex, Hex, TF) is assumed to have the same probability scale/ \\(\\sum_{s=1}^S\\) scale. Intuitively, we specify our prior by imagining a “thought experiment” in which the sample size is \\(\\sum_{s=1}^S\\) scale=50, in this case. This sample size is assumed in our prior to be distributed equally across the \\(S=5\\) states, so that we think that there are scale=10 individuals in each of the states. This is a relatively strong prior. For example, a Dirichlet(0.1,0,1,0.1,0.1,0.1) would indicate the same thought experiment with a sample size of just \\(5\\times 0.1=0.5\\) individuals, of which \\(0.1\\) is allocated to each of the states.\nOf course, there’s nothing special about this construction — in fact simply convenience. We may have thought about this problem much more carefully and determined that in fact we would expect, in the prior, more individuals to be in the STW state, say twice as many as in UTW, three times as many as in Pex, four times as many as in Hex and 10 times as many as in TF. This could translate, for example, into a Dirichlet(10,5,3.3,2.5,1) prior. If we felt very confident about this, we may even express our prior into a Dirichlet(1000,500,330,250,100) prior — i.e. with the same proportionality but much larger numbers, to imply bigger precision.\nThe script also proceeds to define the initial values. This is also interesting. We use the mathematical results whereby we can simulate from a Dirichlet distribution by first constructing independent Gamma variables and then rescaling the simulated values by their sum. In R, we create the inits function, in which we first simulate a matrix of Gamma(scale, 1) values using the command rgamma(4*S,scale,1); then we place the resulting vector of \\(4\\times S\\) values into a matrix of dimension \\(4\\times S\\), e.g. the object temp.0. Notice that because TF is assumed to be an absorbing state, by definition \\(\\boldsymbol\\lambda_{S}=(0,0,0,0,1)\\). Thus, we do not need to initialise this row of the matrix of parameters \\(\\boldsymbol\\lambda\\) and therefore, we only need a matrix of size \\(4\\times S\\). Next, we create the variable sum.temp.0 in which we record the row totals of temp.0 and then construct the matrix mat.0 by rescaling temp.0 by these totals. Finally, we use the command list to store the named variables we want to output in the function inits, i.e. lambda.0 and lambda.1.\ninits \u0026lt;- function(){ temp.0 \u0026lt;- matrix(rgamma(4*S,scale,1),4,S) sum.temp.0 \u0026lt;- apply(temp.0,1,sum) mat.0 \u0026lt;- temp.0/sum.temp.0 temp.1 \u0026lt;- matrix(rgamma(4*S,scale,1),4,S) sum.temp.1 \u0026lt;- apply(temp.1,1,sum) mat.1 \u0026lt;- temp.1/sum.temp.1 list(lambda.0=rbind(mat.0,rep(NA,S)),lambda.1=rbind(mat.1,rep(NA,S))) } inits() $lambda.0 [,1] [,2] [,3] [,4] [,5] [1,] 0.2620049 0.1656849 0.1735277 0.1906087 0.2081738 [2,] 0.1721008 0.1694657 0.3168546 0.1709276 0.1706513 [3,] 0.2621914 0.2312971 0.2281732 0.1588414 0.1194969 [4,] 0.1646739 0.1817589 0.2073144 0.1667142 0.2795386 [5,] NA NA NA NA NA $lambda.1 [,1] [,2] [,3] [,4] [,5] [1,] 0.1258738 0.1952657 0.3466712 0.1115298 0.2206596 [2,] 0.1677436 0.1495265 0.2917020 0.2010646 0.1899634 [3,] 0.2207405 0.1171875 0.2113657 0.2074268 0.2432796 [4,] 0.1412204 0.1312841 0.2364687 0.1709408 0.3200860 [5,] NA NA NA NA NA Once the BUGS model has run, we have estimates for the transition probabilities \\(\\boldsymbol\\lambda\\). Given these simulations, we can construct the Markov model “virtual” follow up, using the following code.\n# Now run the Markov model from R start \u0026lt;- c(1,0,0,0,0) # NB analysis for 1 single patient! # (can create a \u0026quot;virtual\u0026quot; population of N individuals # and allocate them across the states at time j=0) # Markov transitions m.0 \u0026lt;- m.1 \u0026lt;- array(NA,c(n.sims,S,(J+1))) for (s in 1:S){ m.0[,s,1] \u0026lt;- start[s] m.1[,s,1] \u0026lt;- start[s] } lam0=lam1=array(NA,c(n.sims,S,S)) for (i in 1:n.sims) { lam0[i,,]=rbind(lambda.0[i,,],c(0,0,0,0,1)) lam1[i,,]=rbind(lambda.1[i,,],c(0,0,0,0,1)) for (j in 2:(J+1)){ for (s in 1:S){ # Use the new matrices lam0 and lam1 to do the matrix multiplication m.0[i,s,j] \u0026lt;- sum(m.0[i,,j-1]*lam0[i,,s]) m.1[i,s,j] \u0026lt;- sum(m.1[i,,j-1]*lam1[i,,s]) } } } For simplicity, we run the MM for a single patient and we initialise the “cohort” so that the patient is in state \\(s=1\\) (STW) at the beginning of the follow up (\\(j=0\\)). There’s no special reason for this; we could have a larger size of the cohort (e.g. the sum of the vector start); or we may have a different initial distribution across the states.\nThen we construct the arrays m.0 and m.1 in which we will store the total transitions in each state and for each time. We use the recursive relationship \\(\\boldsymbol{m}_{j+1}=\\boldsymbol{m}_j \\boldsymbol\\Lambda_j\\), where \\(\\boldsymbol\\Lambda_j\\) is the \\(S\\times S\\) matrix storing the transition probabilities for all the states, at time \\(j\\). Notice that because the objects lambda.0 and lambda.1 are obtained as output from bugs, they are arrays where the first dimension is the number of simulations produced.\nNotice that BUGS stores the simulations for the “random” part of the matrix \\(\\boldsymbol\\Lambda_j\\). In fact, because the last row (containing the transition probabilities off the absorbing state TF) is deterministically defined, BUGS doesn’t consider it a parameter. Thus, before we can apply the matrix multiplication, we need to construct a matrix, which we call lam0 and lam1 respectively for the control and active treatment arm, in the code above, in which we stack up the \\(i-\\) MCMC simulated values for the first \\((S-1)\\) rows of the transition probability matrix (estimating the transitions off the states STW, UTW, Pex and Hex) and a row vector of values \\((0 0 0 0 1)\\), which indicates that for the state TF, the only possible movement is to remain in it.\nThe rest of the script post-process the output of the Bayesian model to create a “Markov trace”, i.e. a barplot displaying the number/proportion of patients transitioning in each state at each time point. This is a useful graph, as it allows us to try and make sense of the underlying population dynamics that is implied by the Markov model we have constructed. In real-life applications, we would want to produce this graph and validate it with the help of the wider research team (including clinicians and experts of the subject matter).\nIn the above barplots, the darker bar indicates the proportion of individuals in STW and increasingly lighter colours indicate, respectively, the proportion of people in UTW, Pex, Hex and TF. As is possible to see, the active treatment (SFC) seems to be associated with a population dynamics in which more people tend to remain in the most favourable outcome (STW).\nFinally, we define and apply discounting to the resulting costs and effects and then use BCEA to produce the economic analysis. The code is fairly straighforward. Firstly, we define the discount rate, set at 3.5% for both benefits and costs. Notice that this is the “standard” annual discount rate suggested by NICE. In this particular example, the virtual follow up is set at \\(J=12\\) weeks, so technically we don’t really need to discount the output (as 12 weeks is barely 4 months, let alone more than one year!). Then we fill in the elements of the vectors disc.b and disc.c with the discounted series of values.\n## General formulation to apply discount delta.b \u0026lt;- 0.035 # discount rate for benefits (3.5%) delta.c \u0026lt;- 0.035 # discount rate for costs (3.5%) # Defines the discount factors disc.b \u0026lt;- numeric(); disc.c \u0026lt;- numeric() disc.b[1] \u0026lt;- 1; disc.c[1] \u0026lt;- 1 for (j in 2:J) { disc.b[j] \u0026lt;- (1+delta.b)^(j-1) disc.c[j] \u0026lt;- (1+delta.c)^(j-1) } disc.b  [1] 1.000000 1.035000 1.071225 1.108718 1.147523 1.187686 1.229255 1.272279 [9] 1.316809 1.362897 1.410599 1.459970 Essentially, the discounted multiplier for time \\(j=0\\) is simply 1 (no discounting); at time \\(j=1\\) it is 1.035; and at the end of follow up is 1.45997.\nThen, we actually apply these discounting multipliers to the estimated effects and costs.\ndisc.cost0 \u0026lt;- disc.eff0 \u0026lt;- disc.cost1 \u0026lt;- disc.eff1 \u0026lt;- matrix(NA,mm1$n.sims,J) for (j in 1:J) { disc.cost0[,j] \u0026lt;- cost0[,j]/disc.c[j] disc.cost1[,j] \u0026lt;- cost1[,j]/disc.c[j] disc.eff0[,j] \u0026lt;- m.0[,1,j]/disc.b[j] disc.eff1[,j] \u0026lt;- m.1[,1,j]/disc.b[j] } # Shows difference between raw and discounted costs (for 1st simulation) cost0[1,]  [1] 100.23659 104.27436 89.77322 76.01900 66.42544 60.41060 56.81619 [8] 54.71874 53.50987 52.81776 52.42299 52.19835 disc.cost0[1,]  [1] 100.23659 100.74817 83.80426 68.56478 57.88594 50.86410 46.22001 [8] 43.00843 40.63601 38.75402 37.16364 35.75304 As is possible to see, the raw costs are actually higher than the discounted counterparts (apart from the first element, for which the discounting multiplier is 1 and so, effectively, no discounting occurs).\nAt this point, we define the economic outcome as the sum of the discounted series of values for benefits and costs, over the \\(J=12\\) time points, using the following code.\n# Sums the values across all time points and creates matrix of costs c \u0026lt;- matrix(NA,n.sims,2) c[,1] \u0026lt;- apply(cost0,1,sum) c[,2] \u0026lt;- apply(cost1,1,sum) # Effectiveness e \u0026lt;- matrix(NA,n.sims,2) e[,1] \u0026lt;- apply(m.0[,1,],1,sum) e[,2] \u0026lt;- apply(m.1[,1,],1,sum) And to complete the analysis, we run BCEA\n# Cost-effectiveness analysis library(BCEA) ints \u0026lt;- c(\u0026quot;FP\u0026quot;,\u0026quot;SFC\u0026quot;) m \u0026lt;- bcea(e,c,ref=2,interventions=ints,Kmax=300) which can be used to summarise the economic model.\nsummary(m) NB: k (wtp) is defined in the interval [0 - 300] Cost-effectiveness analysis summary Reference intervention: SFC Comparator intervention: FP SFC dominates for all k in [0 - 300] Analysis for willingness to pay parameter k = 300 Expected utility FP -250.17 SFC 523.57 EIB CEAC ICER SFC vs FP 773.74 0.9995 -94.31 Optimal intervention (max expected utility) for k = 300: SFC EVPI 0.018456  ","date":1655910900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655910900,"objectID":"d2125aab84ba71e170d46ac82dc6f8b7","permalink":"/practical/09_mm/cohort-model/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/09_mm/cohort-model/","section":"practical","summary":"Instructions The following exercise helps you run the Markov model and the underlying Bayesian model to estimate the transition probabilities in Rand BUGS. You should also look specifically at BMHE and actually run through the calculations to make sure you understand how the process works.","tags":[],"title":"Cohort discrete Markov model","type":"book"},{"authors":null,"categories":[],"content":" Survival analysis using survHE Preliminaries This practical assumes that you have installed survHE, a R package specifically designed to perform survival analysis in health economic evaluation and with advanced facilities for Bayesian modelling.\nYou can install survHE you can either use the “official” CRAN version, or the most-updated, “development”. This can take a little time, as there are several “dependencies” (i.e. packages that are required for survHE to work properly).\n# Install survHE from CRAN install.packages(\u0026quot;survHE\u0026quot;) # Or the development version fro GitHub devtools::install.github(\u0026quot;giabaio/survHE\u0026quot;) If survHE is installed, you simply need to load it into your R session, as usual and then we can also load some data.\n# Loads survHE in the session library(survHE) Loading required package: flexsurv Loading required package: survival load(\u0026quot;survival_data.Rdata\u0026quot;) This loads a dataset called dat, which contains some survival data. In particular, the dataset includes the patients ID; the time of progression to a more severe stage of cancer; an indicator for the event of interest (mortality); an indicator for the treatment arm (coded as 0 = control and 1 = active treatment); an indicator for the patients’ sex (0 = male; 1 = female); the patients’ age (in years); and the Index of Multiple Deprivation (IMD) score (this is a census-based, area-level measure of socio-economic circumstances. It is coded as categorical variable taking values in the interval \\([1;5]\\), where 1 indicates the least deprived and 5 indicates the most deprived areas). We can inspect it as usual, using built-in R functions.\n# Loads survHE in the session head(dat)  ID_patient time event arm sex age imd 1 1 0.03 0 0 1 32 2 2 2 0.03 0 0 1 43 2 3 3 0.92 0 0 1 25 4 4 4 1.48 0 0 0 36 3 5 5 1.64 0 0 1 38 5 6 6 1.64 0 0 0 35 1 table(dat$arm)  0 1 189 178  table(dat$arm,dat$event)  0 1 0 90 99 1 105 73 There are 189 individuals in arm 0 (controls) and 178 in the arm 1 (some active drug). The data include a patient ID, the time at which the event has been observed (e.g. progression to a worse disease state) and an indicator for censoring. Individuals who are not fully observed are associated with censored=1.\n Model fitting in a frequentist setting We are instructed to fit both the Exponential and the Weibull model to the data, assuming a linear predictor of the form \\[g(\\mu_i) = \\log(\\mu_i) = \\beta_0 + \\beta_1\\texttt{arm}_i.\\]\nIn order to analyse the data, we first need to define the model we want to use and the distributions we want to use. We can simply set this out using the following R commands.\n# Defines the model formula and the distributions formula=Surv(time,event)~as.factor(arm) mods=c(\u0026quot;exp\u0026quot;,\u0026quot;weibull\u0026quot;) # Then runs survHE to estimate the two models m1=fit.models(formula=formula,data=dat,distr=mods) The formula specifies the model in terms of regression for the generalised linear predictor, which in this case only depends on the treatment arm (notice that, because arm is a categorical variable, we include it in our analysis as a R “factor”; the first value arm=0 will be used as reference category). Notice also that we need to use the specific notation Surv(time=time, event=event) to tell R and survHE that our data are in survival analysis form. Then we set up a vector mods in which we include some string text identifying the Exponential and Weibull models (more details are available in the survHE documentation). Finally, we are ready to run the function fit.models, which is used by survHE to perform the analysis and estimate the model parameters.\nThe results of the models are stored in an object m1, which contains several elements.\n# Explores the model output names(m1) [1] \u0026quot;models\u0026quot; \u0026quot;model.fitting\u0026quot; \u0026quot;method\u0026quot; \u0026quot;misc\u0026quot;  lapply(m1,names) $models [1] \u0026quot;Exponential\u0026quot; \u0026quot;Weibull (AFT)\u0026quot; $model.fitting [1] \u0026quot;aic\u0026quot; \u0026quot;bic\u0026quot; \u0026quot;dic\u0026quot; $method NULL $misc [1] \u0026quot;time2run\u0026quot; \u0026quot;formula\u0026quot; \u0026quot;data\u0026quot; \u0026quot;model_name\u0026quot; \u0026quot;km\u0026quot;  The R command lapply can be used to “apply” the function names to all the elements of the list m1, to provide details of each of its elements. So, for example, the object m$models contains two objects (Exponential and Weibull (AFT)), in which the estimates are stored.\nThe output for the modelling can be visualised using the print method, as follows.\n# Defines the model formula and the distributions print(m1)  Model fit for the Exponential model, obtained using Flexsurvreg (Maximum Likelihood Estimate). Running time: 0.023 seconds mean se L95% U95% rate 0.0824203 0.00828355 0.0676839 0.100365 as.factor(arm)1 -0.4656075 0.15427131 -0.7679738 -0.163241 Model fitting summaries Akaike Information Criterion (AIC)....: 1274.576 Bayesian Information Criterion (BIC)..: 1282.387 print(m1,2)  Model fit for the Weibull AF model, obtained using Flexsurvreg (Maximum Likelihood Estimate). Running time: 0.011 seconds mean se L95% U95% shape 1.816383 0.1098390 1.613371 2.044941 scale 10.220953 0.5705218 9.161747 11.402616 as.factor(arm)1 0.342019 0.0855445 0.174355 0.509683 Model fitting summaries Akaike Information Criterion (AIC)....: 1203.130 Bayesian Information Criterion (BIC)..: 1214.846 This takes an optional argument, which allows to specify which model should be printed, in case more than one distribution has been selected (e.g. in this case). Notice that, by default, survHE uses maximum likelihood as the “method” to perform the estimation (as reported by the output of the print function).\nThe model output can also be plotted in terms of the resulting survival curves, on top of the Kaplan-Meier estimate. This can be done using the plot command, using the option add.km=TRUE.\n# Defines the model formula and the distributions plot(m1,add.km=TRUE) The resulting graph shows the survival curves within the observed time-frame (0.03—20.92), for all the models fitted in m. As expected from the theory, the Exponential model does not do a good job at following the observed shape of the data, as it is not flexible enough. The Weibull model is much closer to the empirical estimate provided by the Kaplan-Meier curve. This is confirmed by the analysis of the Information Criterion statistics (AIC and BIC): for the Exponential model they are both greater than the equivalent values obtained for the Weibull model, indicating that the latter fit the observed data better.\nIn general terms, the survival curves (which are just 1 — the cumulative probability curves) can be used to read off the relevant probability at a given time. For example, if we consider the following graph, it would be fairly easy to read off that at time \\(t=15\\), the survival probability is roughly about 0.25 (in fact, to be precise, it can be computed with some algebra to be 0.3797412). Similar (approximate) computations can be made on a grid of values (as represented in the graph) for different times and or probability values. It is often useful to compute, at least in an approximate ways, (survival) probabilities using this method.\n Bayesian modelling As mentioned in the lecture, standard MCMC algorithms may struggle with survival data, especially when they are characterised by a large number of censored observations. Thus, survHE implements Bayesian analysis using two alternative Bayesian computation methods. The first one is based on Integrated Nested Laplace Approximation (INLA), while the second uses a variant of MCMC called Hamiltonian Monte Carlo (HMC).\nWithout going too much into the details (some of which are described in the survHE manual, INLA is very fast (almost as fast as the MLE procedure) and produces precise results, but is only available (at present) for a limited set of distributions. On the other hand, HMC is a little slower, but is perhaps a little more flexible and allows for more distributional assumptions.\nIn survHE, it is very simple to specify what “method” of inference should be used, by simply setting the option method to either mle (the default), or inla, or hmc. So, for example, we could replicate the analysis above using INLA by simply using the following command.\n# Runs survHE to estimate the two models using INLA m2=fit.models(formula=formula,data=dat,distr=mods,method=\u0026quot;inla\u0026quot;) # Shows the output for the Exponential model print(m2)  Model fit for the Exponential model, obtained using INLA (Bayesian inference via Integrated Nested Laplace Approximation). Running time: 0.62141 seconds mean se L95% U95% rate 0.0828715 0.00836297 0.0669431 0.100186 as.factor(arm)1 -0.4665097 0.14721656 -0.7474321 -0.177349 Model fitting summaries Akaike Information Criterion (AIC)....: 1276.583 Bayesian Information Criterion (BIC)..: 1288.299 Deviance Information Criterion (DIC)..: 1277.371 # And then for the Weibull model print(m2,2)  Model fit for the Weibull AF model, obtained using INLA (Bayesian inference via Integrated Nested Laplace Approximation). Running time: 1.284 seconds mean se L95% U95% shape 1.764107 0.1114979 1.552269 1.973406 scale 10.281869 0.6121163 9.186980 11.575483 as.factor(arm)1 0.344241 0.0893014 0.182237 0.532154 Model fitting summaries Akaike Information Criterion (AIC)....: 1205.359 Bayesian Information Criterion (BIC)..: 1220.981 Deviance Information Criterion (DIC)..: 1206.669 As is possible to see, many of the results are very similar to the MLE analysis above. This is because, by default, both the INLA and HMC implementation use relatively weak prior distributions for both the location \\(\\mu_i=g^{-1}(\\boldsymbol\\beta)\\) and the ancillary parameters \\(\\boldsymbol\\alpha\\) (see lecture slides). These priors can be modified, but this requires some changes to the call to the fit.models function (see the manual for more details). Because INLA specifies a Bayesian model, there is an additional Information Criterion available, the DIC, which is also printed in the summary tables. Once again, the Weibull model is preferable as it is associated with lower values of the AIC, BIC and DIC.\nIn a very similar way, we can specify the models using HMC as the inferential engine, by using the following command.\n# Runs survHE to estimate the two models using HMC m3=fit.models(formula=formula,data=dat,distr=mods,method=\u0026quot;hmc\u0026quot;) and we can still use the print method to visualise the results.\n# Shows the output for the Exponential model print(m3)  Model fit for the Exponential model, obtained using Stan (Bayesian inference via Hamiltonian Monte Carlo). Running time: 1.1656 seconds mean se L95% U95% rate 0.0821905 0.00792447 0.0676933 0.0988463 as.factor(arm)1 -0.4626672 0.15001521 -0.7682862 -0.1793789 Model fitting summaries Akaike Information Criterion (AIC)....: 1276.579 Bayesian Information Criterion (BIC)..: 1288.295 Deviance Information Criterion (DIC)..: 1274.295 # And then for the Weibull model print(m3,2)  Model fit for the Weibull AF model, obtained using Stan (Bayesian inference via Hamiltonian Monte Carlo). Running time: 2.6113 seconds mean se L95% U95% shape 1.804606 0.1106481 1.594570 2.018583 scale 10.273537 0.5923679 9.243481 11.492342 as.factor(arm)1 0.346925 0.0892387 0.177872 0.518907 Model fitting summaries Akaike Information Criterion (AIC)....: 1205.143 Bayesian Information Criterion (BIC)..: 1220.765 Deviance Information Criterion (DIC)..: 1203.313 Once again, the results are fairly similar, numerically, due to the fact that the priors are relatively weak and there are enough data to consistently inform the posterior distributions for the parameters. Again, the survHE manual explains in more details how the priors can be modified in order to include genuine information. Because HMC is an MCMC algorithm, we can check the convergence diagnostics, much as we had done for the BUGS output in the previous practicals. In particular, we could check the traceplots and histograms for the posterior distributions of the parameters using built-in functions in the rstan package, which survHE uses to perform the HMC analysis, as in the following.\n# Traceplots for the parameters of the Exponential model (the first element of m3$models) rstan::traceplot(m3$models[[1]]) # Histograms for the parameters of the Weibull model (the second element of m3$models) rstan::stan_hist(m3$models[[1]]) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. A more familiar version of the summary statistics table for the HMC output can be obtained by adding another optional argument to the call to print, as follows.\n# Shows the output for the Exponential model print(m3,2,original=TRUE) Inference for Stan model: WeibullAF. 2 chains, each with iter=2000; warmup=1000; thin=1; post-warmup draws per chain=1000, total post-warmup draws=2000. mean se_mean sd 2.5% 25% 50% beta[1] 2.327924 0.001666 0.057311 2.223919 2.286785 2.325146 beta[2] 0.346925 0.002579 0.089239 0.177872 0.286620 0.343483 alpha 1.804606 0.002958 0.110648 1.594570 1.726319 1.803580 scale 10.273537 0.017270 0.592368 9.243481 9.843244 10.228174 lp__ -600.346661 0.039879 1.251980 -603.624141 -600.881002 -600.046654 75% 97.5% n_eff Rhat beta[1] 2.366375 2.441681 1184 0.999171 beta[2] 0.407953 0.518907 1197 0.999553 alpha 1.883740 2.018583 1399 0.999804 scale 10.658682 11.492342 1177 0.999187 lp__ -599.475113 -598.892282 986 1.002001 Samples were drawn using NUTS(diag_e) at Mon Jun 6 14:39:34 2022. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). We can easily see that the \\(\\hat{R}\\) statistic is below the arbitrary threshold of 1.1 for all the nodes and that the effective sample size n_eff is also rather close to the nominal sample size of 2000, indicating that convergence is reached and autocorrelation is not an issue.\nWe can plot the results of all the model, selectively, by specifying a more complex call to the plot function, for example as in the following.\nplot(MLE=m1,INLA=m2,HMC=m3, # Selects which models from the three fitted objects mods=c(1,2,3,4,5,6), # Specifies colours to plot the curves colour=c(\u0026quot;blue\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;yellow\u0026quot;,\u0026quot;magenta\u0026quot;,\u0026quot;orange\u0026quot;), # Defines the time horizon over which to make the plot t=seq(0,50) ) Here, the option MLE=m1,INLA=m2,HMC=m3,mod=c(1,2,3,4,5,6) instructs R to first stack together the three objects m1, m2 and m3 (and give them the names MLE, INLA and HMC) and the to select the models 1 to 6 (in this case, all of them, because in each method we have fitted two distributions). Then we specify colours and labels. As is possible to see, there is virtually no difference in the estimates for the Exponential model, while there are some minor ones for the Weibull. We can also set an option t=seq(0,50), which instructs R to extrapolate the survival curves beyond the observed data and up to time = 50.\n Probabilistic sensitivity analysis survHE is designed to perform automatically PSA on the survival curves, based on the underlying uncertainty in the model parameters. Irrespective of the inferential engine (MLE or Bayesian), the function make.surv uses a simulation approach (based either on boostrap in the case of MLE, or simulations from the posterior distributions in the case of the Bayesian models) to then reconstruct the entire probability distribution of the survival curves, in a specified time range.\nFor example, the following code constructs an object psa1 in which nsim=1000 simulations for the survival curves of mod=2 (the Weibull specification) in m1 (the MLE analysis) are stored.\n# Performs PSA on the survival curves for the Weibull model (under MLE) psa1=make.surv(m1,mod=2,t=seq(0.01,50),nsim=1000) psa.plot(psa1,offset=2.5,col=c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;)) The specialised function psa.plot can be used to visualise the resulting survival curves and the underlying uncertainty. psa.plot can be customised, e.g. by specifying the colour with which the curves need to be plotted, or the distance between the terms of the label, which appears in the top part of the graph. These describe the combination of covariates associated with each curve — in this case, the blue curve is associated with a value of the intercept of 1 and a value of the treatment arm of 0 (i.e. the control arm), while the red curve is associated with a value of 1 for the treatment arm (i.e. the active treatment).\nIn fact, the most recent (and current) version of survHE can use the simpler function plot to perform the extrapolation and PSA (see here).\nWithout getting into the technical details, the process can be replicated for the Bayesian models — the main difference here is in the fact that in this case (and particularly under HMC), the resulting simulations will be a better approximation of the underlying joint probability distribution of all the model parameters. As mentioned in the classes, in cases where there is substantial correlation among the parameters of the survival model (\\(\\boldsymbol\\alpha,\\boldsymbol\\beta\\)), then this is likely to give results that may differ from the rougher approximation based on bootstrap.\n  ","date":1655895600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655895600,"objectID":"b5919da3c38663e10e05b06ddeb81906","permalink":"/practical/08_survival/solutions/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/practical/08_survival/solutions/","section":"practical","summary":"Survival analysis using survHE Preliminaries This practical assumes that you have installed survHE, a R package specifically designed to perform survival analysis in health economic evaluation and with advanced facilities for Bayesian modelling.","tags":[],"title":"Practical 8. Survival analysis — SOLUTIONS","type":"book"},{"authors":null,"categories":[],"content":" Bivariate Normal model The data are stored in a list format because the variables are non-balanced, in terms of sample size. In other words, the data for arm \\(t=1\\) contain fewer points than for \\(t=2\\). It would be possible to format this dataset using a data.frame, but this would mean having a single column for each variable (stacking together the two treatment arms) and adding a treatment indicator.\nIn any case, we can visualise the relevant information using usual R commands, e.g. names to check the naming of the variables and the $ operator to access them.\n# Reads in the data list data=readRDS(\u0026quot;missing_data.rds\u0026quot;) # Checks the name of the variables names(data) [1] \u0026quot;c\u0026quot; \u0026quot;e\u0026quot; \u0026quot;n\u0026quot; \u0026quot;u\u0026quot; # And shows some values data$c [[1]] [1] NA 0.1 NA NA NA 0.1 NA NA NA 516.0 [11] NA NA NA NA 404.0 NA NA 116.0 145.0 436.0 [21] 782.0 NA 145.0 NA NA NA 2.0 NA NA 0.1 [31] NA NA NA NA 193.0 0.1 NA NA 64.0 NA [41] NA NA NA NA NA 1039.0 NA 2.0 NA NA [51] NA NA 0.1 NA 246.0 NA NA NA 141.0 400.0 [61] 116.0 0.1 NA 360.0 281.0 NA 0.1 NA NA NA [71] 107.0 NA NA 123.0 NA [[2]] [1] NA NA NA 183.0 NA NA NA NA 0.1 107.0 NA NA [13] NA NA 516.0 NA NA NA NA NA NA NA NA NA [25] NA 0.1 NA NA NA NA NA NA NA NA NA NA [37] 0.1 NA NA 194.0 60.0 227.0 NA NA NA 266.0 NA NA [49] 0.1 169.0 346.0 NA NA 0.1 NA NA NA 366.0 NA NA [61] NA 380.5 NA NA NA NA NA NA 268.0 123.0 NA NA [73] NA 389.5 NA NA NA NA NA NA NA NA NA NA As is possible to see, the element data$c is itself a list. The data are clearly affected by many missing values (recorded in R as NA). We can also inspect the distribution of the variables using histograms, e.g. using the following commands.\n# Produces a histogram of the cost distribution in arm t=1 hist(data$c[[1]]) In general terms, it is important to make some preliminary analysis to evaluate the impact of missingness. For instance, if you had a large dataset with only a handful of missing values, then perhaps you could simply do a “Complete Case Analysis”, which probably would not have a massive impact on your overall results. But if missingness is very prevalent, then this would imply the need for more sophisticated analyses, as well as the fact that the results would be by necessity to be taken with a rather large pinch of salt… We can summarise the proportion of missing data using the following command.\nlapply(1:2,function(x) table(is.na(data$e[[x]]))/sum(table(is.na(data$e[[x]])))) [[1]] FALSE TRUE 0.36 0.64 [[2]] FALSE TRUE 0.2261905 0.7738095  In this case, we can use the R command table to tabulate the data (on the effectiveness variable, named e) depending on whether they are NA (= missing) or not. We use the lapply function to perform the operation on both the elements of the list data$e. The code\ntable(is.na(data$e[[x]]))/sum(table(is.na(data$e[[x]]))) can be used to compute the proportion (instead of the absolute number) of missingness in each arm. As is possible to see, in this particular case, missingness is a big problem, as in the two arms there are respectively 64.00% and 77.38% of missing values.\nWe can then proceed to the modelling part. Firstly, we fit a bivariate Normal model for costs and effects; while this is not ideal because it clearly fails to accommodate the marked skewness in the data (as evidenced by the histogram above), it is a good starting point and at least allows us to account for potential correlation between costs and effects (and their missing mechanisms).\nThe model code is not too dissimilar to those used in Practical 4 (on individual level data), but it does require some modifications, to account for the missing data. We firstly assume a marginal Normal distribution for the effectiveness \\[ e_i \\sim \\mbox{Normal}\\left( \\phi_{eit},\\psi_{et} \\right), \\] where the individual mean response (QALYs) is modelled as a linear regression as a function of the baseline utility \\[ \\phi_{eit} = \\alpha_0 + \\alpha_1 (u_{it}-\\mu_{ut}). \\]\nIn the BUGS code, we need to be careful in defining the Normal distribution, which requires the precision instead of the variance. For this reason, we code for example\neff1[i] ~ dnorm(phi.e1[i],tau.e[1]) where tau.e[1] is the precision, i.e. \\(\\tau_{e1}=\\psi_{et}^{-1}\\). In addition, the linear predictor simply translates the regression for \\(\\phi_{eit}\\) — we notice here that we center the covariate \\(u_{it}\\) for simplicity in the interpretation and ease of convergence (in this way, \\(\\alpha_0\\) is the overall mean QALY in the population).\nThe next thing to understand is that, because also \\(u_{it}\\) is affected by missing values, we need to model it explicitly, even if it is used as covariate in the model. Thus, we need to include a suitable `BUGS} statement to define a probability distribution to represent it. We choose for simplicity the same form as the QALYs \\(e_{it}\\) and thus specify a Normal distribution depending on a mean \\(\\mu_{ut}\\) and a precision \\(\\tau_{ut}\\).\nFinally, we can model the conditional distribution of the costs, given the effectiveness variable. The assumption is a model \\[ c_{it} \\sim \\mbox{Normal}(\\phi_{cit},\\psi_{ct}), \\] where the mean is specified as a linear predictor \\[ \\phi_{cit} = \\beta_0 + \\beta_1 (e_{it}-\\mu_{et}). \\] Because we are again centering the covariate included in this model, we can simply characterise the population mean costs and benefits as \\(\\mu_{ct} = \\beta_0\\) and \\(\\mu_{et}=\\alpha_0\\), which we can then use in the economic analysis.\nThe `BUGS} code is replicated for the two treatment arms and it then specifies the prior distributions. The model parameters are \\(\\boldsymbol{\\theta}=(\\boldsymbol\\alpha,\\boldsymbol\\beta,\\boldsymbol\\sigma_{e},\\boldsymbol\\sigma_{c},\\boldsymbol\\mu_u,\\boldsymbol\\sigma_u)\\), where:\n \\(\\boldsymbol\\alpha=(\\alpha_0,\\alpha_1)\\stackrel{iid}{\\sim}\\mbox{Normal}(0,v)\\), with \\(v\\) a large variance (e.g. a precision of 0.00001); \\(\\boldsymbol\\beta=(\\beta_0,\\beta_1)\\stackrel{iid}{\\sim}\\mbox{Normal}(0,v)\\), with \\(v\\) a large variance (e.g. a precision of 0.00001); \\(\\boldsymbol\\sigma_e=(\\sigma_{e1},\\sigma_{e2})\\) are the standard deviations for the effectiveness measures in the two treatment arms. We specify vague priors on the log-standard deviation scale, i.e. \\(\\log \\sigma_{et}\\sim \\mbox{Uniform}(-5,10)\\). This of course induces a prior on \\(\\sigma_{et}\\) and then on \\(\\sigma_{et}^2\\) and then on \\(\\tau_{et}\\); \\(\\boldsymbol\\sigma_c=(\\sigma_{c1},\\sigma_{c2})\\) are the standard deviations for the cost measures in the two treatment arms. We specify vague priors on the log-standard deviation scale, i.e. \\(\\log \\sigma_{ct}\\sim \\mbox{Uniform}(-5,10)\\). This of course induces a prior on \\(\\sigma_{ct}\\) and then on \\(\\sigma_{et}^2\\) and then on \\(\\tau_{ct}\\); \\(\\boldsymbol\\mu_u=(\\mu_{u1},\\mu_{u2})\\stackrel{iid}{\\sim}\\mbox{Normal}(0,v)\\), with \\(v\\) a large variance (e.g. a precision of 0.00001); \\(\\boldsymbol\\sigma_u=(\\sigma_{u1},\\sigma_{u2})\\) are the standard deviations for the baseline utility in the two treatment arms. We specify vague priors on the log-standard deviation scale, i.e. \\(\\log \\sigma_{ut}\\sim \\mbox{Uniform}(-5,10)\\). This of course induces a prior on \\(\\sigma_{ut}\\) and then on \\(\\sigma_{ut}^2\\) and then on \\(\\tau_{ut}\\).  The BUGS code maps these assumptions directly and also adds some lines to derive the analytic form of the conditional variance and precision for the costs. These are useful, but are not necessarily fundamental parameters.\nWe can now run the model using OpenBUGS.\n# Loads the package library(R2OpenBUGS) # Defines: # 1. model file filein=\u0026quot;Normal_Normal.txt\u0026quot; # 2. data list datalist=list(N1=data$n[[1]],eff1=data$e[[1]],cost1=data$c[[1]],u1=data$u[[1]], N2=data$n[[2]],eff2=data$e[[2]],cost2=data$c[[2]],u2=data$u[[2]]) # 3. parameters to monitor params\u0026lt;-c(\u0026quot;mu.e\u0026quot;,\u0026quot;sd.e\u0026quot;,\u0026quot;alpha0\u0026quot;,\u0026quot;alpha1\u0026quot;,\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;,\u0026quot;Delta_e\u0026quot;, \u0026quot;Delta_c\u0026quot;,\u0026quot;mu.c\u0026quot;,\u0026quot;sd.c\u0026quot;,\u0026quot;eff1\u0026quot;,\u0026quot;eff2\u0026quot;,\u0026quot;cost1\u0026quot;,\u0026quot;cost2\u0026quot;) # 4. number of iterations n.iter\u0026lt;-10000 # 5. sets up initial values for crucial parameters inits=function(){ list(alpha0=rnorm(2),alpha1=rnorm(2),beta0=rnorm(2),beta1=rnorm(2),mu.u=rnorm(2)) } # 6. runs the model NN=bugs(data=datalist,inits=inits,parameters.to.save=params, model.file=filein,n.chains=2,n.iter=n.iter,n.thin=1,DIC=TRUE) All is fairly straightforward; we first point R to the .txt file including the model code, then we define a datalist in the format that matches the name of the variables in the model code, then define the vector of parameters to monintor and select a suitable number of iterations for the MCMC procedure.\nIn this case, it is important to generate suitable initial values. For instance, if you did not create the function inits} and letBUGS} initialise the chains, the programme would return an error and cannot start the procedure. We set up the initial values for \\(\\boldsymbol\\alpha,\\boldsymbol\\beta,\\mu_u\\) from Normal(0,1) distributions, which ensures that the algorithm can sample reasonable starting points and run.\nWe can check that the model has reached convergence and that the estimates are reasonable. For instance, we could use the print method and apply it to the object NN, where we have stored the BUGS output. However, we have monitored many parameters and so this may be complicated to see. In cases such as this, it is probably more effective to use the element `$summary” to visualise the data, for instance using the following code\nhead(NN$summary[,c(\u0026quot;mean\u0026quot;,\u0026quot;sd\u0026quot;,\u0026quot;2.5%\u0026quot;,\u0026quot;97.5%\u0026quot;,\u0026quot;Rhat\u0026quot;,\u0026quot;n.eff\u0026quot;)])  mean sd 2.5% 97.5% Rhat n.eff mu.e[1] 0.87119481 0.02232442 0.8268950 0.914805 1.000952 10000 mu.e[2] 0.91179054 0.02240187 0.8679975 0.956400 1.001053 9700 sd.e[1] 0.07969351 0.01191242 0.0605200 0.107200 1.000956 10000 sd.e[2] 0.08954983 0.01636157 0.0641200 0.127800 1.000912 10000 alpha0[1] 0.87119481 0.02232442 0.8268950 0.914805 1.000952 10000 alpha0[2] 0.91179054 0.02240187 0.8679975 0.956400 1.001053 9700 which would simply show the first few rows of the overall summary table for a selected set of columns (those displaying the mean, sd, 2.5- and 97.5% quantiles, as well as the convergence statistics). We could also specify the parameters for which we want to see the summary statistics, for instance using the following code\nNN$summary[grep(\u0026quot;alpha\u0026quot;,rownames(NN$summary)), c(\u0026quot;mean\u0026quot;,\u0026quot;sd\u0026quot;,\u0026quot;2.5%\u0026quot;,\u0026quot;97.5%\u0026quot;,\u0026quot;Rhat\u0026quot;,\u0026quot;n.eff\u0026quot;)]  mean sd 2.5% 97.5% Rhat n.eff alpha0[1] 0.8711948 0.02232442 0.8268950 0.914805 1.000952 10000 alpha0[2] 0.9117905 0.02240187 0.8679975 0.956400 1.001053 9700 alpha1[1] 0.7912292 0.15647962 0.4809875 1.091025 1.001219 4700 alpha1[2] 0.2751874 0.08561333 0.1061975 0.443105 1.000901 10000 to show the selected summary statistics for all the nodes whose name contains the keyword alpha (this is done using the grep function — see help(grep) in your R terminal, for more details).\nAnother way to simply check convergence when the number of model parameters is very large is to plot the output for both \\(\\hat{R}\\) and \\(n_{eff}\\) for all parameters. We can do this simply using the following command\nplot(NN$summary[,\u0026quot;Rhat\u0026quot;]) to check that the value of \\(\\hat{R}\\) is below the arbitrary threshold of 1.1 for all nodes. If this is the case, then the model has reached convergence; if not, we can investigate further and check which node has not converged yet. Of course the graph can be annotated and made prettier, but this crude version can still be very helpful.\nAnother similar graphical representation can be made to check the number of effective samples obtained using the MCMC procedure.\nplot(NN$summary[,\u0026quot;n.eff\u0026quot;]) As is possible to see here, most of the nodes have a value that is close to the nominal sample size, indicating virtually no issues with autocorrelation.\nOnce we are satisfied about the output of the Bayesian model, we can feed it to BCEA to perform the economic analysis.\n# Extracts the simulated values for the population mean effectiveness and costs e=cbind(NN$sims.list$mu.e[,1],NN$sims.list$mu.e[,2]) c=cbind(NN$sims.list$mu.c[,1],NN$sims.list$mu.c[,2]) # Post-processes the results using BCEA library(BCEA)  Attaching package: \u0026#39;BCEA\u0026#39; The following object is masked from \u0026#39;package:graphics\u0026#39;: contour CEA_NN=bcea(e=e,c=c,ref = 2) # Shows the C/E plane ceplane.plot(CEA_NN) Notice that we first create suitable matrices with the simulations for the two main variables \\((\\mu_{et},\\mu_{ct})\\); we extract these simulations from the object NN — in particular, they are stored in the two lists $sims.list$mu.e and $sims.list$mu.c, respectively. Each of these two objects has dimension (10000, 2), i.e. 10000 rows (simulations) and 2 columns (treatment arms). We use the R function cbind to “bind” together these values, so that the resulting objects e and c are matrices, as required by BCEA.\nBecause we have monitored the variables eff1,eff2,cost1,cost2 in the object NN, we can also check what the model is doing in terms of imputing the missing values. As suggested in the script, the easiest way doing so is by plotting the estimated posterior distribution, which we do using the following commands.\nhist(NN$sims.list$eff1[,1],xlab=\u0026quot;QALYs for the first missing individual in t=1\u0026quot;,main=\u0026quot;\u0026quot;) abline(v=1,lwd=2) sum(NN$sims.list$eff1[,1]\u0026gt;1)/NN$n.sims [1] 0.0036 The graph shows the posterior for eff1, the measure of effectiveness in treatment arm \\(t=1\\). As is possible to see, about 0.360% of the simulated values above the theoretical upper limit of 1 — which is of course not reasonable. Another way of visualising this feature of the model output is by using the specialised plotting function coefplot (which is available from the R package bmhe — which you would need to install), to produce a graph such as the following.\nbmhe::coefplot(NN,parameter=\u0026quot;eff1\u0026quot;) + geom_vline(xintercept=1,linetype=2,size=1.3,col=\u0026quot;red\u0026quot;)  For each of the 48 individuals affected by missing QALYs in \\(t=1\\), this shows a description of the posterior distribution (imputed values). The dots represent the means, while the lines extend over the 95% credible interval. As is possible to see, there are quite a few individuals whose imputed values distributions expands above 1, although the means are all below this threshold.\n Beta-Gamma model The Beta-Gamma model is theoretically more appropriate, because it correctly recognises the natural range of the two variables — the interval [0-1] for the QALYs and the open positive real line \\((0,\\infty)\\) for the costs. Thus, it seems a more suitable modelling structure for this problem.\nIn general terms, the model code is not much more complicated than the one for the bivariate Normal — the main complication is that the regression models for the individual average QALYs and costs cannot be constructed as linear predictor. Instead, we need to rescale the average QALYs (defined in [0-1]) onto \\((-\\infty,\\infty)\\), which we do using a logistic regression \\[ \\mbox{logit}(\\phi_{eit}) = \\alpha_0 + \\alpha_1 (u_{it}-\\mu_{ut}). \\] This implies that the overall average effectiveness measure \\(\\mu_{et}\\) is just a rescaled version of the intercept \\(\\alpha_0\\) (given that we are centering the covariate \\(u_{it}\\)). In other words, we need to take the inverse logit transformation and define \\[ \\mu_{et} = \\frac{\\exp(\\alpha_0)}{1+\\exp(\\alpha_0)}. \\]\nSimilarly, we need to rescale the conditional regression for the average costs \\(\\phi_{cit}\\) as a function of the centered effectiveness, which we can do using a log-linear regression \\[ \\mbox{log}(\\phi_{cit}) = \\beta_0 + \\beta_1 (e_{it}-\\mu_{et}) \\] and again the overall average cost is obtained by rescaling the intercept \\(\\beta_0\\) as \\[ \\mu_{ct} = \\exp(\\beta_0). \\]\nIn fact, the main complication in the BUGS script is in the parameterisation of the Beta and Gamma distributions. BUGS requires that the Beta distribution is defined in terms of two scale parameters and thus we code \\(e_{it} \\sim \\mbox{Beta}(a_{ti},b_{ti})\\) for each intervention arm and individuals. However, we need to actually model the individual mean \\(\\phi_{eit}\\) and so we need to link this with \\((a_{ti},b_{ti})\\). We can use the properties of the Beta distribution and derive that \\[ a_{it} = \\phi_{eit}\\left[\\frac{\\phi_{eit}(1-\\phi_{eit})}{\\psi_{et}} - 1\\right] \\qquad \\mbox{ and } \\qquad b_{it} = (1-\\phi_{eit})\\left[\\frac{\\phi_{eit}(1-\\phi_{eit})}{\\psi_{et}} - 1\\right],\\] where \\(\\psi_{et}\\) is the variance for the Beta distribution (which is coded as ss.e in the model file). By modelling the mean \\(\\phi_{eit}\\) (through the priors induced on the parameters \\(\\boldsymbol\\alpha\\)) and standard deviation \\(\\sigma_{et}\\), we then imply a prior on \\((a_{it},b_{it})\\).\nA further complication is given by the fact that, because of the mathematical properties of the Beta distribution, \\(\\sigma_{et}\\) is constrained in its range by the value taken by the overall mean \\(\\mu_{et}\\); for this reason, we define the prior as Uniform in the range $(0,).\nAs for the Gamma distribution, we need to follow a similar strategy: the “original scale” parameters (which are required by BUGS) are the shape and rate say \\((\\zeta_{it},\\lambda_{it})\\); however, these can be related to the mean and variance using the mathematical properties of the Gamma distribution as \\[ \\mbox{shape}_{it} = \\zeta_{it} = \\frac{\\phi_{cit}^2}{\\sigma_{ct}^2} \\qquad \\mbox{ and } \\qquad \\mbox{rate}_{it} = \\lambda_{it} = \\frac{\\phi_{cit}}{\\sigma_{ct}^2}. \\] Again, priors on \\(\\boldsymbol\\beta\\) and \\(\\sigma_{ct}\\) (which in the model code is indicated as sd.c[t] and is given a truncated \\(t\\) distribution, to provide a continuous distribution on the positive real line) induce suitable priors for the shape and rate.\nThe interesting point of this model is that because of its relative complexity, OpenBUGS struggles to determine suitable initial values from which to start the simulation. In fact, we have tested several configurations, providing initial values for all the relevant model parameters and still have failed to run the model. Conversely, JAGS is able to start and successfully estimate this model and data. The main reason for this crucial difference is probabaly to do with the specific version of the sampler used by the two pieces of software.\nIn other words, to be able to run the Beta-Gamma model, it is necessary to install JAGS (which is available and easy to install from https://sourceforge.net/projects/mcmc-jags/files/. Then we need to install the R package R2jags (which is the equivalent to R2OpenBUGS). Fortunately, the differences between JAGS and OpenBUGS are virtually none. Thus, we can simply run the model following the script and using the commands below:\n# Installs and R2jags install.packages(\u0026quot;R2jags\u0026quot;) # Then loads the package library(R2jags) (this is of course only necessary once). When the package is installed, we just need to load it into our workspace and then proceed with the commands.\n# 1. specifies the model file filein=\u0026quot;Beta_Gamma.txt\u0026quot; # 2. data list datalist=list(N1=data$n[[1]],eff1=data$e[[1]],cost1=data$c[[1]],u1=data$u[[1]], N2=data$n[[2]],eff2=data$e[[2]],cost2=data$c[[2]],u2=data$u[[2]]) # 3. parameters to monitor params\u0026lt;-c(\u0026quot;mu.e\u0026quot;,\u0026quot;sd.e\u0026quot;,\u0026quot;alpha0\u0026quot;,\u0026quot;alpha1\u0026quot;,\u0026quot;beta0\u0026quot;,\u0026quot;beta1\u0026quot;,\u0026quot; Delta_e\u0026quot;,\u0026quot;Delta_c\u0026quot;,\u0026quot;mu.c\u0026quot;,\u0026quot;sd.c\u0026quot;,\u0026quot;eff1\u0026quot;,\u0026quot;eff2\u0026quot;,\u0026quot;cost1\u0026quot;,\u0026quot;cost2\u0026quot;) # 4. number of iterations n.iter\u0026lt;-10000 # 5. reset the inits function inits=NULL Finally, we’re ready to run the main jags function to run the model:\nJAGS compiles and initialises the model successfully and it does not even require us to specify the initial values (we still can do so, if we wanted to have full control, of course).\nWe can now replicate the analysis performed above to check and summarise convergence, as well as to feed the output of our Beta-Gamma Bayesian model to BCEA, to perform the economic analysis. Notice that the JAGS object BG has a slightly different structure and contains an “extra-layer” in comparison to its OpenBUGS counterpart NN}. As a result, in order to access the simulations, we need to go deeper inside the object and look into the element $BUGSoutput$sims.list.\ne\u0026lt;-cbind(BG$BUGSoutput$sims.list$mu.e[,1],BG$BUGSoutput$sims.list$mu.e[,2]) c\u0026lt;-cbind(BG$BUGSoutput$sims.list$mu.c[,1],BG$BUGSoutput$sims.list$mu.c[,2]) CEA_BG\u0026lt;-bcea(e=e,c=c,ref = 2) # Plots the CEAC ceac.plot(CEA_NN) points(CEA_BG$k,CEA_BG$ceac,t=\u0026quot;l\u0026quot;,col=\u0026quot;red\u0026quot;) Interestingly, the results are different, depending on the model used for imputation of the missing data. The graph above shows the CEACs for the two model specifications and as is possible to see the Normal-Normal model somewhat overestimates the probability of cost-effectiveness. This is likely due to the higher mean QALYs associated with each individual, given the imputations exceed the theoretical range of 1. If we check the coefplot for the Beta-Gamma model, we now see that all the distributions are within the range [0-1], as should be. In the graph we display the distributions for the Beta-Gamma (black) and Normal-Normal (red) models. A clear indication of the “pull” towards higher values that the bivariate Normal model shows is given by individual number 6. The red line is shifted upwards in comparison to the black one — this is because the Normal-Normal model does not know that it should not go above one (because the Normal distribution is unrestricted) and so it pulls up the estimate for this individual. This translate in slightly higher values for the QALYs, which in turn artificially increase the cost-effectiveness profile of the active intervention.\n ","date":1655978400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655978400,"objectID":"c4dc4ca1610d846360dfec6fcb8417fb","permalink":"/practical/10_missing/solutions/","publishdate":"2021-06-22T00:00:00Z","relpermalink":"/practical/10_missing/solutions/","section":"practical","summary":"Bivariate Normal model The data are stored in a list format because the variables are non-balanced, in terms of sample size. In other words, the data for arm \\(t=1\\) contain fewer points than for \\(t=2\\).","tags":[],"title":"Practical 10. Missing data - SOLUTIONS","type":"book"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1656080100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656080100,"objectID":"a3ae4e4e9193b67ac9af2a18cfbcf9dd","permalink":"/syllabus/practical15/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical15/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","VoI"],"title":"Practical 15. Computing EVSI using regression","type":"talk"},{"authors":["Anna Heath"],"categories":null,"content":" Summary: Regression-based methods for EVSI computation Using SAVI to compute EVSI  Back to syllabus\n ","date":1656078300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656078300,"objectID":"c4e292a3868b84d7d970df128b5d0e2a","permalink":"/syllabus/lecture16/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture16/","section":"syllabus","summary":" Summary: Regression-based methods for EVSI computation Using SAVI to compute EVSI  Back to syllabus\n ","tags":["Lectures","Voi"],"title":"Lecture 16: Regression-based EVSI","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1656075600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656075600,"objectID":"fdd10c8c52ae3d145266d929123617a9","permalink":"/syllabus/practical14/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical14/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","VoI"],"title":"Practical 14. Computing the EVSI using Monte Carlo simulations","type":"talk"},{"authors":["Anna Heath"],"categories":null,"content":" Summary: Efficient Computation methods for EVSI Efficient Nested Monte Carlo Using the EVSI Web App  Back to syllabus\n ","date":1656069300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656069300,"objectID":"2df60e935f9dd5d9f992c40fab92c2ce","permalink":"/syllabus/lecture15/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture15/","section":"syllabus","summary":" Summary: Efficient Computation methods for EVSI Efficient Nested Monte Carlo Using the EVSI Web App  Back to syllabus\n ","tags":["Lectures","Voi"],"title":"Lecture 15: Calculating expected value of sample information","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1656066600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656066600,"objectID":"991582958840c0af13d4a9c55db38c2f","permalink":"/syllabus/practical13/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical13/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","VoI"],"title":"Practical 13. Generating data for EVSI","type":"talk"},{"authors":["Anna Heath"],"categories":null,"content":" Summary: Variation in individual outcomes Predictive distribution data simulation Data generation in Excel and R  Back to syllabus\n ","date":1656063900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656063900,"objectID":"31016f6ff4e0932d2b57d71475085fb8","permalink":"/syllabus/lecture14/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture14/","section":"syllabus","summary":" Summary: Variation in individual outcomes Predictive distribution data simulation Data generation in Excel and R  Back to syllabus\n ","tags":["Lectures","Voi"],"title":"Lecture 14: Generating data for the analysis of the EVSI","type":"talk"},{"authors":["Gianluca Baio"],"categories":null,"content":" Summary: Theory behind the EVSI Expected net benefit of sampling Optimising trial design  Back to syllabus\n ","date":1656061200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656061200,"objectID":"2754fd61f9174ac1fcf393cbef5dc847","permalink":"/syllabus/lecture13/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture13/","section":"syllabus","summary":" Summary: Theory behind the EVSI Expected net benefit of sampling Optimising trial design  Back to syllabus\n ","tags":["Lectures","Voi"],"title":"Lecture 13: Expected value of sample information","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655996400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655996400,"objectID":"2d51cded3f00e0c5784884b6cba4e01c","permalink":"/syllabus/practical12/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical12/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","VoI"],"title":"Practical 12. Computing the EVPPI in BCEA and SAVI","type":"talk"},{"authors":["Anna Heath"],"categories":null,"content":" Summary: Theory behind the EVPPI Computational methods: regression-based methods  Back to syllabus\n ","date":1655991900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655991900,"objectID":"4c7b2796a3bfe00630f5725a25bfd615","permalink":"/syllabus/lecture12/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture12/","section":"syllabus","summary":" Summary: Theory behind the EVPPI Computational methods: regression-based methods  Back to syllabus\n ","tags":["Lectures","Voi"],"title":"Lecture 12: Expected value of partial information","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655989200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655989200,"objectID":"225e5c906bdcf7d5a120ac8eb6d4e0c1","permalink":"/syllabus/practical11/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical11/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","VoI"],"title":"Practical 11. Computing the EVPI using nested Monte Carlo simulations","type":"talk"},{"authors":["Anna Heath"],"categories":null,"content":" Summary: Introduction to Research prioritisation Value of Information and relevant measures Applications/examples  Back to syllabus\n ","date":1655982000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655982000,"objectID":"b1bfcd77b0933b9b5fae52d8aea396dc","permalink":"/syllabus/lecture11/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture11/","section":"syllabus","summary":" Summary: Introduction to Research prioritisation Value of Information and relevant measures Applications/examples  Back to syllabus\n ","tags":["Lectures","Voi"],"title":"Lecture 11: Introduction to Value of Information","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655978400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655978400,"objectID":"ef343d3333bf3ab8309dd9bf169d4227","permalink":"/syllabus/practical10/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical10/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","ILD"],"title":"Practical 10. Missing data","type":"talk"},{"authors":["Nathan Green"],"categories":null,"content":" Summary: Background – what’s difficult about missing data? Missing data mechanisms Missing data in health economic evaluation Applications/examples  Back to syllabus\n ","date":1655974800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655974800,"objectID":"45512c6e12435a4d2012ff9246b0fa9c","permalink":"/syllabus/lecture10/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture10/","section":"syllabus","summary":" Summary: Background – what’s difficult about missing data? Missing data mechanisms Missing data in health economic evaluation Applications/examples  Back to syllabus\n ","tags":["Lectures","ILD"],"title":"Lecture 10: Missing data in cost-effectiveness modelling","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655910900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655910900,"objectID":"7cd160d302908ffbe526916dcb1b7767","permalink":"/syllabus/practical9/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical9/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","Survival"],"title":"Practical 9. Markov models","type":"talk"},{"authors":["Nathan Green"],"categories":null,"content":" Summary: State-transition models for clinical history Bayesian implementation\n Markov models \u0026amp; survival analysis  Back to syllabus\n ","date":1655907300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655907300,"objectID":"ab7482b92cb175fefb2a64a34b6e888e","permalink":"/syllabus/lecture9/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture9/","section":"syllabus","summary":" Summary: State-transition models for clinical history Bayesian implementation\n Markov models \u0026amp; survival analysis  Back to syllabus\n ","tags":["Lectures","ALD","Survival"],"title":"Lecture 9: Markov models","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655902800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655902800,"objectID":"11217a479ef3d05f17cd43fd76dde703","permalink":"/syllabus/practical8/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical8/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","ILD","Survival"],"title":"Practical 8. Survival analysis","type":"talk"},{"authors":["Gianluca Baio"],"categories":null,"content":" Summary: Survival data in health economic evaluations Issues with Bayesian modelling PSA in survival analysis  Notes The practical uses the R packages survHE. If you are interested, you can look at the extended survHE tutorial (here) and the survHE paper.\nBack to syllabus\n  ","date":1655895600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655895600,"objectID":"ed64369c38a111e15fdca5c9a83d522a","permalink":"/syllabus/lecture8/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture8/","section":"syllabus","summary":"Summary: Survival data in health economic evaluations Issues with Bayesian modelling PSA in survival analysis  Notes The practical uses the R packages survHE. If you are interested, you can look at the extended survHE tutorial (here) and the survHE paper.","tags":["Lectures","ILD","Survival"],"title":"Lecture 8: Survival analysis in HTA","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655892000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655892000,"objectID":"79a824cd1e8e6bf5e53a18ffc1330ff0","permalink":"/syllabus/practical7/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical7/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals"],"title":"Practical 7. PSA to structural uncertainty","type":"talk"},{"authors":["Gianluca Baio"],"categories":null,"content":" Summary: The rationale for structural uncertainty Model error and PSA to model uncertainty  Back to syllabus\n ","date":1655888400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655888400,"objectID":"f989cd358d16c9ef7029141022818c58","permalink":"/syllabus/lecture7/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture7/","section":"syllabus","summary":" Summary: The rationale for structural uncertainty Model error and PSA to model uncertainty  Back to syllabus\n ","tags":["Lectures"],"title":"Lecture 7: Model error and structural analysis","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655825400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655825400,"objectID":"6bcf00fffd879c6c6510bd1a45b9c970","permalink":"/syllabus/practical6/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical6/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","ALD"],"title":"Practical 6. Network meta-analysis","type":"talk"},{"authors":["Gianluca Baio"],"categories":null,"content":" Summary: Motivation Fixed effects NMA Random effects NMA Applications/examples  Back to syllabus\n ","date":1655820900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655820900,"objectID":"a939a4351fc8371dca6f4e0e07fc5b22","permalink":"/syllabus/lecture6/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture6/","section":"syllabus","summary":" Summary: Motivation Fixed effects NMA Random effects NMA Applications/examples  Back to syllabus\n ","tags":["Lectures","ALD"],"title":"Lecture 6: Evidence synthesis and network meta-analysis","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655813700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655813700,"objectID":"19682640d2dda033234dfe360a5156ab","permalink":"/syllabus/practical5/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical5/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","ALD"],"title":"Practical 5. Evidence synthesis and decision models","type":"talk"},{"authors":["Nathan Green"],"categories":null,"content":" Summary: Role of evidence synthesis in decision modelling: Absolute and relative effects Meta-analysis of aggregated summaries from RCTs Evidence synthesis in health economics: Influenza example  Back to syllabus\n ","date":1655810100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655810100,"objectID":"6950a33ce8c47f81b1bf67610eccc585","permalink":"/syllabus/lecture5/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture5/","section":"syllabus","summary":"Summary: Role of evidence synthesis in decision modelling: Absolute and relative effects Meta-analysis of aggregated summaries from RCTs Evidence synthesis in health economics: Influenza example  Back to syllabus","tags":["Lectures","ALD"],"title":"Lecture 5: Aggregated level data","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655805600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655805600,"objectID":"2185f1ae493b375093869abca674e36b","permalink":"/syllabus/practical4/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical4/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","ILD"],"title":"Practical 4. Cost-effectiveness analysis with individual-level data","type":"talk"},{"authors":["Anna Heath"],"categories":null,"content":" Summary: Data \u0026amp; “standard” analysis Example: UK700 Analysis of cost data Analysis of cost-effectiveness data  Back to syllabus\n ","date":1655802000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655802000,"objectID":"9bfb63c14ff28402f2d08b346b5d3f2b","permalink":"/syllabus/lecture4/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture4/","section":"syllabus","summary":" Summary: Data \u0026amp; “standard” analysis Example: UK700 Analysis of cost data Analysis of cost-effectiveness data  Back to syllabus\n ","tags":["Lectures","ILD"],"title":"Lecture 4: Individual level data in health economics","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655742600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655742600,"objectID":"a5a7bc5a3b1395de51d6948c6d264ecb","permalink":"/syllabus/practical3/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical3/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","BCEA"],"title":"Practical 3. Introduction to R and cost-effectiveness analysis using BCEA","type":"talk"},{"authors":["Gianluca Baio"],"categories":null,"content":" Summary: Health economic evaluation: What is and why do we need health economics? A framework for health economic evaluation Standard vs Bayesian HTA Decision-making: Cost-effectiveness plane; ICER; EIB  Back to syllabus\n ","date":1655739000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655739000,"objectID":"61b665cc02801ee0f82d137d7bdffa07","permalink":"/syllabus/lecture3/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/syllabus/lecture3/","section":"syllabus","summary":"Summary: Health economic evaluation: What is and why do we need health economics? A framework for health economic evaluation Standard vs Bayesian HTA Decision-making: Cost-effectiveness plane; ICER; EIB  Back to syllabus","tags":["Lectures"],"title":"Lecture 3: Introduction to health economic evaluation","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655733600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655733600,"objectID":"daae76108e2b99d4d71c8d7285e59d60","permalink":"/syllabus/practical2/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical2/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","BUGS"],"title":"Practical 2. MCMC in BUGS","type":"talk"},{"authors":["Nathan Green"],"categories":null,"content":" Summary Bayes theorem for learning about parameters from observed data.\n Introduction to Markov chain Monte Carlo (MCMC) Introduction to BUGS for estimating posterior distributions of parameters given data  Back to syllabus\n ","date":1655726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655726400,"objectID":"ce86123883445b1ea52c48cf16a5ed4f","permalink":"/syllabus/lecture2/","publishdate":"2020-01-11T00:00:00Z","relpermalink":"/syllabus/lecture2/","section":"syllabus","summary":"Summary Bayes theorem for learning about parameters from observed data.\n Introduction to Markov chain Monte Carlo (MCMC) Introduction to BUGS for estimating posterior distributions of parameters given data  Back to syllabus","tags":["Lectures","BUGS"],"title":"Lecture 2: Learning from data using MCMC and BUGS","type":"talk"},{"authors":[],"categories":null,"content":" Back to syllabus\n","date":1655723700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655723700,"objectID":"d7ef0021b857183fc4730c4f9d274d0b","permalink":"/syllabus/practical1/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/practical1/","section":"syllabus","summary":"Back to syllabus","tags":["Practicals","BUGS"],"title":"Practical 1. Monte Carlo in BUGS","type":"talk"},{"authors":["Gianluca Baio"],"categories":null,"content":" Summary: The Bayesian paradigm — expressing uncertainty using probabilities Overview of probability distributions for different types of quantities Inductive inference: Bayesian reasoning, Basic ideas, Forming ‘priors’” Predicting data with uncertain parameters. Introduction to Monte Carlo sampling in BUGS  Back to syllabus\n ","date":1655719200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655719200,"objectID":"7bd4d5371726f153db5167e990219bef","permalink":"/syllabus/lecture1/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/syllabus/lecture1/","section":"syllabus","summary":"Summary: The Bayesian paradigm — expressing uncertainty using probabilities Overview of probability distributions for different types of quantities Inductive inference: Bayesian reasoning, Basic ideas, Forming ‘priors’” Predicting data with uncertain parameters.","tags":["Lectures","BUGS"],"title":"Lecture 1: Introduction to Bayesian reasoning, computation and BUGS","type":"talk"},{"authors":null,"categories":null,"content":" Please give scores and comments on the lecture and practical sessions to help us improve the course for the future. 🙇\nYour Name: Your Email: Affiliation: Title:  1. How do you assess the quality of the lectures from 1 (very poor) to 10 (excellent)    Very poor                       Excellent   Additional comments\n  2. How do assess the quality of the practicals from 1 (very poor) to 10 (excellent)    Very poor                       Excellent   Additional comments\n  3. What lecture(s) did you think were the most useful \n1. Introduction to Bayesian reasoning, computation and BUGS 2. Learning from data using MCMC and BUGS 3. Introduction to health economic evaluation 4. Individual level data in health economics 5. Aggregated level data 6. Evidence synthesis and network meta-analysis 7. Model error and structural analysis 8. Survival analysis in HTA 9. Markov models 10. Missing data in cost-effectiveness modelling 11. Introduction to Value of Information 12. Expected value of partial information 13. Expected value of sample information 14. Generating data for the analysis of the EVSI 15. Calculating expected value of sample information 16. Regression-based EVSI Additional comments\n  4. Was the material useful?   Lecture slides\n Not useful at all             Extremely useful   R practicals  Not useful at all             Extremely useful   Additional comments\n  5. Did you feel you were prepared for the level of complexity of the lecture?   Not very prepared at all             Very prepared   Additional comments\n  6. Do you think you will use any of the concepts/methods/tools taught at the course in your applied work? \nDefinitely - I will modify my workflow to include some or all of these May be - I will try and use these and convince my boss/colleagues of their usefulness Probably not - they are too complicated Definitely not - I don’t think they were useful at all Additional comments\n  7. Do you think the period of the year in which the summer school was held was good?   Not so much             Excellent   Additional comments\n  8. Do you think the location of the course was good?   Very poor             Excellent   Additional comments\n  9. Would you recommend the course to colleagues? \nDefinitely May be Probably not Definitely not Additional comments\n 10. What did you think about the lecturers? Gianluca Baio\na. Knowledgeable b. Friendly c. Helpful d. None of the above  -- Comments\n Anna Heath\na. Knowledgeable b. Friendly c. Helpful d. None of the above  -- Comments\n Nathan Green\na. Knowledgeable b. Friendly c. Helpful d. None of the above  -- Comments\n \n11. Any other comment?:   Submit   ","date":1654732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654732800,"objectID":"958b770e2a85e04ae99e35daf0850bf8","permalink":"/feedback/","publishdate":"2022-06-09T00:00:00Z","relpermalink":"/feedback/","section":"","summary":"Please give scores and comments on the lecture and practical sessions to help us improve the course for the future. 🙇\nYour Name: Your Email: Affiliation: Title:  1. How do you assess the quality of the lectures from 1 (very poor) to 10 (excellent)    Very poor                       Excellent   Additional comments","tags":null,"title":"Course feedback","type":"page"},{"authors":null,"categories":null,"content":" Day 1: Monday, 20 June 2022   Start  End  Topic  Lecturer  Room      10:00  11:00  Lecture 1: Introduction to Bayesian reasoning, computation and BUGS  Gianluca Baio  Extranef-126    11:00  11:15  Coffee break      11:15  12:00  Practical 1. Monte Carlo in BUGS   Extranef-126    12:00  13:00  Lecture 2: Learning from data using MCMC and BUGS  Nathan Green  Extranef-126    13:00  14:00  Lunch      14:00  15:15  Practical 2. MCMC in BUGS   Extranef-126    15:15  15:30  Coffee break      15:30  16:30  Lecture 3: Introduction to health economic evaluation  Gianluca Baio  Extranef-126    16:30  17:00  Practical 3. Introduction to R and cost-effectiveness analysis using BCEA   Extranef-126      Day 2: Tuesday, 21 June 2022   Start  End  Topic  Lecturer  Room      09:00  10:00  Lecture 4: Individual level data in health economics  Anna Heath  Extranef-126    10:00  11:00  Practical 4. Cost-effectiveness analysis with individual-level data   Extranef-126    11:00  11:15  Coffee break      11:15  12:15  Lecture 5: Aggregated level data  Nathan Green  Extranef-126    12:15  13:15  Practical 5. Evidence synthesis and decision models   Extranef-126    13:15  14:15  Lunch      14:15  15:15  Lecture 6: Evidence synthesis and network meta-analysis  Gianluca Baio  Extranef-126    15:15  15:30  Coffee break      15:30  16:30  Practical 6. Network meta-analysis   Extranef-126      Day 3: Wednesday, 22 June 2022   Start  End  Topic  Lecturer  Room      09:00  10:00  Lecture 7: Model error and structural analysis  Gianluca Baio  Extranef-126    10:00  10:45  Practical 7. PSA to structural uncertainty   Extranef-126    10:45  11:00  Coffee break      11:00  12:00  Lecture 8: Survival analysis in HTA  Gianluca Baio  Extranef-126    12:00  13:00  Lunch      13:00  14:00  Practical 8. Survival analysis   Extranef-126    14:00  14:15  Coffee break      14:15  15:15  Lecture 9: Markov models  Nathan Green  Extranef-126    15:15  16:00  Practical 9. Markov models   Extranef-126      Day 4: Thursday, 23 June 2022   Start  End  Topic  Lecturer  Room      09:00  10:00  Lecture 10: Missing data in cost-effectiveness modelling  Nathan Green  Extranef-110    10:00  10:45  Practical 10. Missing data   Extranef-110    10:45  11:00  Coffee break      11:00  12:00  Lecture 11: Introduction to Value of Information  Anna Heath  Extranef-110    12:00  13:00  Lunch      13:00  13:45  Practical 11. Computing the EVPI using nested Monte Carlo simulations   Extranef-110    13:45  14:45  Lecture 12: Expected value of partial information  Anna Heath  Extranef-110    14:45  15:00  Coffee break      15:00  16:00  Practical 12. Computing the EVPPI in BCEA and SAVI   Extranef-110      Day 5: Friday, 24 June 2022   Start  End  Topic  Lecturer  Room      09:00  09:45  Lecture 13: Expected value of sample information  Gianluca Baio  Extranef-109    09:45  10:15  Lecture 14: Generating data for the analysis of the EVSI  Anna Heath  Extranef-109    10:15  10:30  Coffee break      10:30  11:15  Practical 13. Generating data for EVSI   Extranef-109    11:15  12:00  Lecture 15: Calculating expected value of sample information  Anna Heath  Extranef-109    12:00  13:00  Lunch      13:00  13:45  Practical 14. Computing the EVSI using Monte Carlo simulations   Extranef-109    13:45  14:15  Lecture 16: Regression-based EVSI  Anna Heath  Extranef-109    14:15  15:00  Practical 15. Computing EVSI using regression   Extranef-109      ","date":1654732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654732800,"objectID":"bc4025bc8065e668524babefec73eb41","permalink":"/daily-schedule/","publishdate":"2022-06-09T00:00:00Z","relpermalink":"/daily-schedule/","section":"","summary":"Day 1: Monday, 20 June 2022   Start  End  Topic  Lecturer  Room      10:00  11:00  Lecture 1: Introduction to Bayesian reasoning, computation and BUGS  Gianluca Baio  Extranef-126    11:00  11:15  Coffee break      11:15  12:00  Practical 1.","tags":null,"title":"Daily schedule","type":"page"},{"authors":null,"categories":null,"content":"On your own machine Since both R and BUGS (or JAGS, which is an alternative Bayesian software) are free, you can install them on your own laptop/desktop, if you want.\nThe following gives you a quick guide on how you install the software that is required for the analyses that we will go through in the practicals:\n OpenBUGS JAGS (optional) R and specifically the packages R2OpenBUGS and BCEA. Other optional packages (e.g. reshape, dplyr, INLA and R2jags) may need to be installed The R front-end Rstudio (optional) A spreadsheet calculator; if you have a valid license for MS Excel on your machine that is OK. If not, you can download the freely available LibreOffice, which is a decent surrogate (at least for the tasks that we will require in the practicals).   The file install.R can be used to install all the necessary packages (in line with the specification of the Binder Virtual Machine).\nYou can also download a .zip file with the code and scripts for all the practicals, here.\n   Notice that you don\u0026rsquo;t have to install both OpenBUGS and JAGS \u0026mdash; the former is sufficient for the purposes of this course.   Specific installation instructions  MS Windows users If you are a Windows user, your setting should be fairly easy.\n Download the file OpenBUGS323setup.exe from the webpage https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2014/07/OpenBUGS323setup.zip, extract the exe file from the downloaded zip file and run it by double-clicking on it. Download R from http://cran.r-project.org/bin/windows/ (click on the link \u0026ldquo;install R for the first time\u0026rdquo;\u0026quot;). Once you have installed R, open it and type in the terminal the command install.packages(\u0026quot;R2OpenBUGS\u0026quot;). This command will download the package R2OpenBUGS, which is needed to interface OpenBUGS with R. Follow the on-screen instructions (you will be asked to select a \u0026ldquo;mirror from which to obtain the file). Make sure that the download has worked by typing in the terminal the command library(R2OpenBUGS). If you do not see any error message, then the package has been successfully installed. Install the BCEA package that we will use throughout the practicals, by typing install.packages(\u0026quot;BCEA\u0026quot;). Repeat the installation process for the other packages that are used in the practicals (e.g. reshape and dplyr). Since this is optional, you can leave this final step to when it is actually needed.  If you like, you can also install JAGS, following these steps.\n Go to http://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Windows/ and click on the latest available executable file (currently, JAGS-4.3.0.exe). Running this file will install JAGS on your machine. If you do so, then in the R terminal type the command install.packages(\u0026quot;R2jags\u0026quot;). This will allow you to use JAGS (instead of OpenBUGS) when doing the practicals \u0026mdash; notice that to make a package available you will have to load it to the workspace by using the library(name_package) command.      Linux or Mac OS users Installing R and BCEA Linux or Mac OS users should follow slightly different approaches. The installation of R is pretty much the same as for MS WIndows users. From the webpage http://cran.r-project.org/ select your operating system (Linux or Mac OS) and then your version (eg debian, redhat, suse or ubuntu, for Linux). Follow the instructions to install the software. Once this is done, open R and install the package BCEA by typing at the terminal the command install.packages(\u0026quot;BCEA\u0026quot;). You can use similar commands to install other packages.\nInstalling OpenBUGS and JAGS in Linux While both OpenBUGS and JAGS run natively in Linux (see below for details on how to install them directly), the graphical interface is not available for OpenBUGS. Because we will use it for at least the first few practicals, it is advisable to install wine, a \u0026ldquo;compatibility layer\u0026rdquo;\u0026quot; that allows to run Windows applications from Linux or Mac. Instructions are available at https://www.winehq.org/download/.\nWhen you have installed wine, you can also install OpenBUGS, which you will be then able to access using the graphical interface, by following these steps.\n Go to the webpage https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2014/07/OpenBUGS323setup.zip, extract from the .zip file the windows executable file OpenBUGS323setup.exe and place it in your default directory for wine programs (usually ~/.wine/drive_c). Run the OpenBUGS installer  Open a terminal window; Move to the directory where you placed the OpenBUGS executable you downloaded in step 1, something like: cd ~/.wine/drive_c Type: wine OpenBUGS323setup.exe; Wait for a while and then follow the prompts to install \u0026mdash; by default, the installation folder is ~/.wine/drive_c/Program Files/OpenBUGS323 NB: There may be an error at the end, this is OK. Close down the Terminal Window   Test OpenBUGS by opening a new terminal window typing the command: wine ~/.wine/drive_c/Program Files/OpenBUGS323/OpenBUGS.exe Download the R2OpenBUGS package from http://www.openbugs.info/w/UserContributedCode Open R and install R2OpneBUGS by typing the command install.packages(\u0026quot;R2OpenBUGS\u0026quot;)  You can also install the Linux version of OpenBUGS (available from here) by following the instructions given at https://www.mrc-bsu.cam.ac.uk/software/bugs/openbugs/building-and-packaging-openbugs/openbugs-linux-installation/. This will work just as well, but you won\u0026rsquo;t be able to access the graphical interface.\n NB: Under Linux, you may need to also install additional packages to support the OpenBUGS installation. For instance, under Debian or Ubuntu, you may need to also run in your terminal\nsudo apt-get install g++-multilib  to install the library g++multilib.\n  In addition, you can also install JAGS, following these steps.\n Go to http://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/ and download the latest tar.gz file (currently, JAGS-4.3.0.tar.gz) Open a terminal window and extract the content of the archive file by typing the command tar xzvf JAGS-4.3.0.tar.gz Move to the directory (which has just been created) using the command cd JAGS-4.3.0 Run the configuration by typing sudo ./configure --prefix=/us \u0026amp;\u0026amp; sudo make \u0026amp;\u0026amp; sudo make install Clean up the unnecessary files and folder by typing cd .. \u0026amp;\u0026amp; sudo rm -fr JAGS-4.3.0 \u0026amp;\u0026amp; rm JAGS-4.3.0.tar.gz  If you decide to do so, then open R and install the package R2JAGS (using the same command as for the installation of the package R2OpneBUGS). Notice that you don\u0026rsquo;t have to install both OpenBUGS and JAGS \u0026mdash; the former is sufficient for the purposes of this course.\nInstalling OpenBUGS and JAGS in Mac OS While similar in spirit, installation under Mac OS is slightly more complex, because the process requires some extra software that is not automatically installed. Basically, you need to follow this procedure, as detailed at the website http://www.jkarreth.net/bayes-icpsr.html#bugsmac.\n Update your Mac OS to the newest version. Install Xcode through the App Store. Check if you have X11 installed (this is a a windowing system, common on Unix-like operating systems, which, believe it or not, MaxOs is!): hit Command-Space, type X11, and see if the program shows up. If not, install it from here. Download a the stable pre-compiled version of wine. Instructions to install wine on Mac OS are available here. Download OpenBUGS323setup.exe (windows executable) and place it in your default directory for wine programs (usually ~/.wine/drive_c). Run the OpenBUGS installer:  Open XQuartz and a Terminal Window; Move to the directory where you placed the OpenBUGS executable you downloaded in step 2; Type: wine OpenBUGS323setup.exe; Wait for a while and then follow the prompts to install \u0026mdash; remember the directory you created to install it (default is ~/[username]/.wine/drive_c/Program Files/OpenBUGS323) NB: There may be an error at the end, this is OK. Close down the Terminal Window   It is possible that you need to specify the installation directory to tell your system specifically where to look for BUGS. Typically this will mean adding the option bugs.directory = \u0026quot;/Users/yourusername/.wine/drive_c/Program Files/OpenBUGS232\u0026quot; (or similar, depending on where you have installed the software!) to the call to the bugs function under R2OpenBUGS. Note that you need to replace \u0026ldquo;yourusername\u0026rdquo; with your Mac\u0026rsquo;s user name.  Notice that, if you like, you can install R under wine (rather than natively in Mac OS). Download the MS Windows executable file from CRAN and repeat the instructions above, replacing the command wine OpenBUGS323setup.exe with the command wine R-XXXX.exe, where R-XXXX.exe is the name of the executable file.\n There are some reports that OpenBUGS may fail to work on some Mac OS versions. Sometimes, when trying to use OpenBUGS from R, it will complain that it can\u0026rsquo;t find the programme. The bugs function in the R2OpenBUGS package takes an additional input OpenBUGS.pgm, which should be set to the full path to the OpenBUGS executable file. You can try and issue the R command Sys.which(\u0026quot;OpenBUGS\u0026quot;) at a R terminal and see whether it returns a full path and then pass that string as value for OpenBUGS.pgm, eg if Sys.which(\u0026quot;OpenBUGS\u0026quot;) returns the string /usr/local/bin/OpenBUGS, then set\nbugs(..., OpenBUGS.pgm=\u0026quot;/usr/local/bin/OpenBUGS\u0026quot;)    In addition, you can also install JAGS, following these steps, as detailed at the webpage http://www.jkarreth.net/bayes-icpsr.html#jagsmac.\n Install the most recent version of R from the CRAN website. (Optional) Download and install RStudio (NB: select the version for the your release of MacOS!). Install Clang (currently clang-8.0.0.pkg) and GNU Fortran (currently, gfortran-6.1.pkg.dmg) from the CRAN tools directory. Note that the most updated release for these may vary so check you select the correct one. Now install JAGS version 4.3.0 (JAGS-4.3.0.dmg) from here. Detailed instructions quoted from the JAGS readme file:  Download the disk image from the JAGS website. Double click on the disk image to mount (this may not be required). Double click on the JAGS-4.3.0.mpkg file within the mounted disk image. Follow the instructions in the installer. If you receive a warning that this software cannot be installed because it comes from an unidentified developer, you need to go to \u0026ldquo;System Preferences\u0026rdquo;\u0026quot; \u0026gt; \u0026ldquo;Security \u0026amp; Privacy\u0026rdquo;\u0026quot;, and authorize the installation there before proceeding. Authenticate as the administrative user. The first user you create when setting up Mac OS X has administrator privileges by default.   Start the Terminal and type jags to see if you receive the message: Welcome to JAGS 4.3.0. Open R and install the packages R2jags, coda, R2WinBUGS, lattice, and rjags, by typing install.packages(c(\u0026quot;R2jags\u0026quot;, \u0026quot;coda\u0026quot;, \u0026quot;R2WinBUGS\u0026quot;, \u0026quot;lattice\u0026quot;, \u0026quot;rjags\u0026quot;)) in the R command line.   NB: This post may be helpful in finding instructions to install JAGS under MacOs. In general, the JAGS sourceforge page has several posts/requests for help that you may find very similar to your own.      Virtual machine It is of course helpful to set up your own machine with all the relevant software, so you can re-use it later in life. However, you can also use a virtual machine (VM) we have created for you (specifically using binder).\nWe have set up a VM (if you are interested, it is based on Linux), in which we have installed:\n A stable version of R and Rstudio. Specifically, the current installation uses R 4.1.2 (2021-11-01) and Rstudio 2022.02.0 Build 443; OpenBUGS 3.2.3; JAGS 4.3.0; The packages R2jags, R2OpenBUGS, BCEA, tidyverse and all the relevant \u0026ldquo;dependencies\u0026rdquo;. The package bmhe, which contains a set of utility functions to be used specifically for the practicals.   If you want to install the package bmhe so you can use it on your local machine, you need to type the following command on your R terminal:\ninstall.packages(\u0026quot;remotes\u0026quot;) remotes::install_github(\u0026quot;giabaio/bmhe_utils\u0026quot;)  This will first install the package remotes from the main R repository (CRAN) and then use it to install bmhe from its GitHub repository. Installing packages from a GitHub repository is becoming a very popular and at times very efficient alternative, so it\u0026rsquo;s helpful to know how to do it\u0026hellip;\nNB The full list of packages installed in the binder VM is given in the file install.R that is available under the root (the main, landing folder: this is your working directory when you switch the binder VM on).\n  This means that when you access the VM, you won\u0026rsquo;t have to fiddle with the setting and will be able to use it for the practicals without further issues with installation.\nTo use the binder VM, simply go to the GitHub repository (note that this is the \u0026ldquo;branch\u0026rdquo; named bmhe and this is the one you need to use \u0026mdash; not the other branches!) and click on the button in the bottom-left corner (marked with the text \u0026ldquo;launch binder\u0026rdquo;). This will open the \u0026ldquo;BinderHub\u0026rdquo; allowing us to share the GitHub repository in the form of a remote Rstudio environment, with all the relevant files already installing. Alternatively, simply direct your browser to this URL.\nThis should load up fairly quickly (as it\u0026rsquo;s been compiled already \u0026mdash; it is possible, though unlikely, that it may need to recompile the binder VM, but even then, it should not take more than a minute or so\u0026hellip;). Once it\u0026rsquo;s done, your browser will have a Rstudio instance open.\nYou can use the bottom-right corner window as a file-navigator. The tab labelled as Files will show the file system for the VM. You don\u0026rsquo;t need to worry about all the files and can click to practical, which will navigate to the folder containing the files for the various practicals. If you keep navigating the files, you can reach the relevant path and open the various R scripts, for instance moving to practical/01_monte-carlo/ shows four .R files that can be used to do the first practical (effectively only using R).\n The only drawback of using the binder VM is that for now it is not \u0026ldquo;persistent\u0026rdquo;. This means that the changes you make to the files are not saved from one session to the next (a tool existed to turn the binder into a persistent VM, but it has been temporarily discontinued).\nIn addition, it can also be a bit slow (it is after all a remote machine\u0026hellip;), although this obviously depends on how fast/powerful your computer is and you need to be online while working on Rstudio, which you don\u0026rsquo;t on a local installation.\nFor these reasons, I probably would recommend against using it, if you were doing \u0026ldquo;serious\u0026rdquo; work.\nBut: it may be very helpful in terms of getting on the right track without having to worry about the subtleties of the software installation. In addition, tools such as docker and binder are generally very helpful to share your work and guarantee reproducible research, so it is probably worth spending a little time investigating how they work\u0026hellip;\n  Helpful tools Here\u0026rsquo;s a list of some other helpful tools for (Bayesian) modelling in HTA applications\n  SHELF: a package of documents, templates and software to carry out elicitation of probability distributions for uncertain quantities from a group of experts\n  SAVI: a web-app to do Value of Information computations\n  BCEAweb: a web-app to run BCEA with PSA and VoI applications\n  ","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"8c16d398a963ec4949cfc90916633b90","permalink":"/tips/computer-specification/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/tips/computer-specification/","section":"tips","summary":"On your own machine Since both R and BUGS (or JAGS, which is an alternative Bayesian software) are free, you can install them on your own laptop/desktop, if you want.","tags":[""],"title":"Computer specification","type":"tips"},{"authors":["NICE Decision Support Unit"],"categories":null,"content":"","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"34666e3d66ec98b918968ed344a26615","permalink":"/publication/nice-dsu-es/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/nice-dsu-es/","section":"publication","summary":"The National Institute for Health and Clinical Excellence (formerly, National Institute for Clinical Excellence, [NICE](https://www.nice.org.uk/)) is arguably the most important Health Technology Assessment (HTA) Agency, globally. NICE has a series of \"Technical Support Documents\", in which specific topics that are highly prevalent in HTA are discussed and guidance is provided to ensure best practices are used by modellers.","tags":[""],"title":"Evidence Synthesis Series","type":"publication"},{"authors":["NICE Decision Support Unit"],"categories":null,"content":"","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"ea4d7d1b670b7839b63ba71da3f3e01d","permalink":"/publication/nice-dsu-partition-survival/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/nice-dsu-partition-survival/","section":"publication","summary":"The National Institute for Health and Clinical Excellence (formerly, National Institute for Clinical Excellence, [NICE](https://www.nice.org.uk/)) is arguably the most important Health Technology Assessment (HTA) Agency, globally. NICE has a series of \"Technical Support Documents\", in which specific topics that are highly prevalent in HTA are discussed and guidance is provided to ensure best practices are used by modellers.","tags":[""],"title":"Partitioned Survival Analysis","type":"publication"},{"authors":["NICE Decision Support Unit"],"categories":null,"content":"","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"d1f1f2179f4d42d78e28ca8d7bbef89d","permalink":"/publication/nice-dsu-survival/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/nice-dsu-survival/","section":"publication","summary":"The National Institute for Health and Clinical Excellence (formerly, National Institute for Clinical Excellence, [NICE](https://www.nice.org.uk/)) is arguably the most important Health Technology Assessment (HTA) Agency, globally. NICE has a series of \"Technical Support Documents\", in which specific topics that are highly prevalent in HTA are discussed and guidance is provided to ensure best practices are used by modellers.","tags":[""],"title":"Survival Analysis","type":"publication"},{"authors":["M Harrer","P Cuijpers","T Furukawa","D Ebert"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"201d6bdb94327dd0d97c362280fde15f","permalink":"/publication/harrer-etal-2020/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/publication/harrer-etal-2020/","section":"publication","summary":"This guide shows you how to conduct meta-analyses in R from scratch. It is useful when performing real-world (network) meta-analyses.","tags":[""],"title":"Doing Meta-Analysis in R","type":"publication"},{"authors":["Gianluca Baio"],"categories":null,"content":"","date":1602201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602201600,"objectID":"b4da43bdc7dc3bdbc24810d2f203f2b8","permalink":"/publication/baio-notes/","publishdate":"2020-10-09T00:00:00Z","relpermalink":"/publication/baio-notes/","section":"publication","summary":"","tags":null,"title":"Introduction to statistical concepts","type":"publication"},{"authors":["Gianluca Baio"],"categories":null,"content":"","date":1602201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602201600,"objectID":"51d9323fb5cb7969ba1aec6d91d0ce5f","permalink":"/publication/baio-2019/","publishdate":"2020-10-09T00:00:00Z","relpermalink":"/publication/baio-2019/","section":"publication","summary":"Survival analysis features heavily as an important part of health economic evaluation, an increasingly important component of medical research. In this setting, it is important to estimate the mean time to the survival endpoint using limited information (typically from randomised trials) and thus it is useful to consider parametric survival models. In this paper, we review the features of the R package survHE, specifically designed to wrap several tools to perform survival analysis for economic evaluation. In particular, survHE embeds both a standard, frequentist analysis (through the R package flexsurv) and a Bayesian approach, based on Hamiltonian Monte Carlo (via the R package rstan) or Integrated Nested Laplace Approximation (with the the R package INLA). Using this composite approach, we obtain maximum flexibility and are able to pre-compile a wide range of parametric models, with a view of simplifying the modellers’ work and allowing them to move away from non-optimal work flows, including spreadsheets (e.g., Microsoft Excel).","tags":null,"title":"survHE: Survival analysis for health economic evaluation and cost-effectiveness modelling","type":"publication"},{"authors":null,"categories":null,"content":" Running a model in OpenBUGS Start OpenBUGS by double clicking on the OpenBUGS icon (or double click on the file OpenBUGS.exe, typically in somewhere like the OpenBUGS\\OpenBUGS322 directory in C:\\Program Files (x86)).\n Open the file containing model code as follows:\n Point to File on the tool bar and click once with left mouse button (LMB); Highlight Open option and click once with LMB; Select appropriate directory and double click on file to open. Files for OpenBUGS input are sometimes in .txt plain text format rather than the default .odc “compound document” format — so you may need to select “Files of type” .txt.  Select the Model menu as follows:\n Point to Model on the tool bar and click once with LMB. Highlight Specification option and click once with LMB.  Focus the window containing the model code by clicking the LMB once anywhere in the window — the top panel of the window should then become highlighted in blue to indicate that the window is currently in focus.\n Highlight the word model at the beginning of the code by dragging the mouse over the word whilst holding down the LMB.\n Check the model syntax by moving the mouse over the check model box in the Specification Tool window and clicking once with the LMB.\n A message saying\n model is syntactically correct\n should appear in the bottom left of the OpenBUGS program window. Any error messages will appear in the same place if there is a syntax error.\n  Open the data, if there are observed data in the model. The data can either be stored in a separate file, in which case open this file (or multiple files), or they may be stored in the same file as the model code.\n Load the data as follows:\n Highlight the word list at the beginning of the data file.\n Click once with the LMB on the load data box in the Specification Tool window.\n A message saying\n data loaded\n should appear in the bottom left of the OpenBUGS program window.\n  Select number of chains (sets of samples to simulate) by typing the number of chains required in the white box in the Specification Tool window.\n The default is 1, but we will typically use 2 or more. Running more than one chain, starting from different initial values, makes convergence checking easier (see later).  Compile the model by clicking once with the LMB on the compile box in the Specification Tool window.\n A message saying\n model compiled\n should appear in the bottom left of the OpenBUGS program window.\n  Open the initial values files. The initial values can either be stored in separate file(s), in which case open these files, or they may be stored in the same file as the model code.\n Load any initial values as follows:\n Highlight the word list at the beginning of the first set of initial values.\n Click once with LMB on the load inits box in the Specification Tool window.\n A message saying\n initial values loaded: model contains uninitialized nodes (try running gen inits or loading more files)\n should appear in the bottom left of the OpenBUGS program window.\n Repeat process for the second set of initial values, if running two chains.\n A message saying\n initial values loaded: model initialized\n should now appear in the bottom left of the OpenBUGS program window.\n Sometimes we can get away with not supplying any initial values ourselves, and we can just click gen inits to let OpenBUGS generate these automatically. Though this won’t generally work if any of the priors are vague. Also it is handy to start different chains at widely dispersed initial values to assess convergence.\n  Close the Specification Tool window by clicking once with LMB on the X button in top right corner of window.\n You are now ready to start running the simulation:\n Before doing so, you may want to set some monitors to store the sampled values for selected parameters (see section Monitoring parameter values below). To run the simulation, select the Update option from the Model menu. Type the number of updates (iterations of the simulation) you require in the appropriate white box (default is 1000). Click once on the update box. The program will now start simulating values for each parameter in the model. This may take a few seconds — the box marked iteration will tell you how many updates have currently been completed. The number of times this value is revised depends on the value you have set for refresh (see white box above iteration). The default is every 100 iterations. If the model is very fast, you should increase this to, e.g. 1000 or 10000, then the model will run even faster since OpenBUGS is not unnecessarily redrawing the screen hundreds of times in a split second. If the model is very slow, you may like to decrease it to, e.g. 10 or 1 so that OpenBUGS does not appear to “freeze” during sampling.  When the updates are finished, the message\n updates took *** s\n will appear in the bottom left of the OpenBUGS program window (where *** is the number of seconds taking to complete the simulation).\n If you set monitors for any parameters you can now check convergence and view graphical and numerical summaries of the samples (see below).\n To save any files created during your OpenBUGS run, focus the window containing the information you want to save, and select the Save As option from the File menu.\n To quit OpenBUGS, select the Exit option from the File menu.\n   Monitoring parameter values In order to check convergence and obtain posterior summaries of the model parameters, you first need to set monitors for each parameter of interest. This tells OpenBUGS to store the values sampled for those parameters; otherwise, OpenBUGS automatically discards the simulated values. There are two types of monitors in OpenBUGS:\nSample monitors  Setting a sample monitor tells OpenBUGS to store every value it simulates for that parameter. You will need to set sample monitors if you want to view trace plots of the samples to check convergence (see section Checking convergence below) and if you want to obtain posterior quantiles, for example, the posterior 95% Bayesian credible interval for that parameter. To set a sample monitor:  Select Samples from the Inference menu. Type the name of the parameter to be monitored in the white box marked node. Click once with the LMB on the box marked set Repeat for each parameter to be monitored.   Summary monitors  Setting a summary monitor tells OpenBUGS to store the running mean and standard deviation for the parameter. The values saved contain less information than saving each individual sample in the simulation, but require much less storage. This is an important consideration when running long simulations (1000’s of iterations) and storing values for many variables. To set a summary monitor:  Select Summary from the Inference menu. Type the name of the parameter to be monitored in the white box marked node. Click once with the LMB on the box marked set Repeat for each parameter to be monitored.     Note: you should not set a summary monitor until you are happy that convergence has been reached (see next section), since it is not possible to discard any of the pre-convergence (‘burn-in’) values from the running mean summary once it is set.    Checking convergence Checking convergence requires considerable care. It is very difficult to say conclusively that a chain (simulation) has converged, only to diagnose when it definitely hasn’t converged. The following are practical guidelines for assessing convergence:\n For models with many parameters, it is impractical to check convergence for every parameter, so just chose a random selection of relevant parameters to monitor.  For example, rather than checking convergence for every element of a vector of random effects, just chose a random subset (say, the first 5 or 10).  Examine trace plots of the sample values versus iteration to look for evidence of when the simulation appears to have stabilised:  To obtain ‘live’ trace plots for a parameter:  Select Samples from the Inference menu. Type the name of the parameter in the white box marked node. Click once with the LMB on the box marked trace: an empty graphics window will appear on screen. Repeat for each parameter required. Once you start running the simulations (using the Update Tool, trace plots for these parameters will appear ‘live’ in the graphics windows.  To obtain a trace plot showing the full history of the samples for any parameter for which you have previously set a sample monitor and carried out some updates:  Select Samples from the Inference menu. Type the name of the parameter in the white box marked node (or select name from pull down list). Click once with the LMB on the box marked history: a graphics window showing the sample trace will appear. Repeat for each parameter required.  In the graph here (Panel a) the chain converges by about 250 iterations. Running the simulation for longer (5000 iterations) and discarding the first 1000 as a “burn-in” produces the history plot in (Panel b). This looks like a “fat hairy caterpillar”, the typical appearance of a chain which has converged to the target distribution, and can be treated as a sequence of independent samples from that distribution. The graph here (Panel c), is the typical appearance of a chain which has converged to the target posterior distribution, but is slow to mix around that distribution. That is, the successive draws from the distribution are highly autocorrelated. In this case we should run the chain for longer (as in (Panel d)) to get sufficiently precise summaries of the posterior — see the next section for more advice. If you are running more than 1 chain simultaneously, the trace and history plots will show each chain in a different colour. This is shown in here (Panel e). We can be reasonably confident that convergence has been achieved when all the chains appear to be overlapping one another.    How many iterations after convergence? Once you are happy that convergence has been achieved, you will need to run the simulation for a further number of iterations to obtain samples that can be used for posterior inference. The more samples you save, the more accurate will be your posterior estimates. Therefore the number of samples to run depends on how many significant figures, for example, you need in your results.\nTo assess the accuracy of the posterior mean for each parameter, you can simply look at the Monte Carlo standard error, which is provided in the summaries given by stats (see the next section). This is an estimate of the difference between the mean of the sampled values (which we are using as our estimate of the posterior mean for each parameter) and the true posterior mean.\nTo assess the accuracy of the whole distribution, you can compare the Monte Carlo standard error \\(\\mbox{SE}(\\hat\\mu)\\) to the sample standard deviation \\(\\hat\\sigma\\) (also reported in the summary statistics table given by stats). There is some theory which suggests that for the reported 95% posterior quantiles to have about 94.5% to 95.5% true coverage, \\(\\mbox{SE}(\\hat\\mu)\\) should be 1% or less of \\(\\hat\\sigma\\). Equivalently the effective sample size of the chain1 should be greater than about 4000.\nAlternatively you can just take an informal approach, and run a sufficient number of samples to ensure that the posterior summaries of interest don’t appear to change within the desired number of significant figures.\n Obtaining summary statistics of the posterior distribution To obtain summaries of the monitored samples, to be used for posterior inference:\n Select Samples from the Inference menu. Type the name of the parameter to be summarised in the white box marked node (or select name from the pull down list, or type * to select all monitored parameters). Type the iteration number which you want to start your summary from in the white box marked beg: this allows the pre-convergence ‘burn-in’ samples to be discarded. Click once with LMB on box marked stats: a table reporting various summary statistics based on the sampled values of the selected parameter will appear.   Plotting summaries of the posterior OpenBUGS includes options for producing various plots of posterior summary statistics. The plot options include:\nKernel density plot: plots an estimate of the shape of the (univariate) marginal posterior distribution of a parameter see online manual for further details (select OpenBUGS User Manual from the Manuals menu in OpenBUGS and then take a look at the subsection on Density plots in the OpenBUGS Graphics section). Box plots, caterpillar plots or density strips: these plots show a side-by-side comparison of the posterior distributions of each element of a vector of parameters summarised either as a point estimate and 95% interval (caterpillar plot), by the mean, interquartile range and 95% interval (box plot), or a representation of the whole distribution using varying shading (density strips). This is often used for random effects parameters. For example, suppose you have a vector of random effects called p in your model:  You should have already set a samples monitor on the appropriate vector (p) and carried out a suitable number of updates. Then select Compare from the Inference menu. Type the name of the parameter vector to be plotted (in this case p) in the white box marked node. If you want to discard any pre-convergence burn-in samples before plotting, type the appropriate iteration number in the white box marked beg. Click once with LMB on the button marked either box plot, caterpillar or density strips as required.  Model fit: this option produces a ‘time series’ type plot and is suitable for plotting an ordered sequence of parameter estimates against corresponding values of a known variable, e.g. plotting posterior estimates of the fitted values of a growth curve against time. For example, in the rats model from the OpenBUGS examples Vol I, you could use this option to produce a plot of the vector of 5 fitted values for the weight of each rat (mu[i,]) against age (x), as follows:  You should have already set a samples monitor on the appropriate vector (i.e. mu, the mean of the normal distribution assumed for the responses, Y) and carried out an appropriate number of updates. Then select Compare from the Inference menu. Type the name of the (stochastic) parameter vector to be plotted on the vertical axis in the white box marked node (e.g. mu[1,] to produce the plot for rat 1). Type the name of the known (i.e. not stochastic) variable to be plotted on the horizontal axis in the white box marked axis (in this case, x, the name of the vector of ages at which each rat was measured). An optional argument is to type the name of another known (i.e. not stochastic) variable in the white box marked other (for example, Y[1,] — this would plot the observed measurements for rat 1 as well as the fitted values mu[1,]). If you want to discard any pre-convergence burn-in samples before plotting, type the appropriate iteration number in the white box marked beg. Click once with LMB on the button marked model fit. The resulting plot shows the posterior median (solid red line) and posterior 95% interval (dashed blue line) for the values of node (in this case, the fitted values mu[1,] for rat 1) against the values of the variable specified in the axis box (in this case, x, the age of the rat at each measurement). The black dots show the values of the variable specified in the other box (in this case,Y[1,], the observed weights for rat 1).  There are various options for customising all these plots (e.g. changing the order in which the elements of the vector are plotted, switching the \\(x\\) and \\(y\\) axis, etc.). To access these options, click on the window containing the plot to focus it, then place the mouse somewhere in the plot window and click once with the right mouse button. A menu will appear and you should select the Properties option. This will open another menu called Plot Properties which provides options for editing plot margins, axis labels and fonts etc. (these are generic options for all OpenBUGS plots), plus some special options specific only to certain plots (click on the Special button at the bottom of the Plot Properties menu). See the online manual for further details (select OpenBUGS User Manual from the Manuals menu in OpenBUGS, then go to OpenBUGS Graphics, or the Inference Menu then Compare).   Some notes on the BUGS language Basic syntax  The symbol \u0026lt;- represents logical dependence, ie m \u0026lt;- a + b*x indicates that m has the same value as the expression to the right hand side.\n The symbol ~ represents stochastic dependence, eg y ~ dunif(a,b) indicates that the variable y is modelled using a Uniform(\\(a,b\\)) distribution.\n Can use arrays and loops\nfor (i in 1:n){ r[i] ~ dbin(p[i],n[i]) p[i] ~ dunif(0,1) } Some functions can appear on left-hand-side of an expression, e.g.\nlogit(p[i])\u0026lt;- a + b*x[i] log(m[i]) \u0026lt;- c + d*y[i] mean(p[]) to take mean of whole array, or mean(p[m:n]) to take mean of elements m to n. Also for sum(p[]).\n dnorm(0,1)I(0,) means the distribution will be restricted to the range \\((0,\\infty)\\).\n   Functions in the BUGS language  p \u0026lt;- step(x - 0.7) = 1 if x \\(geq\\) 0.7 and 0 otherwise. Hence monitoring p and recording its mean will give the probability that x \\(\\geq\\) 0.7. p \u0026lt;- equals(x, 0.7) = 1 if x = 0.7 and 0 otherwise. tau \u0026lt;- 1/pow(s,2) sets \\(\\tau = 1/s^2\\). s \u0026lt;- 1/ sqrt(tau) sets \\(s=1/\\tau\\). p[i,k] \u0026lt;- inprod(pi[], Lambda[i,,k]) sets \\(p_{ik} = \\sum_{j} \\pi_j \\Lambda_{ijk}\\) Many other mathematical functions: see Functions under the Help menu in OpenBUGS for full syntax.   OpenBUGS data formats OpenBUGS accepts data files in:\nRectangular format  n[] r[] 47 0 148 18 ... 360 24 END R / S-Plus-like ‘list’ format:  list( N=12, n = c(47,148,119,810,211,196,148,215,207,97,256,360), r = c(0,18,8,46,8,13,9,31,14,8,29,24) ) The more flexible ‘list’ format is recommended, since data often consist of mixtures of scalars and vectors/arrays, or vectors/arrays of different lengths.\n    The Monte Carlo standard error is higher in chains which are autocorrelated (i.e. look less like independent samples from a posterior, and are slower to explore the posterior distribution, so give less accurate estimates of the posterior mean). The “effective sample size” \\(n_{eff}\\) denotes that a chain of \\(n \u0026gt; n_{eff}\\) autocorrelated samples as much information as \\(n_{eff}\\) independent samples. There are several ways of calculating \\(n_{eff}\\) — three different methods are used in Bayesian Methods in Health Economics, The BUGS Book and R2OpenBUGS!↩︎\n   ","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"68b882587b8f3193fd02ce63c4d3153a","permalink":"/tips/using-bugs/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/tips/using-bugs/","section":"tips","summary":"Running a model in OpenBUGS Start OpenBUGS by double clicking on the OpenBUGS icon (or double click on the file OpenBUGS.exe, typically in somewhere like the OpenBUGS\\OpenBUGS322 directory in C:\\Program Files (x86)).","tags":null,"title":"Using BUGS","type":"tips"},{"authors":["A Gabrio","Gianluca Baio","A Manca"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"39d16a02a5e11103cab6472764bacf5a","permalink":"/publication/gabrioetal-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/gabrioetal-2019/","section":"publication","summary":"","tags":null,"title":"Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment","type":"publication"},{"authors":["A Gabrio","A Mason","Gianluca Baio"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"8fb43973b924a50c92981c6fbbe011ef","permalink":"/publication/gabrio-etal-2018/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/publication/gabrio-etal-2018/","section":"publication","summary":"Cost-effectiveness analyses (CEAs) alongside randomised controlled trials (RCTs) are increasingly designed to collect resource use and preference-based health status data for the purpose of healthcare technology assessment. However, because of the way these measures are collected, they are prone to missing data, which can ultimately affect the decision of whether an intervention is good value for money. We examine how missing cost and effect outcome data are handled in RCT-based CEAs, complementing a previous review (covering 2003–2009, 88 articles) with a new systematic review (2009–2015, 81 articles) focussing on two different perspectives. First, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a quality evaluation scheme, providing a structured approach that can be used to guide the collection of information, elicitation of the assumptions, choice of methods and considerations of possible limitations of the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods. Our review shows that missing data in within-RCT CEAs are still often inadequately handled and the overall level of information provided to support the chosen methods is rarely satisfactory.","tags":null,"title":"A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data","type":"publication"},{"authors":["Gianluca Baio","A Berardi","Anna Heath"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"548b8d8b0d0284be95fe8e9356e5a73b","permalink":"/publication/baioetal-2016/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/baioetal-2016/","section":"publication","summary":"The book provides a description of the process of health economic evaluation and modelling for cost-effectiveness analysis, particularly from the perspective of a Bayesian statistical approach. Some relevant theory and introductory concepts are presented using practical examples and two running case studies. The book also describes in detail how to perform health economic evaluations using the R package [`BCEA`](https://gianluca.statistica.it/software/bcea/) (Bayesian Cost-Effectiveness Analysis). `BCEA` can be used to post-process the results of a Bayesian cost-effectiveness model and perform advanced analyses producing standardised and highly customisable outputs. It presents all the features of the package, including its many functions and their practical application, as well as its user-friendly web interface. The book is a valuable resource for statisticians and practitioners working in the field of health economics wanting to simplify and standardise their workflow, for example in the preparation of dossiers in support of marketing authorisation, or academic and scientific publications.","tags":null,"title":"Bayesian Cost-Effectiveness Analysis with the R package BCEA","type":"publication"},{"authors":["H Wickham","G Grolemund"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"ab7212099e4a7b66ba410eaeb5413841","permalink":"/publication/wickham-etal-2020/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/publication/wickham-etal-2020/","section":"publication","summary":"This is a very good introduction to R, with examples and hints on how to use modern tools, such as the [`tidyverse`](www.tidyverse.org). The book is available for free as an online resource, so you can browse and use it with no restriction. This book introduces R, RStudio, and the `tidyverse`, a collection of R packages designed to work together to facilitate data analysis. Suitable for readers with no previous experience on programming in R.","tags":[""],"title":"R for Data Science","type":"publication"},{"authors":["J Kruschke"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433116800,"objectID":"07096bd1203e3359c4b03e77b95cf02b","permalink":"/publication/kruschke-2015/","publishdate":"2015-06-01T00:00:00Z","relpermalink":"/publication/kruschke-2015/","section":"publication","summary":"This book aims at explaining how to actually do Bayesian data analysis, \"*by real people (like you), for realistic data (like yours)*\". It has a wealth of examples and explanations of mathematical details focussing on the intuitions and the practicalities of the modelling. There is a companion project in which the original examples (in JAGS) have been converted in [`bmrs`](https://github.com/paul-buerkner/brms) (a R package based on running [Stan](https://mc-stan.org/users/interfaces/rstan) in the background and with simplified syntax) and [`tidyverse`](http://style.tidyverse.org/)","tags":[""],"title":"Doing Bayesian Data Analysis","type":"publication"},{"authors":["G Grolemund"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"edf0040141d50560941c24322f4521cd","permalink":"/publication/gromelund-2014/","publishdate":"2021-06-16T00:00:00Z","relpermalink":"/publication/gromelund-2014/","section":"publication","summary":"This is a very good introduction to R, with examples and hints on how to use modern tools, such as the [`tidyverse`](www.tidyverse.org). The book is available for free as an online resource, so you can browse and use it with no restriction. This book introduces R, RStudio, and the `tidyverse`, a collection of R packages designed to work together to facilitate data analysis. Suitable for readers with no previous experience on programming in R.","tags":[""],"title":"Hands-On Programming with R","type":"publication"},{"authors":["A Gelman, JB Carlin, HS Stern, DB Dunson, A Vehtari, DB Rubin"],"categories":null,"content":"","date":1383523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383523200,"objectID":"708a359f4161d28102fa9511435c2d4d","permalink":"/publication/gelman-etal-2013/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/publication/gelman-etal-2013/","section":"publication","summary":"This classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors—all leaders in the statistics community—introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice.","tags":[""],"title":"Bayesian Data Analysis, 3rd edition","type":"publication"},{"authors":["Gianluca Baio"],"categories":null,"content":"","date":1351728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351728000,"objectID":"ebab34a25124194390c43f76f8bd2999","permalink":"/publication/baio-2012/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/publication/baio-2012/","section":"publication","summary":"Health economics is concerned with the study of the cost-effectiveness of health care interventions. This book provides an overview of Bayesian methods for the analysis of health economic data. After an introduction to the basic economic concepts and methods of evaluation, it presents Bayesian statistics using accessible mathematics. The next chapters describe the theory and practice of cost-effectiveness analysis from a statistical viewpoint, and Bayesian computation, notably MCMC. The final chapter presents three detailed case studies covering cost-effectiveness analyses using individual data from clinical trials, evidence synthesis and hierarchical models and Markov models. The text uses WinBUGS and JAGS with datasets and code available online","tags":[""],"title":"Bayesian Methods in Health Economics","type":"publication"},{"authors":["D Lunn, C Jackson, N Best, A Thomas, D Spiegelhalter"],"categories":null,"content":"","date":1351728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351728000,"objectID":"838eeef16d9ee41f0f8ad61fced32fd7","permalink":"/publication/lunn-etal-2012/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/publication/lunn-etal-2012/","section":"publication","summary":"The BUGS Book provides a practical introduction to this program and its use. The text presents complete coverage of all the functionalities of BUGS, including prediction, missing data, model criticism, and prior sensitivity. It also features a large number of worked examples and a wide range of applications from various disciplines. The book introduces regression models, techniques for criticism and comparison, and a wide range of modelling issues before going into the vital area of hierarchical models, one of the most common applications of Bayesian methods. It deals with essentials of modelling without getting bogged down in complexity. The book emphasises model criticism, model comparison, sensitivity analysis to alternative priors, and thoughtful choice of prior distributions—all those aspects of the \"art\" of modelling that are easily overlooked in more theoretical expositions. More pragmatic than ideological, the authors systematically work through the large range of \"tricks\" that reveal the real power of the BUGS software, for example, dealing with missing data, censoring, grouped data, prediction, ranking, parameter constraints, and so on. Many of the examples are biostatistical, but they do not require domain knowledge and are generalisable to a wide range of other application areas. Full code and data for examples, exercises, and some solutions can be found on the book’s website.","tags":[""],"title":"The BUGS Book","type":"publication"},{"authors":["NJ Welton, AJ Sutton, N Cooper, KR Abrams, AE Ades"],"categories":null,"content":"","date":1335830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1335830400,"objectID":"a51a8532847571998b2902c2beda410e","permalink":"/publication/welton-etal-2012/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/publication/welton-etal-2012/","section":"publication","summary":"In the evaluation of healthcare, rigorous methods of quantitative assessment are necessary to establish interventions that are both effective and cost-effective. Usually a single study will not fully address these issues and it is desirable to synthesize evidence from multiple sources. This book aims to provide a practical guide to evidence synthesis for the purpose of decision making, starting with a simple single parameter model, where all studies estimate the same quantity (pairwise meta-analysis) and progressing to more complex multi-parameter structures (including meta-regression, mixed treatment comparisons, Markov models of disease progression, and epidemiology models). A comprehensive, coherent framework is adopted and estimated using Bayesian methods.","tags":[""],"title":"Evidence Synthesis for Decision Making in Healthcare","type":"publication"},{"authors":["M Daniels","J Hogan"],"categories":null,"content":"","date":1212278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212278400,"objectID":"4b763db63716431ec8310d6007d249df","permalink":"/publication/daniels-hogan-2008/","publishdate":"2008-06-01T00:00:00Z","relpermalink":"/publication/daniels-hogan-2008/","section":"publication","summary":"The book first reviews modern approaches to formulate and interpret regression models for longitudinal data. It then discusses key ideas in Bayesian inference, including specifying prior distributions, computing posterior distribution, and assessing model fit. The book carefully describes the assumptions needed to make inferences about a full-data distribution from incompletely observed data. For settings with ignorable dropout, it emphasizes the importance of covariance models for inference about the mean while for nonignorable dropout, the book studies a variety of models in detail. It concludes with three case studies that highlight important features of the Bayesian approach for handling nonignorable missingness. With suggestions for further reading at the end of most chapters as well as many applications to the health sciences, this resource offers a unified Bayesian approach to handle missing data in longitudinal studies.","tags":null,"title":"Missing data in longitudinal studies: strategies for Bayesian modeling and sensitivity analysis","type":"publication"},{"authors":["A Gelman","J Hill"],"categories":null,"content":"","date":1188604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1188604800,"objectID":"8c0cb873e9d06e129e474e01848ae0c5","permalink":"/publication/gelman-hill-2007/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/gelman-hill-2007/","section":"publication","summary":"This is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, post-stratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.","tags":[""],"title":"Data Analysis Using Regression and Multilevel/Hierarchical Models","type":"publication"},{"authors":["A Briggs","M Schulpher","K Claxton"],"categories":null,"content":"","date":1149120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1149120000,"objectID":"975124761d2619af9a4592386bb3a932","permalink":"/publication/briggs-etal-2006/","publishdate":"2006-06-01T00:00:00Z","relpermalink":"/publication/briggs-etal-2006/","section":"publication","summary":"In financially constrained health systems across the world, increasing emphasis is being placed on the ability to demonstrate that health care interventions are not only effective, but also cost-effective.  This book deals with decision modelling techniques that can be used to estimate the value for money of various interventions including medical devices, surgical procedures, diagnostic technologies, and pharmaceuticals.  Particular emphasis is placed on the importance of the appropriate representation of uncertainty in the evaluative process and the implication this uncertainty has for decision making and the need for future research. This highly practical guide takes the reader through the key principles and approaches of modelling techniques. It begins with the basics of constructing different forms of the model, the population of the model with input parameter estimates, analysis of the results, and progression to the holistic view of models as a valuable tool for informing future research exercises.  Case studies and exercises are supported with online templates and solutions.","tags":[""],"title":"Decision Modelling for Health Economic Evaluation","type":"publication"},{"authors":["D Spiegelhalter, KR Abrams, JP Myles"],"categories":null,"content":"","date":1099267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1099267200,"objectID":"4c7be6280ac2d895dca54defeb9b06d1","permalink":"/publication/spiegelhalter-etal-2004/","publishdate":"2020-12-11T00:00:00Z","relpermalink":"/publication/spiegelhalter-etal-2004/","section":"publication","summary":"Bayesian Approaches to Clinical Trials and Health-Care Evaluation provides a valuable overview of this rapidly evolving field, including basic Bayesian ideas, prior distributions, clinical trials, observational studies, evidence synthesis and cost-effectiveness analysis. Covers a broad array of essential topics, building from the basics to more advanced techniques. Bayesian Approaches to Clinical Trials and Health-Care Evaluation is suitable for students and researchers in medical statistics, statisticians in the pharmaceutical industry, and anyone involved in conducting clinical trials and assessment of health-care technology.","tags":[""],"title":"Bayesian approaches to clinical trials and health care evaluation","type":"publication"}]